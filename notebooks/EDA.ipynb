{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ddaf0ce",
   "metadata": {},
   "source": [
    " EDA FOR CREDITRUST FINANCIAL\n",
    " ML Engineer Analysis - Customer Complaint Intelligence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80eec5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\G5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\G5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\G5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading omw-eng: Package 'omw-eng' not found in\n",
      "[nltk_data]     index\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üì¶ SECTION 1: EXECUTIVE SETUP & BUSINESS CONTEXT\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Professional imports for advanced NLP\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-eng')\n",
    "\n",
    "# Set professional aesthetics\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3570cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: d:\\10 acadamy\\Intelligent Complaint Analysis for Financial Services\\data\\raw\\complaints.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()  # This is 'd:/10 acadamy/Intelligent Complaint Analysis for Financial Services/notebokks'\n",
    "\n",
    "# Go up one level to project root, then navigate to data/raw\n",
    "project_root = os.path.dirname(current_dir)  # Goes up one level\n",
    "data_path = os.path.join(project_root, 'data', 'raw', 'complaints.csv')\n",
    "\n",
    "print(f\"Loading data from: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcec0fe",
   "metadata": {},
   "source": [
    "DATA LOADING WITH MEMORY OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ce5cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üì¶ PHASE 1: DATA ACQUISITION & INITIAL ASSESSMENT\n",
      "====================================================================================================\n",
      "‚úÖ Correct data path calculated: d:\\10 acadamy\\Intelligent Complaint Analysis for Financial Services\\data\\raw\\complaints.csv\n",
      "üöÄ Loading 464K+ complaint database...\n",
      "   üìä Chunk 5: 50,000 records loaded\n",
      "   üìä Chunk 10: 50,000 records loaded\n",
      "   üìä Chunk 15: 50,000 records loaded\n",
      "   üìä Chunk 20: 50,000 records loaded\n",
      "   üìä Chunk 25: 50,000 records loaded\n",
      "   üìä Chunk 30: 50,000 records loaded\n",
      "   üìä Chunk 35: 50,000 records loaded\n",
      "   üìä Chunk 40: 50,000 records loaded\n",
      "   üìä Chunk 45: 50,000 records loaded\n",
      "   üìä Chunk 50: 50,000 records loaded\n",
      "   üìä Chunk 55: 50,000 records loaded\n",
      "   üìä Chunk 60: 50,000 records loaded\n",
      "   üìä Chunk 65: 50,000 records loaded\n",
      "   üìä Chunk 70: 50,000 records loaded\n",
      "   üìä Chunk 75: 50,000 records loaded\n",
      "   üìä Chunk 80: 50,000 records loaded\n",
      "   üìä Chunk 85: 50,000 records loaded\n",
      "   üìä Chunk 90: 50,000 records loaded\n",
      "   üìä Chunk 95: 50,000 records loaded\n",
      "   üìä Chunk 100: 50,000 records loaded\n",
      "   üìä Chunk 105: 50,000 records loaded\n",
      "   üìä Chunk 110: 50,000 records loaded\n",
      "   üìä Chunk 115: 50,000 records loaded\n",
      "   üìä Chunk 120: 50,000 records loaded\n",
      "   üìä Chunk 125: 50,000 records loaded\n",
      "   üìä Chunk 130: 50,000 records loaded\n",
      "   üìä Chunk 135: 50,000 records loaded\n",
      "   üìä Chunk 140: 50,000 records loaded\n",
      "   üìä Chunk 145: 50,000 records loaded\n",
      "   üìä Chunk 150: 50,000 records loaded\n",
      "   üìä Chunk 155: 50,000 records loaded\n",
      "   üìä Chunk 160: 50,000 records loaded\n",
      "   üìä Chunk 165: 50,000 records loaded\n",
      "   üìä Chunk 170: 50,000 records loaded\n",
      "   üìä Chunk 175: 50,000 records loaded\n",
      "   üìä Chunk 180: 50,000 records loaded\n",
      "   üìä Chunk 185: 50,000 records loaded\n",
      "   üìä Chunk 190: 50,000 records loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìà SECTION 2: DATA LOADING WITH MEMORY OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üì¶ PHASE 1: DATA ACQUISITION & INITIAL ASSESSMENT\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Get the correct path to your data\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.dirname(current_dir)\n",
    "data_path = os.path.join(project_root, 'data', 'raw', 'complaints.csv')\n",
    "\n",
    "print(f\"‚úÖ Correct data path calculated: {data_path}\")\n",
    "\n",
    "# Optimized data types for memory efficiency\n",
    "dtype_strategy = {\n",
    "    'Complaint ID': 'str',\n",
    "    'Date received': 'str',\n",
    "    'Product': 'category',\n",
    "    'Sub-product': 'category',\n",
    "    'Issue': 'category',\n",
    "    'Sub-issue': 'category',\n",
    "    'Company': 'category',\n",
    "    'State': 'category',\n",
    "    'ZIP code': 'str',\n",
    "    'Tags': 'category',\n",
    "    'Consumer consent provided?': 'category',\n",
    "    'Submitted via': 'category',\n",
    "    'Company response to consumer': 'category',\n",
    "    'Timely response?': 'category',\n",
    "    'Consumer disputed?': 'category',\n",
    "    'Consumer complaint narrative': 'object'\n",
    "}\n",
    "\n",
    "# Load data in chunks\n",
    "print(\"üöÄ Loading 464K+ complaint database...\")\n",
    "chunks = []\n",
    "chunk_size = 50000\n",
    "\n",
    "# CRITICAL: Use data_path variable here, not the hardcoded string\n",
    "for i, chunk in enumerate(pd.read_csv(data_path,\n",
    "                                       dtype=dtype_strategy,\n",
    "                                       chunksize=chunk_size,\n",
    "                                       parse_dates=['Date received'],\n",
    "                                       infer_datetime_format=True)):\n",
    "    chunks.append(chunk)\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"   üìä Chunk {i+1}: {len(chunk):,} records loaded\")\n",
    "    \n",
    "df = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b92d02e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ DATA LOADED SUCCESSFULLY\n",
      "   Total Records: 9,609,797\n",
      "   Total Features: 18\n",
      "   Memory Usage: 12188.49 MB\n",
      "   Time Range: 2011-12-01 to 2025-06-23\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n‚úÖ DATA LOADED SUCCESSFULLY\")\n",
    "print(f\"   Total Records: {df.shape[0]:,}\")\n",
    "print(f\"   Total Features: {df.shape[1]}\")\n",
    "print(f\"   Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"   Time Range: {df['Date received'].min().date()} to {df['Date received'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4dbcf1",
   "metadata": {},
   "source": [
    "EXECUTIVE DATA QUALITY DASHBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47dc5daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üîç PHASE 2: DATA QUALITY ASSESSMENT\n",
      "====================================================================================================\n",
      "üìä DATAFRAME SHAPE: (9609797, 18)\n",
      "   ‚Ä¢ Total Rows: 9,609,797\n",
      "   ‚Ä¢ Total Columns: 18\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîç MISSING VALUES ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìã Total missing cells in dataset: 32,030,923\n",
      "\n",
      "üìä TOP 10 COLUMNS WITH MISSING VALUES:\n",
      "--------------------------------------------------\n",
      "                              Missing_Count  Missing_Percentage\n",
      "Tags                                8981029           93.457011\n",
      "Consumer disputed?                  8841498           92.005044\n",
      "Consumer complaint narrative        6629041           68.982113\n",
      "Company public response             4770207           49.638999\n",
      "Consumer consent provided?          1649561           17.165409\n",
      "Sub-issue                            839522            8.736105\n",
      "Sub-product                          235295            2.448491\n",
      "State                                 54516            0.567296\n",
      "ZIP code                              30228            0.314554\n",
      "Company response to consumer             20            0.000208\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL FIELD - Consumer Complaint Narrative:\n",
      "   ‚Ä¢ Missing narratives: 6,629,041\n",
      "   ‚Ä¢ Percentage missing: 69.0%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üìà MISSING VALUES HEATMAP PREVIEW\n",
      "--------------------------------------------------------------------------------\n",
      "(Showing heatmap for 10,000 sample rows)\n",
      "\n",
      "üìä COLUMNS WITH > 0% MISSING VALUES:\n",
      "   ‚Ä¢ Tags: 93.4% missing (9,340 rows)\n",
      "   ‚Ä¢ Consumer disputed?: 91.9% missing (9,186 rows)\n",
      "   ‚Ä¢ Consumer complaint narrative: 68.3% missing (6,834 rows)\n",
      "   ‚Ä¢ Company public response: 49.2% missing (4,923 rows)\n",
      "   ‚Ä¢ Consumer consent provided?: 17.0% missing (1,696 rows)\n",
      "   ‚Ä¢ Sub-issue: 8.6% missing (865 rows)\n",
      "   ‚Ä¢ Sub-product: 2.5% missing (249 rows)\n",
      "   ‚Ä¢ State: 0.5% missing (55 rows)\n",
      "   ‚Ä¢ ZIP code: 0.3% missing (31 rows)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìä SECTION 3: EXECUTIVE DATA QUALITY DASHBOARD\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üîç PHASE 2: DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# First, display the DataFrame shape\n",
    "print(f\"üìä DATAFRAME SHAPE: {df.shape}\")\n",
    "print(f\"   ‚Ä¢ Total Rows: {df.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Total Columns: {df.shape[1]}\")\n",
    "\n",
    "# Create comprehensive data quality report\n",
    "quality_metrics = {}\n",
    "\n",
    "# 1. Missing Values Analysis\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"üîç MISSING VALUES ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "\n",
    "# Display missing values summary\n",
    "print(f\"\\nüìã Total missing cells in dataset: {missing_data.sum():,}\")\n",
    "\n",
    "# Display top 10 columns with most missing values\n",
    "print(\"\\nüìä TOP 10 COLUMNS WITH MISSING VALUES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a DataFrame for better display\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percentage': missing_percentage\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "# Display top 10\n",
    "print(missing_df.head(10).to_string())\n",
    "\n",
    "quality_metrics['missing_values'] = {\n",
    "    'total_missing_cells': missing_data.sum(),\n",
    "    'missing_percentage_overall': (missing_data.sum() / (df.shape[0] * df.shape[1]) * 100),\n",
    "    'critical_missing_narratives': missing_data['Consumer complaint narrative'],\n",
    "    'critical_missing_percentage': missing_percentage['Consumer complaint narrative']\n",
    "}\n",
    "\n",
    "# Display the critical narrative missing info\n",
    "print(f\"\\n‚ö†Ô∏è  CRITICAL FIELD - Consumer Complaint Narrative:\")\n",
    "print(f\"   ‚Ä¢ Missing narratives: {quality_metrics['missing_values']['critical_missing_narratives']:,}\")\n",
    "print(f\"   ‚Ä¢ Percentage missing: {quality_metrics['missing_values']['critical_missing_percentage']:.1f}%\")\n",
    "\n",
    "# 2. Visualize missing values\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"üìà MISSING VALUES HEATMAP PREVIEW\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# For large datasets, sample to create visualization\n",
    "if len(df) > 10000:\n",
    "    sample_size = min(10000, len(df))\n",
    "    missing_sample = df.sample(sample_size).isnull()\n",
    "    print(f\"(Showing heatmap for {sample_size:,} sample rows)\")\n",
    "else:\n",
    "    missing_sample = df.isnull()\n",
    "\n",
    "# Calculate percentage of missing per column\n",
    "missing_summary = missing_sample.sum().sort_values(ascending=False)\n",
    "missing_pct = (missing_summary / len(missing_sample)) * 100\n",
    "\n",
    "print(\"\\nüìä COLUMNS WITH > 0% MISSING VALUES:\")\n",
    "for col in missing_pct[missing_pct > 0].index:\n",
    "    print(f\"   ‚Ä¢ {col}: {missing_pct[col]:.1f}% missing ({missing_summary[col]:,} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d8aa359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã DATA QUALITY METRICS:\n",
      "--------------------------------------------------------------------------------\n",
      "1Ô∏è‚É£  Completeness:\n",
      "   ‚Ä¢ Narratives Missing: 6,629,041 (69.0%)\n",
      "   ‚Ä¢ Overall Data Completeness: 81.5%\n",
      "\n",
      "2Ô∏è‚É£  Uniqueness:\n",
      "   ‚Ä¢ Duplicate Complaints: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìã DATA QUALITY METRICS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"1Ô∏è‚É£  Completeness:\")\n",
    "print(f\"   ‚Ä¢ Narratives Missing: {quality_metrics['missing_values']['critical_missing_narratives']:,} \"\n",
    "      f\"({quality_metrics['missing_values']['critical_missing_percentage']:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Overall Data Completeness: {(100 - quality_metrics['missing_values']['missing_percentage_overall']):.1f}%\")\n",
    "\n",
    "# 2. Duplicate Analysis\n",
    "duplicate_count = df.duplicated(subset=['Complaint ID']).sum()\n",
    "quality_metrics['duplicates'] = {\n",
    "    'total_duplicates': duplicate_count,\n",
    "    'duplicate_percentage': (duplicate_count / len(df)) * 100\n",
    "}\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  Uniqueness:\")\n",
    "print(f\"   ‚Ä¢ Duplicate Complaints: {duplicate_count:,} \"\n",
    "      f\"({quality_metrics['duplicates']['duplicate_percentage']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ded17b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£  Temporal Coverage:\n",
      "   ‚Ä¢ Time Period: 2011-12-01 to 2025-06-23\n",
      "   ‚Ä¢ Total Days: 4,953 days\n",
      "   ‚Ä¢ Average Complaints/Day: 1940.2\n"
     ]
    }
   ],
   "source": [
    "# 3. Temporal Coverage\n",
    "date_range_days = (df['Date received'].max() - df['Date received'].min()).days\n",
    "quality_metrics['temporal'] = {\n",
    "    'date_range_days': date_range_days,\n",
    "    'complaints_per_day': len(df) / date_range_days,\n",
    "    'start_date': df['Date received'].min().date(),\n",
    "    'end_date': df['Date received'].max().date()\n",
    "}\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Temporal Coverage:\")\n",
    "print(f\"   ‚Ä¢ Time Period: {quality_metrics['temporal']['start_date']} to {quality_metrics['temporal']['end_date']}\")\n",
    "print(f\"   ‚Ä¢ Total Days: {date_range_days:,} days\")\n",
    "print(f\"   ‚Ä¢ Average Complaints/Day: {quality_metrics['temporal']['complaints_per_day']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2535b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4Ô∏è‚É£  Data Diversity:\n",
      "   ‚Ä¢ Unique Products: 21\n",
      "   ‚Ä¢ Unique Companies: 7,674\n",
      "   ‚Ä¢ Unique Issues: 178\n",
      "   ‚Ä¢ States Covered: 63\n"
     ]
    }
   ],
   "source": [
    "# 4. Cardinality Analysis\n",
    "quality_metrics['cardinality'] = {\n",
    "    'unique_products': df['Product'].nunique(),\n",
    "    'unique_companies': df['Company'].nunique(),\n",
    "    'unique_states': df['State'].nunique(),\n",
    "    'unique_issues': df['Issue'].nunique()\n",
    "}\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£  Data Diversity:\")\n",
    "print(f\"   ‚Ä¢ Unique Products: {quality_metrics['cardinality']['unique_products']}\")\n",
    "print(f\"   ‚Ä¢ Unique Companies: {quality_metrics['cardinality']['unique_companies']:,}\")\n",
    "print(f\"   ‚Ä¢ Unique Issues: {quality_metrics['cardinality']['unique_issues']}\")\n",
    "print(f\"   ‚Ä¢ States Covered: {quality_metrics['cardinality']['unique_states']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb1ce3e",
   "metadata": {},
   "source": [
    "ADVANCED PRODUCT ANALYSIS - BUSINESS FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89a95ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üéØ PHASE 3: PRODUCT ANALYSIS - CREDITRUST BUSINESS MAPPING\n",
      "====================================================================================================\n",
      "‚ö†Ô∏è  APPLYING NLP-VIABILITY FILTER (69% of data lacks narratives)\n",
      "   ‚Ä¢ Original dataset: 9,609,797 complaints\n",
      "   ‚Ä¢ NLP-viable dataset: 2,980,756 complaints (31.0%)\n",
      "\n",
      "üìä BUSINESS-RELEVANT COMPLAINT DISTRIBUTION:\n",
      "--------------------------------------------------------------------------------\n",
      "üìà OVERALL TRENDS (All 9.6M complaints):\n",
      "   ‚Ä¢ Total Complaints in Database: 9,609,797\n",
      "   ‚Ä¢ Complaints Relevant to CrediTrust: 1,105,974 (11.5%)\n",
      "\n",
      "üéØ NLP-ANALYZABLE DATA (3.0M with narratives):\n",
      "   ‚Ä¢ NLP-viable Complaints: 2,980,756\n",
      "   ‚Ä¢ Business-relevant & NLP-viable: 515,810 (17.3% of viable data)\n",
      "\n",
      "üìä PRODUCT-WISE BREAKDOWN:\n",
      "--------------------------------------------------------------------------------\n",
      "Product                     Total   NLP-Viable   Viable %\n",
      "   ‚Ä¢ Credit Card               448,335      197,126      44.0% ‚ö†Ô∏è MEDIUM\n",
      "   ‚Ä¢ Personal Loan             135,172       66,276      49.0% ‚ö†Ô∏è MEDIUM\n",
      "   ‚Ä¢ Savings Account           377,383      155,204      41.1% ‚ö†Ô∏è MEDIUM\n",
      "   ‚Ä¢ Money Transfer            145,084       97,204      67.0% ‚úÖ HIGH\n",
      "\n",
      "üìà KEY BUSINESS INSIGHT:\n",
      "   ‚Ä¢ Only 46.6% of business-relevant complaints have analyzable narratives\n",
      "   ‚Ä¢ For NLP/AI analysis, focus on 515,810 complaints\n",
      "   ‚Ä¢ 590,164 business complaints cannot be text-analyzed\n",
      "\n",
      "üìã SUMMARY DATAFRAME:\n",
      "           Product  Total_Complaints  NLP_Viable  Viable_Pct  Missing_Narratives\n",
      "0      Credit Card            448335      197126   43.968461              251209\n",
      "1    Personal Loan            135172       66276   49.030864               68896\n",
      "2  Savings Account            377383      155204   41.126389              222179\n",
      "3   Money Transfer            145084       97204   66.998428               47880\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìà SECTION 4: ADVANCED PRODUCT ANALYSIS - BUSINESS FOCUS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üéØ PHASE 3: PRODUCT ANALYSIS - CREDITRUST BUSINESS MAPPING\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# CRITICAL: First filter for NLP-viable data\n",
    "print(\"‚ö†Ô∏è  APPLYING NLP-VIABILITY FILTER (69% of data lacks narratives)\")\n",
    "viable_df = df[df['Consumer complaint narrative'].notna()].copy()\n",
    "print(f\"   ‚Ä¢ Original dataset: {len(df):,} complaints\")\n",
    "print(f\"   ‚Ä¢ NLP-viable dataset: {len(viable_df):,} complaints ({len(viable_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Create business-focused product mapping\n",
    "product_mapping = {\n",
    "    # Credit Cards (Our Core Product)\n",
    "    'Credit card': 'Credit Card',\n",
    "    'Credit card or prepaid card': 'Credit Card',\n",
    "    'Prepaid card': 'Credit Card',\n",
    "    \n",
    "    # Personal Loans (Our Product)\n",
    "    'Payday loan, title loan, or personal loan': 'Personal Loan',\n",
    "    'Consumer Loan': 'Personal Loan',\n",
    "    'Vehicle loan or lease': 'Personal Loan',\n",
    "    \n",
    "    # Savings Accounts (Our Product)\n",
    "    'Bank account or service': 'Savings Account',\n",
    "    'Checking or savings account': 'Savings Account',\n",
    "    'Savings account': 'Savings Account',\n",
    "    \n",
    "    # Money Transfers (Our Product)\n",
    "    'Money transfer, virtual currency, or money service': 'Money Transfer',\n",
    "    'Virtual currency': 'Money Transfer',\n",
    "    \n",
    "    # Other categories for context\n",
    "    'Mortgage': 'Mortgage',\n",
    "    'Student loan': 'Student Loan',\n",
    "    'Debt collection': 'Debt Collection',\n",
    "    'Credit reporting, credit repair services, or other personal consumer reports': 'Credit Reporting'\n",
    "}\n",
    "\n",
    "# Apply mapping to BOTH datasets\n",
    "df['Product_Category'] = df['Product'].map(product_mapping).fillna('Other')\n",
    "viable_df['Product_Category'] = viable_df['Product'].map(product_mapping).fillna('Other')\n",
    "\n",
    "# Business Impact Analysis\n",
    "print(\"\\nüìä BUSINESS-RELEVANT COMPLAINT DISTRIBUTION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "our_products = ['Credit Card', 'Personal Loan', 'Savings Account', 'Money Transfer']\n",
    "\n",
    "# Analyze FULL dataset for overall trends\n",
    "business_df_full = df[df['Product_Category'].isin(our_products)]\n",
    "total_business_complaints_full = len(business_df_full)\n",
    "\n",
    "# Analyze NLP-VIABLE dataset for text analysis\n",
    "business_df_viable = viable_df[viable_df['Product_Category'].isin(our_products)]\n",
    "total_business_complaints_viable = len(business_df_viable)\n",
    "\n",
    "print(f\"üìà OVERALL TRENDS (All 9.6M complaints):\")\n",
    "print(f\"   ‚Ä¢ Total Complaints in Database: {len(df):,}\")\n",
    "print(f\"   ‚Ä¢ Complaints Relevant to CrediTrust: {total_business_complaints_full:,} \"\n",
    "      f\"({(total_business_complaints_full/len(df)*100):.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ NLP-ANALYZABLE DATA (3.0M with narratives):\")\n",
    "print(f\"   ‚Ä¢ NLP-viable Complaints: {len(viable_df):,}\")\n",
    "print(f\"   ‚Ä¢ Business-relevant & NLP-viable: {total_business_complaints_viable:,} \"\n",
    "      f\"({(total_business_complaints_viable/len(viable_df)*100):.1f}% of viable data)\")\n",
    "\n",
    "# Detailed product breakdown - SHOW BOTH PERSPECTIVES\n",
    "print(\"\\nüìä PRODUCT-WISE BREAKDOWN:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Product':<20} {'Total':>12} {'NLP-Viable':>12} {'Viable %':>10}\")\n",
    "\n",
    "for product in our_products:\n",
    "    # Full dataset counts\n",
    "    total_count = len(df[df['Product_Category'] == product])\n",
    "    \n",
    "    # NLP-viable counts\n",
    "    viable_count = len(viable_df[viable_df['Product_Category'] == product])\n",
    "    \n",
    "    # Calculate percentage viable\n",
    "    viable_pct = (viable_count / total_count * 100) if total_count > 0 else 0\n",
    "    \n",
    "    # Determine severity\n",
    "    if viable_pct > 50:\n",
    "        severity = \"‚úÖ HIGH\"\n",
    "    elif viable_pct > 30:\n",
    "        severity = \"‚ö†Ô∏è MEDIUM\"\n",
    "    else:\n",
    "        severity = \"üö® LOW\"\n",
    "    \n",
    "    print(f\"   ‚Ä¢ {product:<20} {total_count:>12,} {viable_count:>12,} {viable_pct:>9.1f}% {severity}\")\n",
    "\n",
    "# Calculate overall viability percentage for business products\n",
    "total_viable_pct = (total_business_complaints_viable / total_business_complaints_full * 100) if total_business_complaints_full > 0 else 0\n",
    "\n",
    "print(f\"\\nüìà KEY BUSINESS INSIGHT:\")\n",
    "print(f\"   ‚Ä¢ Only {total_viable_pct:.1f}% of business-relevant complaints have analyzable narratives\")\n",
    "print(f\"   ‚Ä¢ For NLP/AI analysis, focus on {total_business_complaints_viable:,} complaints\")\n",
    "print(f\"   ‚Ä¢ {total_business_complaints_full - total_business_complaints_viable:,} business complaints cannot be text-analyzed\")\n",
    "\n",
    "# Create a visualization-ready summary\n",
    "product_summary = pd.DataFrame({\n",
    "    'Product': our_products,\n",
    "    'Total_Complaints': [len(df[df['Product_Category'] == p]) for p in our_products],\n",
    "    'NLP_Viable': [len(viable_df[viable_df['Product_Category'] == p]) for p in our_products]\n",
    "})\n",
    "\n",
    "product_summary['Viable_Pct'] = (product_summary['NLP_Viable'] / product_summary['Total_Complaints'] * 100)\n",
    "product_summary['Missing_Narratives'] = product_summary['Total_Complaints'] - product_summary['NLP_Viable']\n",
    "\n",
    "print(\"\\nüìã SUMMARY DATAFRAME:\")\n",
    "print(product_summary.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f67a1",
   "metadata": {},
   "source": [
    "CLASS BALANCE & STATISTICAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74cce42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "‚öñÔ∏è PHASE 4: CLASS BALANCE & STATISTICAL ANALYSIS\n",
      "====================================================================================================\n",
      "üìä USING NLP-VIABLE BUSINESS DATA FROM SECTION 4\n",
      "   ‚Ä¢ Business-relevant complaints: 515,810\n",
      "   ‚Ä¢ Business complaints with narratives: 515,810\n",
      "\n",
      "üìä PRODUCT DISTRIBUTION (NLP-Viable Business Data):\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ Credit Card           197,126 complaints ( 38.2%) üö® HIGH\n",
      "   ‚Ä¢ Savings Account       155,204 complaints ( 30.1%) üö® HIGH\n",
      "   ‚Ä¢ Money Transfer         97,204 complaints ( 18.8%) ‚ö†Ô∏è MEDIUM\n",
      "   ‚Ä¢ Personal Loan          66,276 complaints ( 12.8%) ‚úÖ LOW\n",
      "\n",
      "‚úÖ Saved class balance visualization: reports/class_balance_analysis.html\n",
      "\n",
      "üìä STATISTICAL IMBALANCE ANALYSIS (NLP-Viable Business Data):\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ Max/Min Ratio: 2.97x (Higher = More Imbalanced)\n",
      "   ‚Ä¢ Gini Coefficient: 0.711 (0=Perfect Balance, 1=Maximum Imbalance)\n",
      "   ‚Ä¢ Entropy Score: 1.886\n",
      "   ‚úÖ GOOD: Class balance is acceptable for AI modeling\n",
      "\n",
      "üìà NARRATIVE VIABILITY BY PRODUCT CATEGORY:\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ Credit Card           197,126/ 448,335 ( 44.0%) have narratives\n",
      "   ‚Ä¢ Personal Loan          66,276/ 135,172 ( 49.0%) have narratives\n",
      "   ‚Ä¢ Savings Account       155,204/ 377,383 ( 41.1%) have narratives\n",
      "   ‚Ä¢ Money Transfer         97,204/ 145,084 ( 67.0%) have narratives\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìä SECTION 5: CLASS BALANCE & STATISTICAL ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"‚öñÔ∏è PHASE 4: CLASS BALANCE & STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# CRITICAL: Use the filtered business data from Section 4\n",
    "print(\"üìä USING NLP-VIABLE BUSINESS DATA FROM SECTION 4\")\n",
    "print(f\"   ‚Ä¢ Business-relevant complaints: {len(business_df_viable):,}\")\n",
    "print(f\"   ‚Ä¢ Business complaints with narratives: {len(business_df_viable):,}\")\n",
    "\n",
    "# Calculate product distribution for NLP-VIABLE business data\n",
    "product_distribution = business_df_viable['Product_Category'].value_counts()\n",
    "product_percentage = (product_distribution / len(business_df_viable) * 100)\n",
    "\n",
    "print(\"\\nüìä PRODUCT DISTRIBUTION (NLP-Viable Business Data):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for product, count, percent in zip(product_distribution.index, \n",
    "                                   product_distribution.values, \n",
    "                                   product_percentage.values):\n",
    "    severity = \"üö® HIGH\" if percent > 25 else \"‚ö†Ô∏è MEDIUM\" if percent > 15 else \"‚úÖ LOW\"\n",
    "    print(f\"   ‚Ä¢ {product:<20} {count:>8,} complaints ({percent:>5.1f}%) {severity}\")\n",
    "\n",
    "# 1. Class Balance Visualization - DUAL PERSPECTIVE\n",
    "fig1 = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=('All Products (Full Dataset)', \n",
    "                    'Our Products (Full Dataset)',\n",
    "                    'Our Products (NLP-Viable)'),\n",
    "    specs=[[{'type': 'pie'}, {'type': 'pie'}, {'type': 'pie'}]],\n",
    "    column_widths=[0.33, 0.33, 0.34]\n",
    ")\n",
    "\n",
    "# Chart 1: All products in FULL dataset (top 10)\n",
    "all_counts_full = df['Product'].value_counts().head(10)\n",
    "fig1.add_trace(\n",
    "    go.Pie(\n",
    "        labels=all_counts_full.index,\n",
    "        values=all_counts_full.values,\n",
    "        hole=0.3,\n",
    "        name='All Products (Full)',\n",
    "        marker=dict(colors=px.colors.qualitative.Set3),\n",
    "        textinfo='label+percent',\n",
    "        textposition='inside'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Chart 2: Our products in FULL dataset\n",
    "our_products = ['Credit Card', 'Personal Loan', 'Savings Account', 'Money Transfer']\n",
    "business_df_full = df[df['Product_Category'].isin(our_products)]\n",
    "our_counts_full = business_df_full['Product_Category'].value_counts()\n",
    "\n",
    "fig1.add_trace(\n",
    "    go.Pie(\n",
    "        labels=our_counts_full.index,\n",
    "        values=our_counts_full.values,\n",
    "        hole=0.3,\n",
    "        name='Our Products (Full)',\n",
    "        marker=dict(colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']),\n",
    "        textinfo='label+percent',\n",
    "        textposition='inside'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Chart 3: Our products in NLP-VIABLE dataset (FOR AI ANALYSIS)\n",
    "fig1.add_trace(\n",
    "    go.Pie(\n",
    "        labels=product_distribution.index,\n",
    "        values=product_distribution.values,\n",
    "        hole=0.3,\n",
    "        name='Our Products (NLP-Viable)',\n",
    "        marker=dict(colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']),\n",
    "        textinfo='label+percent',\n",
    "        textposition='inside'\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig1.update_layout(\n",
    "    title_text=\"<b>Class Balance Analysis</b><br><i>Comparing Full Dataset vs NLP-Viable Data</i>\",\n",
    "    title_font_size=16,\n",
    "    showlegend=True,\n",
    "    height=500,\n",
    "    annotations=[\n",
    "        dict(text=\"9.6M Total\", x=0.12, y=1.05, xref=\"paper\", yref=\"paper\", showarrow=False, font=dict(size=12)),\n",
    "        dict(text=f\"{len(business_df_full):,} Business\", x=0.5, y=1.05, xref=\"paper\", yref=\"paper\", showarrow=False, font=dict(size=12)),\n",
    "        dict(text=f\"{len(business_df_viable):,} NLP-Viable\", x=0.88, y=1.05, xref=\"paper\", yref=\"paper\", showarrow=False, font=dict(size=12))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create reports directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "fig1.write_html(\"reports/class_balance_analysis.html\")\n",
    "print(\"\\n‚úÖ Saved class balance visualization: reports/class_balance_analysis.html\")\n",
    "\n",
    "# 2. Statistical Imbalance Metrics - FOR NLP-VIABLE DATA\n",
    "print(\"\\nüìä STATISTICAL IMBALANCE ANALYSIS (NLP-Viable Business Data):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if len(product_distribution) > 1:\n",
    "    imbalance_ratio = product_distribution.max() / product_distribution.min()\n",
    "    gini_coefficient = 1 - sum((product_distribution / product_distribution.sum())**2)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Max/Min Ratio: {imbalance_ratio:.2f}x (Higher = More Imbalanced)\")\n",
    "    print(f\"   ‚Ä¢ Gini Coefficient: {gini_coefficient:.3f} (0=Perfect Balance, 1=Maximum Imbalance)\")\n",
    "    print(f\"   ‚Ä¢ Entropy Score: {(-sum((product_distribution/product_distribution.sum()) * np.log2(product_distribution/product_distribution.sum()))):.3f}\")\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(f\"   ‚ö†Ô∏è  WARNING: Severe class imbalance detected (>10x ratio)\")\n",
    "        print(f\"   üí° RECOMMENDATION: Consider stratified sampling or weighted loss in AI model\")\n",
    "    elif imbalance_ratio > 5:\n",
    "        print(f\"   ‚ö†Ô∏è  NOTICE: Moderate class imbalance detected\")\n",
    "        print(f\"   üí° RECOMMENDATION: Monitor performance across all classes\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ GOOD: Class balance is acceptable for AI modeling\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Not enough product categories for imbalance analysis\")\n",
    "\n",
    "# 3. Narrative Viability by Product\n",
    "print(\"\\nüìà NARRATIVE VIABILITY BY PRODUCT CATEGORY:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for product in our_products:\n",
    "    total = len(df[df['Product_Category'] == product])\n",
    "    viable = len(viable_df[viable_df['Product_Category'] == product])\n",
    "    pct = (viable / total * 100) if total > 0 else 0\n",
    "    \n",
    "    print(f\"   ‚Ä¢ {product:<20} {viable:>8,}/{total:>8,} ({pct:>5.1f}%) have narratives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c147027",
   "metadata": {},
   "source": [
    "ADVANCED TEXT ANALYSIS - NLP DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d09027a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üéØ CREATING NLP-VIABLE DATASET FOR TEXT ANALYSIS\n",
      "====================================================================================================\n",
      "‚úÖ Created viable_df: 2,980,756 complaints with narratives\n",
      "   ‚Ä¢ From total dataset of: 9,609,797 complaints\n",
      "   ‚Ä¢ Percentage with narratives: 31.0%\n",
      "\n",
      "üìä Applying product mapping to NLP-viable data...\n",
      "\n",
      "‚úÖ Created business_df_viable: 515,810 complaints\n",
      "   ‚Ä¢ NLP-viable AND business-relevant\n",
      "   ‚Ä¢ Products: Credit Card, Personal Loan, Savings Account, Money Transfer\n",
      "\n",
      "====================================================================================================\n",
      "üéØ READY FOR TEXT ANALYSIS SECTIONS 6-10\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üéØ CRITICAL: CREATE NLP-VIABLE DATASET BEFORE SECTION 6\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üéØ CREATING NLP-VIABLE DATASET FOR TEXT ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# 1. Filter for complaints WITH narratives (31% of data)\n",
    "viable_df = df[df['Consumer complaint narrative'].notna()].copy()\n",
    "print(f\"‚úÖ Created viable_df: {len(viable_df):,} complaints with narratives\")\n",
    "print(f\"   ‚Ä¢ From total dataset of: {len(df):,} complaints\")\n",
    "print(f\"   ‚Ä¢ Percentage with narratives: {len(viable_df)/len(df)*100:.1f}%\")\n",
    "\n",
    "# 2. Apply product mapping to viable_df\n",
    "print(\"\\nüìä Applying product mapping to NLP-viable data...\")\n",
    "product_mapping = {\n",
    "    'Credit card': 'Credit Card',\n",
    "    'Credit card or prepaid card': 'Credit Card',\n",
    "    'Prepaid card': 'Credit Card',\n",
    "    'Payday loan, title loan, or personal loan': 'Personal Loan',\n",
    "    'Consumer Loan': 'Personal Loan',\n",
    "    'Vehicle loan or lease': 'Personal Loan',\n",
    "    'Bank account or service': 'Savings Account',\n",
    "    'Checking or savings account': 'Savings Account',\n",
    "    'Savings account': 'Savings Account',\n",
    "    'Money transfer, virtual currency, or money service': 'Money Transfer',\n",
    "    'Virtual currency': 'Money Transfer',\n",
    "    'Mortgage': 'Mortgage',\n",
    "    'Student loan': 'Student Loan',\n",
    "    'Debt collection': 'Debt Collection',\n",
    "    'Credit reporting, credit repair services, or other personal consumer reports': 'Credit Reporting'\n",
    "}\n",
    "\n",
    "viable_df['Product_Category'] = viable_df['Product'].map(product_mapping).fillna('Other')\n",
    "\n",
    "# 3. Create business_df_viable (NLP-viable AND business-relevant)\n",
    "our_products = ['Credit Card', 'Personal Loan', 'Savings Account', 'Money Transfer']\n",
    "business_df_viable = viable_df[viable_df['Product_Category'].isin(our_products)]\n",
    "\n",
    "print(f\"\\n‚úÖ Created business_df_viable: {len(business_df_viable):,} complaints\")\n",
    "print(f\"   ‚Ä¢ NLP-viable AND business-relevant\")\n",
    "print(f\"   ‚Ä¢ Products: {', '.join(our_products)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üéØ READY FOR TEXT ANALYSIS SECTIONS 6-10\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed153c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üìù PHASE 5: ADVANCED TEXT ANALYSIS - NLP INSIGHTS\n",
      "====================================================================================================\n",
      "üìä Analyzing NLP-Viable Dataset: 2,980,756 complaints with narrative text\n",
      "   (This is 31.0% of the total 9,609,797 complaints)\n",
      "\n",
      "üìè DOCUMENT LENGTH ANALYSIS:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìù SECTION 6: ADVANCED TEXT ANALYSIS - NLP DEPTH\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìù PHASE 5: ADVANCED TEXT ANALYSIS - NLP INSIGHTS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"üìä Analyzing NLP-Viable Dataset: {len(viable_df):,} complaints with narrative text\")\n",
    "print(f\"   (This is {len(viable_df)/len(df)*100:.1f}% of the total {len(df):,} complaints)\")\n",
    "\n",
    "# 1. Document Length Analysis\n",
    "print(\"\\nüìè DOCUMENT LENGTH ANALYSIS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate comprehensive text statistics ON THE VIABLE DATA\n",
    "viable_df['Narrative_Length_Chars'] = viable_df['Consumer complaint narrative'].str.len()\n",
    "viable_df['Narrative_Length_Words'] = viable_df['Consumer complaint narrative'].str.split().str.len()\n",
    "viable_df['Narrative_Length_Sentences'] = viable_df['Consumer complaint narrative'].apply(\n",
    "    lambda x: len(sent_tokenize(str(x))) if pd.notna(x) else 0\n",
    ")\n",
    "\n",
    "# Use viable_df for statistics\n",
    "text_stats = viable_df[['Narrative_Length_Chars', 'Narrative_Length_Words', 'Narrative_Length_Sentences']].describe()\n",
    "\n",
    "print(\"üìà Summary Statistics (for complaints WITH narratives):\")\n",
    "print(text_stats.round(1))\n",
    "\n",
    "# Identify outliers IN THE VIABLE DATA\n",
    "Q1 = viable_df['Narrative_Length_Words'].quantile(0.25)\n",
    "Q3 = viable_df['Narrative_Length_Words'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers = viable_df[(viable_df['Narrative_Length_Words'] < (Q1 - 1.5 * IQR)) | \n",
    "                     (viable_df['Narrative_Length_Words'] > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "print(f\"\\nüìä Outlier Detection (within narratives):\")\n",
    "print(f\"   ‚Ä¢ Short Outliers (< {Q1 - 1.5 * IQR:.0f} words): {len(outliers[outliers['Narrative_Length_Words'] < (Q1 - 1.5 * IQR)])}\")\n",
    "print(f\"   ‚Ä¢ Long Outliers (> {Q3 + 1.5 * IQR:.0f} words): {len(outliers[outliers['Narrative_Length_Words'] > (Q3 + 1.5 * IQR)])}\")\n",
    "print(f\"   ‚Ä¢ Total Outliers: {len(outliers):,} ({len(outliers)/len(viable_df)*100:.1f}% of viable data)\")\n",
    "\n",
    "# 2. Length Distribution Visualization\n",
    "fig2 = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Character Length Distribution', \n",
    "                    'Word Length Distribution',\n",
    "                    'Sentence Length Distribution',\n",
    "                    'Length vs Product Category'),\n",
    "    specs=[[{'type': 'histogram'}, {'type': 'histogram'}],\n",
    "           [{'type': 'histogram'}, {'type': 'box'}]]\n",
    ")\n",
    "\n",
    "# Character length - USE viable_df\n",
    "fig2.add_trace(\n",
    "    go.Histogram(\n",
    "        x=viable_df['Narrative_Length_Chars'].dropna(),\n",
    "        nbinsx=50,\n",
    "        name='Characters',\n",
    "        marker_color='#FF6B6B'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Word length - USE viable_df\n",
    "fig2.add_trace(\n",
    "    go.Histogram(\n",
    "        x=viable_df['Narrative_Length_Words'].dropna(),\n",
    "        nbinsx=50,\n",
    "        name='Words',\n",
    "        marker_color='#4ECDC4'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Sentence length - USE viable_df\n",
    "fig2.add_trace(\n",
    "    go.Histogram(\n",
    "        x=viable_df['Narrative_Length_Sentences'].dropna(),\n",
    "        nbinsx=30,\n",
    "        name='Sentences',\n",
    "        marker_color='#45B7D1'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Box plot by product - USE business_df_viable (which is a subset of viable_df)\n",
    "for product in our_products:\n",
    "    subset = business_df_viable[business_df_viable['Product_Category'] == product]\n",
    "    fig2.add_trace(\n",
    "        go.Box(\n",
    "            y=subset['Narrative_Length_Words'],\n",
    "            name=product,\n",
    "            boxpoints='outliers',\n",
    "            marker_color={'Credit Card': '#FF6B6B', \n",
    "                         'Personal Loan': '#4ECDC4',\n",
    "                         'Savings Account': '#45B7D1',\n",
    "                         'Money Transfer': '#96CEB4'}[product]\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "fig2.update_layout(\n",
    "    title_text=\"<b>Text Length Analysis</b><br><i>Statistical Distribution of NLP-Viable Complaint Narratives</i>\",\n",
    "    title_font_size=18,\n",
    "    height=700,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Create reports directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "fig2.write_html(\"reports/text_length_analysis.html\")\n",
    "print(\"\\n‚úÖ Saved text length analysis visualization: reports/text_length_analysis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üî§ SECTION 7: VOCABULARY & LINGUISTIC ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üî§ PHASE 6: VOCABULARY & LINGUISTIC ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Note: We are analyzing ONLY the viable complaints (with narratives)\n",
    "print(f\"üìä Analyzing vocabulary for {len(business_df_viable):,} business-relevant, NLP-viable complaints\")\n",
    "\n",
    "# Sample for vocabulary analysis (for performance)\n",
    "sample_size = min(10000, len(business_df_viable))\n",
    "sample_df = business_df_viable.sample(sample_size, random_state=42)\n",
    "print(f\"   ‚Ä¢ Using sample of {sample_size:,} complaints for vocabulary analysis\")\n",
    "\n",
    "def analyze_vocabulary(text_series):\n",
    "    \"\"\"Advanced vocabulary analysis\"\"\"\n",
    "    all_words = []\n",
    "    for text in text_series.dropna():\n",
    "        tokens = word_tokenize(str(text).lower())\n",
    "        all_words.extend(tokens)\n",
    "    \n",
    "    word_counts = Counter(all_words)\n",
    "    total_words = len(all_words)\n",
    "    unique_words = len(word_counts)\n",
    "    \n",
    "    return {\n",
    "        'total_words': total_words,\n",
    "        'unique_words': unique_words,\n",
    "        'vocabulary_richness': unique_words / total_words if total_words > 0 else 0,\n",
    "        'top_words': word_counts.most_common(20)\n",
    "    }\n",
    "\n",
    "print(\"\\nüìä VOCABULARY ANALYSIS ACROSS PRODUCTS (NLP-Viable Data):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "vocab_results = {}\n",
    "for product in our_products:\n",
    "    product_texts = business_df_viable[business_df_viable['Product_Category'] == product]['Consumer complaint narrative']\n",
    "    if len(product_texts) > 0:\n",
    "        vocab_results[product] = analyze_vocabulary(product_texts)\n",
    "        \n",
    "        print(f\"\\n{product}:\")\n",
    "        print(f\"   ‚Ä¢ Total Words: {vocab_results[product]['total_words']:,}\")\n",
    "        print(f\"   ‚Ä¢ Unique Words: {vocab_results[product]['unique_words']:,}\")\n",
    "        print(f\"   ‚Ä¢ Vocabulary Richness: {vocab_results[product]['vocabulary_richness']:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Top 5 Words: {[word for word, count in vocab_results[product]['top_words'][:5]]}\")\n",
    "    else:\n",
    "        print(f\"\\n{product}: No narrative data available\")\n",
    "\n",
    "# Calculate vocabulary overlap\n",
    "print(\"\\nüìä VOCABULARY OVERLAP ANALYSIS (NLP-Viable Products):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get unique words per product from NLP-viable data\n",
    "product_vocabs = {}\n",
    "for product in our_products:\n",
    "    all_words = []\n",
    "    product_data = business_df_viable[business_df_viable['Product_Category'] == product]\n",
    "    for text in product_data['Consumer complaint narrative'].dropna():\n",
    "        tokens = word_tokenize(str(text).lower())\n",
    "        all_words.extend(tokens)\n",
    "    product_vocabs[product] = set(all_words)\n",
    "    print(f\"   ‚Ä¢ {product}: {len(product_vocabs[product]):,} unique words\")\n",
    "\n",
    "# Calculate Jaccard similarity between product vocabularies\n",
    "from itertools import combinations\n",
    "\n",
    "overlap_matrix = pd.DataFrame(index=our_products, columns=our_products)\n",
    "\n",
    "for prod1, prod2 in combinations(our_products, 2):\n",
    "    if len(product_vocabs[prod1]) > 0 and len(product_vocabs[prod2]) > 0:\n",
    "        intersection = len(product_vocabs[prod1].intersection(product_vocabs[prod2]))\n",
    "        union = len(product_vocabs[prod1].union(product_vocabs[prod2]))\n",
    "        jaccard_similarity = intersection / union if union > 0 else 0\n",
    "        \n",
    "        overlap_matrix.loc[prod1, prod2] = jaccard_similarity\n",
    "        overlap_matrix.loc[prod2, prod1] = jaccard_similarity\n",
    "    else:\n",
    "        overlap_matrix.loc[prod1, prod2] = 0\n",
    "        overlap_matrix.loc[prod2, prod1] = 0\n",
    "\n",
    "# Fill diagonal\n",
    "for product in our_products:\n",
    "    overlap_matrix.loc[product, product] = 1.0\n",
    "\n",
    "print(\"\\nJaccard Similarity Matrix (Vocabulary Overlap in NLP-Viable Data):\")\n",
    "print(overlap_matrix.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üßπ SECTION 8: ADVANCED TEXT CLEANING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üßπ PHASE 7: ADVANCED TEXT CLEANING PIPELINE\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"üîß Applying text cleaning to {len(business_df_viable):,} NLP-viable business complaints\")\n",
    "\n",
    "class AdvancedTextCleaner:\n",
    "    \"\"\"Production-grade text cleaner with NLP techniques\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        # Add domain-specific stopwords\n",
    "        self.domain_stopwords = {\n",
    "            'bank', 'account', 'card', 'loan', 'company', \n",
    "            'service', 'customer', 'please', 'thank', 'would',\n",
    "            'could', 'should', 'also', 'however', 'therefore'\n",
    "        }\n",
    "        self.stop_words.update(self.domain_stopwords)\n",
    "        \n",
    "        # Regex patterns for noise removal\n",
    "        self.patterns = {\n",
    "            'email': r'\\S+@\\S+',\n",
    "            'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n",
    "            'ssn': r'\\d{3}-\\d{2}-\\d{4}',\n",
    "            'url': r'https?://\\S+|www\\.\\S+',\n",
    "            'account_number': r'account\\s*(?:no|number|#)?\\s*:?\\s*\\d+',\n",
    "            'date': r'\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}',\n",
    "            'currency': r'\\$\\d+(?:\\.\\d{2})?',\n",
    "            'special_chars': r'[^\\w\\s.,!?;:\\-\\'\"]',\n",
    "            'extra_spaces': r'\\s+'\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Complete text cleaning pipeline\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove boilerplate patterns\n",
    "        boilerplate_phrases = [\n",
    "            r'dear\\s+(?:sir|madam|team|customer\\s+service)',\n",
    "            r'to\\s+whom\\s+it\\s+may\\s+concern',\n",
    "            r'i\\s+am\\s+writing\\s+(?:to|because|regarding)',\n",
    "            r'this\\s+is\\s+(?:a|to)\\s+(?:file|submit|report)',\n",
    "            r'please\\s+be\\s+(?:advised|informed|noted)',\n",
    "            r'thank\\s+you\\s+(?:in\\s+advance|for\\s+your\\s+(?:time|help|attention))',\n",
    "            r'sincerely\\s*yours?',\n",
    "            r'best\\s+regards',\n",
    "            r'kind\\s+regards',\n",
    "            r'regards',\n",
    "            r'respectfully',\n",
    "            r'yours\\s+truly'\n",
    "        ]\n",
    "        \n",
    "        for phrase in boilerplate_phrases:\n",
    "            text = re.sub(phrase, '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove structured patterns\n",
    "        for pattern_name, pattern in self.patterns.items():\n",
    "            if pattern_name in ['email', 'phone', 'ssn', 'url', 'account_number']:\n",
    "                text = re.sub(pattern, '[REDACTED]', text)\n",
    "            elif pattern_name == 'special_chars':\n",
    "                text = re.sub(pattern, ' ', text)\n",
    "            elif pattern_name == 'extra_spaces':\n",
    "                text = re.sub(pattern, ' ', text)\n",
    "        \n",
    "        # Tokenize and process\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords (but keep negation words)\n",
    "        negation_words = {'not', 'no', 'never', 'none', 'nothing', 'nowhere'}\n",
    "        filtered_tokens = [\n",
    "            token for token in tokens \n",
    "            if token not in self.stop_words or token in negation_words\n",
    "        ]\n",
    "        \n",
    "        # Apply lemmatization\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "        \n",
    "        # Reconstruct text\n",
    "        cleaned_text = ' '.join(lemmatized_tokens)\n",
    "        \n",
    "        return cleaned_text.strip()\n",
    "    \n",
    "    def analyze_cleaning_impact(self, original_text, cleaned_text):\n",
    "        \"\"\"Analyze cleaning impact\"\"\"\n",
    "        original_words = len(word_tokenize(str(original_text)))\n",
    "        cleaned_words = len(word_tokenize(str(cleaned_text)))\n",
    "        \n",
    "        return {\n",
    "            'original_length': original_words,\n",
    "            'cleaned_length': cleaned_words,\n",
    "            'reduction_percentage': ((original_words - cleaned_words) / original_words * 100) if original_words > 0 else 0,\n",
    "            'stopwords_removed': original_words - cleaned_words\n",
    "        }\n",
    "\n",
    "print(\"üîß Initializing advanced text cleaner...\")\n",
    "cleaner = AdvancedTextCleaner()\n",
    "\n",
    "# Test cleaning pipeline\n",
    "test_cases = [\n",
    "    \"Dear Sir, I am writing to file a complaint about my credit card billing. My account number is 123456789. Please contact me at john@email.com or 555-123-4567. Thank you in advance.\",\n",
    "    \"This is to REPORT a serious issue with my LOAN. The bank charged me $500 extra fees!!! I want this resolved ASAP.\",\n",
    "    \"Not happy with the service. The representative was not helpful at all. Never using this bank again.\"\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ TESTING CLEANING PIPELINE:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, test_case in enumerate(test_cases[:3]):\n",
    "    cleaned = cleaner.clean_text(test_case)\n",
    "    impact = cleaner.analyze_cleaning_impact(test_case, cleaned)\n",
    "    \n",
    "    print(f\"\\nTest Case {i+1}:\")\n",
    "    print(f\"   Original: {test_case[:100]}...\")\n",
    "    print(f\"   Cleaned: {cleaned[:100]}...\")\n",
    "    print(f\"   Impact: {impact['reduction_percentage']:.1f}% reduction \"\n",
    "          f\"({impact['original_length']} ‚Üí {impact['cleaned_length']} words)\")\n",
    "\n",
    "# Apply cleaning to NLP-viable business data\n",
    "print(f\"\\nüöÄ Applying cleaning to {len(business_df_viable):,} NLP-viable business complaints...\")\n",
    "business_df_viable['Cleaned_Narrative'] = business_df_viable['Consumer complaint narrative'].apply(cleaner.clean_text)\n",
    "\n",
    "# Analyze cleaning impact on NLP-viable data\n",
    "original_lengths = business_df_viable['Consumer complaint narrative'].str.split().str.len()\n",
    "cleaned_lengths = business_df_viable['Cleaned_Narrative'].str.split().str.len()\n",
    "\n",
    "cleaning_summary = {\n",
    "    'avg_original_length': original_lengths.mean(),\n",
    "    'avg_cleaned_length': cleaned_lengths.mean(),\n",
    "    'avg_reduction': ((original_lengths - cleaned_lengths) / original_lengths * 100).mean(),\n",
    "    'total_words_removed': (original_lengths - cleaned_lengths).sum()\n",
    "}\n",
    "\n",
    "print(\"\\nüìä CLEANING IMPACT SUMMARY (NLP-Viable Business Data):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   ‚Ä¢ Average Original Length: {cleaning_summary['avg_original_length']:.1f} words\")\n",
    "print(f\"   ‚Ä¢ Average Cleaned Length: {cleaning_summary['avg_cleaned_length']:.1f} words\")\n",
    "print(f\"   ‚Ä¢ Average Reduction: {cleaning_summary['avg_reduction']:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Total Words Removed: {cleaning_summary['total_words_removed']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üìä SECTION 9: SENTIMENT & TOPIC ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üé≠ PHASE 8: SENTIMENT & TOPIC ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# 1. Sentiment Analysis on CLEANED NLP-viable narratives\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Basic sentiment analysis\"\"\"\n",
    "    if pd.isna(text) or len(str(text).strip()) < 10:\n",
    "        return 0.0\n",
    "    \n",
    "    analysis = TextBlob(str(text))\n",
    "    return analysis.sentiment.polarity  # -1 to 1\n",
    "\n",
    "print(\"üìà Calculating sentiment scores for cleaned NLP-viable narratives...\")\n",
    "business_df_viable['Sentiment_Score'] = business_df_viable['Cleaned_Narrative'].apply(analyze_sentiment)\n",
    "\n",
    "# Sentiment distribution by product\n",
    "sentiment_by_product = business_df_viable.groupby('Product_Category')['Sentiment_Score'].agg(['mean', 'std', 'count'])\n",
    "\n",
    "print(\"\\nüìä SENTIMENT ANALYSIS BY PRODUCT (NLP-Viable Business Data):\")\n",
    "print(\"-\" * 80)\n",
    "for product in our_products:\n",
    "    if product in sentiment_by_product.index:\n",
    "        mean_sentiment = sentiment_by_product.loc[product, 'mean']\n",
    "        if mean_sentiment < -0.1:\n",
    "            sentiment_label = \"üò† NEGATIVE\"\n",
    "        elif mean_sentiment < 0.1:\n",
    "            sentiment_label = \"üòê NEUTRAL\"\n",
    "        else:\n",
    "            sentiment_label = \"üòä POSITIVE\"\n",
    "        print(f\"   ‚Ä¢ {product:<20} {mean_sentiment:>6.3f} {sentiment_label} (n={sentiment_by_product.loc[product, 'count']:,})\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ {product:<20} No data available\")\n",
    "\n",
    "# 2. Topic/Issue Analysis\n",
    "print(\"\\nüìä TOP ISSUES BY PRODUCT CATEGORY (NLP-Viable Data):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get top issues for each product\n",
    "for product in our_products:\n",
    "    product_data = business_df_viable[business_df_viable['Product_Category'] == product]\n",
    "    if len(product_data) > 0:\n",
    "        top_issues = product_data['Issue'].value_counts().head(3)\n",
    "        \n",
    "        print(f\"\\n{product} (n={len(product_data):,}):\")\n",
    "        for issue, count in top_issues.items():\n",
    "            percentage = (count / len(product_data)) * 100\n",
    "            print(f\"   ‚Ä¢ {issue}: {count:,} complaints ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"\\n{product}: No NLP-viable data available\")\n",
    "\n",
    "# 3. Sentiment distribution visualization\n",
    "print(\"\\nüìà SENTIMENT DISTRIBUTION SUMMARY:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Categorize sentiments\n",
    "def categorize_sentiment(score):\n",
    "    if score < -0.1:\n",
    "        return \"Negative\"\n",
    "    elif score < 0.1:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\"\n",
    "\n",
    "business_df_viable['Sentiment_Category'] = business_df_viable['Sentiment_Score'].apply(categorize_sentiment)\n",
    "sentiment_dist = business_df_viable['Sentiment_Category'].value_counts()\n",
    "\n",
    "for sentiment, count in sentiment_dist.items():\n",
    "    percentage = (count / len(business_df_viable)) * 100\n",
    "    print(f\"   ‚Ä¢ {sentiment:<10} {count:>8,} complaints ({percentage:>5.1f}%)\")\n",
    "\n",
    "# 4. Issue-Sentiment correlation\n",
    "print(\"\\nüìä MOST NEGATIVE ISSUES (Top 5 by Average Sentiment):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'Issue' in business_df_viable.columns:\n",
    "    issue_sentiment = business_df_viable.groupby('Issue')['Sentiment_Score'].agg(['mean', 'count'])\n",
    "    # Filter for issues with at least 100 complaints\n",
    "    issue_sentiment = issue_sentiment[issue_sentiment['count'] >= 100]\n",
    "    most_negative = issue_sentiment.sort_values('mean').head(5)\n",
    "    \n",
    "    for issue, row in most_negative.iterrows():\n",
    "        print(f\"   ‚Ä¢ {issue}: {row['mean']:.3f} sentiment (n={row['count']:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b7de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üìà SECTION 10: TF-IDF & KEYWORD ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üîë PHASE 9: TF-IDF & KEYWORD ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"üìä Performing TF-IDF analysis on {len(business_df_viable):,} cleaned NLP-viable narratives\")\n",
    "\n",
    "# Sample data for TF-IDF (for performance)\n",
    "sample_size_tfidf = min(5000, len(business_df_viable))\n",
    "tfidf_sample = business_df_viable.sample(sample_size_tfidf, random_state=42)\n",
    "print(f\"   ‚Ä¢ Using sample of {sample_size_tfidf:,} complaints for TF-IDF analysis\")\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),  # Include bigrams\n",
    "    min_df=5,  # Minimum document frequency\n",
    "    max_df=0.8  # Maximum document frequency\n",
    ")\n",
    "\n",
    "# Fit and transform on CLEANED narratives\n",
    "try:\n",
    "    tfidf_matrix = tfidf.fit_transform(tfidf_sample['Cleaned_Narrative'])\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    \n",
    "    print(f\"‚úÖ TF-IDF matrix created: {tfidf_matrix.shape[0]} documents √ó {tfidf_matrix.shape[1]} features\")\n",
    "    \n",
    "    # Get top keywords for each product\n",
    "    print(\"\\nüîç TOP KEYWORDS BY PRODUCT (TF-IDF on Cleaned NLP-Viable Data):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for product in our_products:\n",
    "        product_mask = tfidf_sample['Product_Category'] == product\n",
    "        \n",
    "        if product_mask.sum() > 0:\n",
    "            # Calculate average TF-IDF for this product\n",
    "            product_tfidf = tfidf_matrix[product_mask].mean(axis=0).A1\n",
    "            top_indices = product_tfidf.argsort()[-10:][::-1]\n",
    "            top_keywords = [feature_names[i] for i in top_indices]\n",
    "            \n",
    "            print(f\"\\n{product} (n={product_mask.sum():,}):\")\n",
    "            print(f\"   ‚Ä¢ Top Keywords: {', '.join(top_keywords[:5])}\")\n",
    "            print(f\"   ‚Ä¢ All Top 10: {', '.join(top_keywords)}\")\n",
    "        else:\n",
    "            print(f\"\\n{product}: No data in sample\")\n",
    "    \n",
    "    # Get overall top keywords\n",
    "    print(\"\\nüîç OVERALL TOP KEYWORDS (All NLP-Viable Business Data):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    overall_tfidf = tfidf_matrix.mean(axis=0).A1\n",
    "    top_indices = overall_tfidf.argsort()[-20:][::-1]\n",
    "    top_keywords = [feature_names[i] for i in top_indices]\n",
    "    \n",
    "    print(\"Top 20 Keywords by TF-IDF Score:\")\n",
    "    for i in range(0, len(top_keywords), 5):\n",
    "        print(f\"   ‚Ä¢ {', '.join(top_keywords[i:i+5])}\")\n",
    "    \n",
    "    # Analyze keyword uniqueness by product\n",
    "    print(\"\\nüìä KEYWORD UNIQUENESS ANALYSIS:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    product_keywords = {}\n",
    "    for product in our_products:\n",
    "        product_mask = tfidf_sample['Product_Category'] == product\n",
    "        if product_mask.sum() > 10:  # Need enough documents\n",
    "            product_tfidf = tfidf_matrix[product_mask].mean(axis=0).A1\n",
    "            # Get keywords where this product has score > 0.1 and others < 0.05\n",
    "            other_products_mask = tfidf_sample['Product_Category'] != product\n",
    "            other_tfidf = tfidf_matrix[other_products_mask].mean(axis=0).A1\n",
    "            \n",
    "            unique_indices = np.where((product_tfidf > 0.1) & (other_tfidf < 0.05))[0]\n",
    "            unique_keywords = [feature_names[i] for i in unique_indices[:5]]  # Top 5 unique\n",
    "            \n",
    "            if len(unique_keywords) > 0:\n",
    "                print(f\"   ‚Ä¢ {product}: {', '.join(unique_keywords)}\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ {product}: No strongly unique keywords\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error in TF-IDF analysis: {e}\")\n",
    "    print(\"   This can happen if there's insufficient text data after cleaning.\")\n",
    "    print(\"   Try reducing min_df parameter or checking cleaned text quality.\")\n",
    "\n",
    "# Additional keyword analysis using frequency\n",
    "print(\"\\nüìä FREQUENCY-BASED KEYWORD ANALYSIS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Analyze most common words in cleaned narratives\n",
    "all_words = []\n",
    "for text in business_df_viable['Cleaned_Narrative'].dropna():\n",
    "    tokens = word_tokenize(str(text))\n",
    "    all_words.extend(tokens)\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "print(f\"Total words in cleaned narratives: {len(all_words):,}\")\n",
    "print(f\"Unique words: {len(word_freq):,}\")\n",
    "\n",
    "print(\"\\nMost Common Words (excluding stopwords):\")\n",
    "common_words = [(word, count) for word, count in word_freq.most_common(30) \n",
    "                if word not in cleaner.stop_words and len(word) > 2]\n",
    "for i in range(0, len(common_words), 5):\n",
    "    words_batch = common_words[i:i+5]\n",
    "    print(f\"   ‚Ä¢ {', '.join([f'{w}({c:,})' for w, c in words_batch])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad98eb41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2610bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
