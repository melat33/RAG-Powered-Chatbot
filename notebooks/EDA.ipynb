{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ddaf0ce",
   "metadata": {},
   "source": [
    " EDA FOR CREDITRUST FINANCIAL\n",
    " ML Engineer Analysis - Customer Complaint Intelligence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80eec5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\G5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\G5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\G5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading omw-eng: Package 'omw-eng' not found in\n",
      "[nltk_data]     index\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üì¶ SECTION 1: EXECUTIVE SETUP & BUSINESS CONTEXT\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Professional imports for advanced NLP\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-eng')\n",
    "\n",
    "# Set professional aesthetics\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3570cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: d:\\10 acadamy\\Intelligent Complaint Analysis for Financial Services\\data\\raw\\complaints.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()  # This is 'd:/10 acadamy/Intelligent Complaint Analysis for Financial Services/notebokks'\n",
    "\n",
    "# Go up one level to project root, then navigate to data/raw\n",
    "project_root = os.path.dirname(current_dir)  # Goes up one level\n",
    "data_path = os.path.join(project_root, 'data', 'raw', 'complaints.csv')\n",
    "\n",
    "print(f\"Loading data from: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcec0fe",
   "metadata": {},
   "source": [
    "DATA LOADING WITH MEMORY OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01ce5cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üì¶ PHASE 1: DATA ACQUISITION & INITIAL ASSESSMENT\n",
      "====================================================================================================\n",
      "‚úÖ Correct data path calculated: d:\\10 acadamy\\Intelligent Complaint Analysis for Financial Services\\data\\raw\\complaints.csv\n",
      "üöÄ Loading 464K+ complaint database...\n",
      "   üìä Chunk 5: 50,000 records loaded\n",
      "   üìä Chunk 10: 50,000 records loaded\n",
      "   üìä Chunk 15: 50,000 records loaded\n",
      "   üìä Chunk 20: 50,000 records loaded\n",
      "   üìä Chunk 25: 50,000 records loaded\n",
      "   üìä Chunk 30: 50,000 records loaded\n",
      "   üìä Chunk 35: 50,000 records loaded\n",
      "   üìä Chunk 40: 50,000 records loaded\n",
      "   üìä Chunk 45: 50,000 records loaded\n",
      "   üìä Chunk 50: 50,000 records loaded\n",
      "   üìä Chunk 55: 50,000 records loaded\n",
      "   üìä Chunk 60: 50,000 records loaded\n",
      "   üìä Chunk 65: 50,000 records loaded\n",
      "   üìä Chunk 70: 50,000 records loaded\n",
      "   üìä Chunk 75: 50,000 records loaded\n",
      "   üìä Chunk 80: 50,000 records loaded\n",
      "   üìä Chunk 85: 50,000 records loaded\n",
      "   üìä Chunk 90: 50,000 records loaded\n",
      "   üìä Chunk 95: 50,000 records loaded\n",
      "   üìä Chunk 100: 50,000 records loaded\n",
      "   üìä Chunk 105: 50,000 records loaded\n",
      "   üìä Chunk 110: 50,000 records loaded\n",
      "   üìä Chunk 115: 50,000 records loaded\n",
      "   üìä Chunk 120: 50,000 records loaded\n",
      "   üìä Chunk 125: 50,000 records loaded\n",
      "   üìä Chunk 130: 50,000 records loaded\n",
      "   üìä Chunk 135: 50,000 records loaded\n",
      "   üìä Chunk 140: 50,000 records loaded\n",
      "   üìä Chunk 145: 50,000 records loaded\n",
      "   üìä Chunk 150: 50,000 records loaded\n",
      "   üìä Chunk 155: 50,000 records loaded\n",
      "   üìä Chunk 160: 50,000 records loaded\n",
      "   üìä Chunk 165: 50,000 records loaded\n",
      "   üìä Chunk 170: 50,000 records loaded\n",
      "   üìä Chunk 175: 50,000 records loaded\n",
      "   üìä Chunk 180: 50,000 records loaded\n",
      "   üìä Chunk 185: 50,000 records loaded\n",
      "   üìä Chunk 190: 50,000 records loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìà SECTION 2: DATA LOADING WITH MEMORY OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üì¶ PHASE 1: DATA ACQUISITION & INITIAL ASSESSMENT\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Get the correct path to your data\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.dirname(current_dir)\n",
    "data_path = os.path.join(project_root, 'data', 'raw', 'complaints.csv')\n",
    "\n",
    "print(f\"‚úÖ Correct data path calculated: {data_path}\")\n",
    "\n",
    "# Optimized data types for memory efficiency\n",
    "dtype_strategy = {\n",
    "    'Complaint ID': 'str',\n",
    "    'Date received': 'str',\n",
    "    'Product': 'category',\n",
    "    'Sub-product': 'category',\n",
    "    'Issue': 'category',\n",
    "    'Sub-issue': 'category',\n",
    "    'Company': 'category',\n",
    "    'State': 'category',\n",
    "    'ZIP code': 'str',\n",
    "    'Tags': 'category',\n",
    "    'Consumer consent provided?': 'category',\n",
    "    'Submitted via': 'category',\n",
    "    'Company response to consumer': 'category',\n",
    "    'Timely response?': 'category',\n",
    "    'Consumer disputed?': 'category',\n",
    "    'Consumer complaint narrative': 'object'\n",
    "}\n",
    "\n",
    "# Load data in chunks\n",
    "print(\"üöÄ Loading 464K+ complaint database...\")\n",
    "chunks = []\n",
    "chunk_size = 50000\n",
    "\n",
    "# CRITICAL: Use data_path variable here, not the hardcoded string\n",
    "for i, chunk in enumerate(pd.read_csv(data_path,\n",
    "                                       dtype=dtype_strategy,\n",
    "                                       chunksize=chunk_size,\n",
    "                                       parse_dates=['Date received'],\n",
    "                                       infer_datetime_format=True)):\n",
    "    chunks.append(chunk)\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"   üìä Chunk {i+1}: {len(chunk):,} records loaded\")\n",
    "    \n",
    "df = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b92d02e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ DATA LOADED SUCCESSFULLY\n",
      "   Total Records: 9,609,797\n",
      "   Total Features: 18\n",
      "   Memory Usage: 12188.49 MB\n",
      "   Time Range: 2011-12-01 to 2025-06-23\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n‚úÖ DATA LOADED SUCCESSFULLY\")\n",
    "print(f\"   Total Records: {df.shape[0]:,}\")\n",
    "print(f\"   Total Features: {df.shape[1]}\")\n",
    "print(f\"   Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"   Time Range: {df['Date received'].min().date()} to {df['Date received'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4dbcf1",
   "metadata": {},
   "source": [
    "EXECUTIVE DATA QUALITY DASHBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47dc5daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üîç PHASE 2: DATA QUALITY ASSESSMENT\n",
      "====================================================================================================\n",
      "üìä DATAFRAME SHAPE: (9609797, 18)\n",
      "   ‚Ä¢ Total Rows: 9,609,797\n",
      "   ‚Ä¢ Total Columns: 18\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîç MISSING VALUES ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìã Total missing cells in dataset: 32,030,923\n",
      "\n",
      "üìä TOP 10 COLUMNS WITH MISSING VALUES:\n",
      "--------------------------------------------------\n",
      "                              Missing_Count  Missing_Percentage\n",
      "Tags                                8981029           93.457011\n",
      "Consumer disputed?                  8841498           92.005044\n",
      "Consumer complaint narrative        6629041           68.982113\n",
      "Company public response             4770207           49.638999\n",
      "Consumer consent provided?          1649561           17.165409\n",
      "Sub-issue                            839522            8.736105\n",
      "Sub-product                          235295            2.448491\n",
      "State                                 54516            0.567296\n",
      "ZIP code                              30228            0.314554\n",
      "Company response to consumer             20            0.000208\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL FIELD - Consumer Complaint Narrative:\n",
      "   ‚Ä¢ Missing narratives: 6,629,041\n",
      "   ‚Ä¢ Percentage missing: 69.0%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üìà MISSING VALUES HEATMAP PREVIEW\n",
      "--------------------------------------------------------------------------------\n",
      "(Showing heatmap for 10,000 sample rows)\n",
      "\n",
      "üìä COLUMNS WITH > 0% MISSING VALUES:\n",
      "   ‚Ä¢ Tags: 93.7% missing (9,370 rows)\n",
      "   ‚Ä¢ Consumer disputed?: 92.0% missing (9,202 rows)\n",
      "   ‚Ä¢ Consumer complaint narrative: 68.9% missing (6,886 rows)\n",
      "   ‚Ä¢ Company public response: 49.2% missing (4,924 rows)\n",
      "   ‚Ä¢ Consumer consent provided?: 16.8% missing (1,678 rows)\n",
      "   ‚Ä¢ Sub-issue: 8.7% missing (867 rows)\n",
      "   ‚Ä¢ Sub-product: 2.7% missing (270 rows)\n",
      "   ‚Ä¢ State: 0.5% missing (51 rows)\n",
      "   ‚Ä¢ ZIP code: 0.2% missing (25 rows)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìä SECTION 3: EXECUTIVE DATA QUALITY DASHBOARD\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üîç PHASE 2: DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# First, display the DataFrame shape\n",
    "print(f\"üìä DATAFRAME SHAPE: {df.shape}\")\n",
    "print(f\"   ‚Ä¢ Total Rows: {df.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Total Columns: {df.shape[1]}\")\n",
    "\n",
    "# Create comprehensive data quality report\n",
    "quality_metrics = {}\n",
    "\n",
    "# 1. Missing Values Analysis\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"üîç MISSING VALUES ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "\n",
    "# Display missing values summary\n",
    "print(f\"\\nüìã Total missing cells in dataset: {missing_data.sum():,}\")\n",
    "\n",
    "# Display top 10 columns with most missing values\n",
    "print(\"\\nüìä TOP 10 COLUMNS WITH MISSING VALUES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a DataFrame for better display\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percentage': missing_percentage\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "# Display top 10\n",
    "print(missing_df.head(10).to_string())\n",
    "\n",
    "quality_metrics['missing_values'] = {\n",
    "    'total_missing_cells': missing_data.sum(),\n",
    "    'missing_percentage_overall': (missing_data.sum() / (df.shape[0] * df.shape[1]) * 100),\n",
    "    'critical_missing_narratives': missing_data['Consumer complaint narrative'],\n",
    "    'critical_missing_percentage': missing_percentage['Consumer complaint narrative']\n",
    "}\n",
    "\n",
    "# Display the critical narrative missing info\n",
    "print(f\"\\n‚ö†Ô∏è  CRITICAL FIELD - Consumer Complaint Narrative:\")\n",
    "print(f\"   ‚Ä¢ Missing narratives: {quality_metrics['missing_values']['critical_missing_narratives']:,}\")\n",
    "print(f\"   ‚Ä¢ Percentage missing: {quality_metrics['missing_values']['critical_missing_percentage']:.1f}%\")\n",
    "\n",
    "# 2. Visualize missing values\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"üìà MISSING VALUES HEATMAP PREVIEW\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# For large datasets, sample to create visualization\n",
    "if len(df) > 10000:\n",
    "    sample_size = min(10000, len(df))\n",
    "    missing_sample = df.sample(sample_size).isnull()\n",
    "    print(f\"(Showing heatmap for {sample_size:,} sample rows)\")\n",
    "else:\n",
    "    missing_sample = df.isnull()\n",
    "\n",
    "# Calculate percentage of missing per column\n",
    "missing_summary = missing_sample.sum().sort_values(ascending=False)\n",
    "missing_pct = (missing_summary / len(missing_sample)) * 100\n",
    "\n",
    "print(\"\\nüìä COLUMNS WITH > 0% MISSING VALUES:\")\n",
    "for col in missing_pct[missing_pct > 0].index:\n",
    "    print(f\"   ‚Ä¢ {col}: {missing_pct[col]:.1f}% missing ({missing_summary[col]:,} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d8aa359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã DATA QUALITY METRICS:\n",
      "--------------------------------------------------------------------------------\n",
      "1Ô∏è‚É£  Completeness:\n",
      "   ‚Ä¢ Narratives Missing: 6,629,041 (69.0%)\n",
      "   ‚Ä¢ Overall Data Completeness: 81.5%\n",
      "\n",
      "2Ô∏è‚É£  Uniqueness:\n",
      "   ‚Ä¢ Duplicate Complaints: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìã DATA QUALITY METRICS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"1Ô∏è‚É£  Completeness:\")\n",
    "print(f\"   ‚Ä¢ Narratives Missing: {quality_metrics['missing_values']['critical_missing_narratives']:,} \"\n",
    "      f\"({quality_metrics['missing_values']['critical_missing_percentage']:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Overall Data Completeness: {(100 - quality_metrics['missing_values']['missing_percentage_overall']):.1f}%\")\n",
    "\n",
    "# 2. Duplicate Analysis\n",
    "duplicate_count = df.duplicated(subset=['Complaint ID']).sum()\n",
    "quality_metrics['duplicates'] = {\n",
    "    'total_duplicates': duplicate_count,\n",
    "    'duplicate_percentage': (duplicate_count / len(df)) * 100\n",
    "}\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  Uniqueness:\")\n",
    "print(f\"   ‚Ä¢ Duplicate Complaints: {duplicate_count:,} \"\n",
    "      f\"({quality_metrics['duplicates']['duplicate_percentage']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ded17b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£  Temporal Coverage:\n",
      "   ‚Ä¢ Time Period: 2011-12-01 to 2025-06-23\n",
      "   ‚Ä¢ Total Days: 4,953 days\n",
      "   ‚Ä¢ Average Complaints/Day: 1940.2\n"
     ]
    }
   ],
   "source": [
    "# 3. Temporal Coverage\n",
    "date_range_days = (df['Date received'].max() - df['Date received'].min()).days\n",
    "quality_metrics['temporal'] = {\n",
    "    'date_range_days': date_range_days,\n",
    "    'complaints_per_day': len(df) / date_range_days,\n",
    "    'start_date': df['Date received'].min().date(),\n",
    "    'end_date': df['Date received'].max().date()\n",
    "}\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Temporal Coverage:\")\n",
    "print(f\"   ‚Ä¢ Time Period: {quality_metrics['temporal']['start_date']} to {quality_metrics['temporal']['end_date']}\")\n",
    "print(f\"   ‚Ä¢ Total Days: {date_range_days:,} days\")\n",
    "print(f\"   ‚Ä¢ Average Complaints/Day: {quality_metrics['temporal']['complaints_per_day']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2535b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4Ô∏è‚É£  Data Diversity:\n",
      "   ‚Ä¢ Unique Products: 21\n",
      "   ‚Ä¢ Unique Companies: 7,674\n",
      "   ‚Ä¢ Unique Issues: 178\n",
      "   ‚Ä¢ States Covered: 63\n"
     ]
    }
   ],
   "source": [
    "# 4. Cardinality Analysis\n",
    "quality_metrics['cardinality'] = {\n",
    "    'unique_products': df['Product'].nunique(),\n",
    "    'unique_companies': df['Company'].nunique(),\n",
    "    'unique_states': df['State'].nunique(),\n",
    "    'unique_issues': df['Issue'].nunique()\n",
    "}\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£  Data Diversity:\")\n",
    "print(f\"   ‚Ä¢ Unique Products: {quality_metrics['cardinality']['unique_products']}\")\n",
    "print(f\"   ‚Ä¢ Unique Companies: {quality_metrics['cardinality']['unique_companies']:,}\")\n",
    "print(f\"   ‚Ä¢ Unique Issues: {quality_metrics['cardinality']['unique_issues']}\")\n",
    "print(f\"   ‚Ä¢ States Covered: {quality_metrics['cardinality']['unique_states']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e1bd817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üìä VISUALIZATION 1: DATA QUALITY DASHBOARD\n",
      "====================================================================================================\n",
      "üé® Creating professional data quality visualizations...\n",
      "‚úÖ Saved: Missing Values Heatmap\n",
      "‚úÖ Saved: Missing Percentages Bar Chart\n",
      "‚úÖ Saved: Data Completeness Gauge\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìà VISUALIZATION 1: DATA QUALITY DASHBOARD\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìä VISUALIZATION 1: DATA QUALITY DASHBOARD\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"üé® Creating professional data quality visualizations...\")\n",
    "\n",
    "# 1. Missing Values Heatmap\n",
    "fig_missing = go.Figure(data=go.Heatmap(\n",
    "    z=df.isnull().astype(int).head(1000).T,  # First 1000 rows\n",
    "    colorscale=['#2E86AB', '#A23B72'],  # Blue for present, Pink for missing\n",
    "    showscale=True,\n",
    "    y=df.columns.tolist(),\n",
    "    x=list(range(min(1000, len(df)))),\n",
    "    hovertemplate='Column: %{y}<br>Row: %{x}<br>Missing: %{z}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig_missing.update_layout(\n",
    "    title=\"<b>Missing Values Heatmap</b><br><i>First 1,000 Complaints</i>\",\n",
    "    title_font_size=16,\n",
    "    height=500,\n",
    "    xaxis_title=\"Complaint Index\",\n",
    "    yaxis_title=\"Columns\",\n",
    "    margin=dict(l=100, r=50, t=80, b=50)\n",
    ")\n",
    "\n",
    "fig_missing.write_html(\"reports/missing_values_heatmap.html\")\n",
    "print(\"‚úÖ Saved: Missing Values Heatmap\")\n",
    "\n",
    "# 2. Missing Percentage Bar Chart\n",
    "missing_percentages = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "missing_percentages_top = missing_percentages[missing_percentages > 0].head(10)\n",
    "\n",
    "fig_missing_bars = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=missing_percentages_top.values,\n",
    "        y=missing_percentages_top.index,\n",
    "        orientation='h',\n",
    "        marker_color=['#A23B72' if 'narrative' in str(col).lower() else '#F18F01' for col in missing_percentages_top.index],\n",
    "        text=[f'{val:.1f}%' for val in missing_percentages_top.values],\n",
    "        textposition='auto',\n",
    "        hovertemplate='%{y}<br>Missing: %{x:.1f}%<extra></extra>'\n",
    "    )\n",
    "])\n",
    "\n",
    "fig_missing_bars.update_layout(\n",
    "    title=\"<b>Top 10 Columns with Missing Values</b>\",\n",
    "    title_font_size=16,\n",
    "    height=400,\n",
    "    xaxis_title=\"Percentage Missing\",\n",
    "    yaxis_title=\"Columns\",\n",
    "    margin=dict(l=150, r=50, t=80, b=50)\n",
    ")\n",
    "\n",
    "fig_missing_bars.write_html(\"reports/missing_percentages_bars.html\")\n",
    "print(\"‚úÖ Saved: Missing Percentages Bar Chart\")\n",
    "\n",
    "# 3. Data Completeness Gauge\n",
    "completeness_score = 100 - quality_metrics['missing_values']['missing_percentage_overall']\n",
    "\n",
    "fig_completeness = go.Figure(go.Indicator(\n",
    "    mode=\"gauge+number\",\n",
    "    value=completeness_score,\n",
    "    title={'text': \"Overall Data Completeness\"},\n",
    "    domain={'x': [0, 1], 'y': [0, 1]},\n",
    "    gauge={\n",
    "        'axis': {'range': [0, 100]},\n",
    "        'bar': {'color': \"#2E86AB\"},\n",
    "        'steps': [\n",
    "            {'range': [0, 60], 'color': \"#A23B72\"},\n",
    "            {'range': [60, 80], 'color': \"#F18F01\"},\n",
    "            {'range': [80, 100], 'color': \"#73AB84\"}\n",
    "        ],\n",
    "        'threshold': {\n",
    "            'line': {'color': \"red\", 'width': 4},\n",
    "            'thickness': 0.75,\n",
    "            'value': 80\n",
    "        }\n",
    "    }\n",
    "))\n",
    "\n",
    "fig_completeness.update_layout(\n",
    "    title=\"<b>Data Quality Score</b>\",\n",
    "    title_font_size=16,\n",
    "    height=400,\n",
    "    margin=dict(l=50, r=50, t=80, b=50)\n",
    ")\n",
    "\n",
    "fig_completeness.write_html(\"reports/data_completeness_gauge.html\")\n",
    "print(\"‚úÖ Saved: Data Completeness Gauge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb1ce3e",
   "metadata": {},
   "source": [
    "ADVANCED PRODUCT ANALYSIS - BUSINESS FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89a95ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üéØ PHASE 3: PRODUCT ANALYSIS - CREDITRUST BUSINESS MAPPING\n",
      "====================================================================================================\n",
      "‚ö†Ô∏è  APPLYING NLP-VIABILITY FILTER (69% of data lacks narratives)\n",
      "   ‚Ä¢ Original dataset: 9,609,797 complaints\n",
      "   ‚Ä¢ NLP-viable dataset: 2,980,756 complaints (31.0%)\n",
      "\n",
      "üìä BUSINESS-RELEVANT COMPLAINT DISTRIBUTION:\n",
      "--------------------------------------------------------------------------------\n",
      "üìà OVERALL TRENDS (All 9.6M complaints):\n",
      "   ‚Ä¢ Total Complaints in Database: 9,609,797\n",
      "   ‚Ä¢ Complaints Relevant to CrediTrust: 1,105,974 (11.5%)\n",
      "\n",
      "üéØ NLP-ANALYZABLE DATA (3.0M with narratives):\n",
      "   ‚Ä¢ NLP-viable Complaints: 2,980,756\n",
      "   ‚Ä¢ Business-relevant & NLP-viable: 515,810 (17.3% of viable data)\n",
      "\n",
      "üìä PRODUCT-WISE BREAKDOWN:\n",
      "--------------------------------------------------------------------------------\n",
      "Product                     Total   NLP-Viable   Viable %\n",
      "   ‚Ä¢ Credit Card               448,335      197,126      44.0% ‚ö†Ô∏è MEDIUM\n",
      "   ‚Ä¢ Personal Loan             135,172       66,276      49.0% ‚ö†Ô∏è MEDIUM\n",
      "   ‚Ä¢ Savings Account           377,383      155,204      41.1% ‚ö†Ô∏è MEDIUM\n",
      "   ‚Ä¢ Money Transfer            145,084       97,204      67.0% ‚úÖ HIGH\n",
      "\n",
      "üìà KEY BUSINESS INSIGHT:\n",
      "   ‚Ä¢ Only 46.6% of business-relevant complaints have analyzable narratives\n",
      "   ‚Ä¢ For NLP/AI analysis, focus on 515,810 complaints\n",
      "   ‚Ä¢ 590,164 business complaints cannot be text-analyzed\n",
      "\n",
      "üìã SUMMARY DATAFRAME:\n",
      "           Product  Total_Complaints  NLP_Viable  Viable_Pct  Missing_Narratives\n",
      "0      Credit Card            448335      197126   43.968461              251209\n",
      "1    Personal Loan            135172       66276   49.030864               68896\n",
      "2  Savings Account            377383      155204   41.126389              222179\n",
      "3   Money Transfer            145084       97204   66.998428               47880\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìà SECTION 4: ADVANCED PRODUCT ANALYSIS - BUSINESS FOCUS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üéØ PHASE 3: PRODUCT ANALYSIS - CREDITRUST BUSINESS MAPPING\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# CRITICAL: First filter for NLP-viable data\n",
    "print(\"‚ö†Ô∏è  APPLYING NLP-VIABILITY FILTER (69% of data lacks narratives)\")\n",
    "viable_df = df[df['Consumer complaint narrative'].notna()].copy()\n",
    "print(f\"   ‚Ä¢ Original dataset: {len(df):,} complaints\")\n",
    "print(f\"   ‚Ä¢ NLP-viable dataset: {len(viable_df):,} complaints ({len(viable_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Create business-focused product mapping\n",
    "product_mapping = {\n",
    "    # Credit Cards (Our Core Product)\n",
    "    'Credit card': 'Credit Card',\n",
    "    'Credit card or prepaid card': 'Credit Card',\n",
    "    'Prepaid card': 'Credit Card',\n",
    "    \n",
    "    # Personal Loans (Our Product)\n",
    "    'Payday loan, title loan, or personal loan': 'Personal Loan',\n",
    "    'Consumer Loan': 'Personal Loan',\n",
    "    'Vehicle loan or lease': 'Personal Loan',\n",
    "    \n",
    "    # Savings Accounts (Our Product)\n",
    "    'Bank account or service': 'Savings Account',\n",
    "    'Checking or savings account': 'Savings Account',\n",
    "    'Savings account': 'Savings Account',\n",
    "    \n",
    "    # Money Transfers (Our Product)\n",
    "    'Money transfer, virtual currency, or money service': 'Money Transfer',\n",
    "    'Virtual currency': 'Money Transfer',\n",
    "    \n",
    "    # Other categories for context\n",
    "    'Mortgage': 'Mortgage',\n",
    "    'Student loan': 'Student Loan',\n",
    "    'Debt collection': 'Debt Collection',\n",
    "    'Credit reporting, credit repair services, or other personal consumer reports': 'Credit Reporting'\n",
    "}\n",
    "\n",
    "# Apply mapping to BOTH datasets\n",
    "df['Product_Category'] = df['Product'].map(product_mapping).fillna('Other')\n",
    "viable_df['Product_Category'] = viable_df['Product'].map(product_mapping).fillna('Other')\n",
    "\n",
    "# Business Impact Analysis\n",
    "print(\"\\nüìä BUSINESS-RELEVANT COMPLAINT DISTRIBUTION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "our_products = ['Credit Card', 'Personal Loan', 'Savings Account', 'Money Transfer']\n",
    "\n",
    "# Analyze FULL dataset for overall trends\n",
    "business_df_full = df[df['Product_Category'].isin(our_products)]\n",
    "total_business_complaints_full = len(business_df_full)\n",
    "\n",
    "# Analyze NLP-VIABLE dataset for text analysis\n",
    "business_df_viable = viable_df[viable_df['Product_Category'].isin(our_products)]\n",
    "total_business_complaints_viable = len(business_df_viable)\n",
    "\n",
    "print(f\"üìà OVERALL TRENDS (All 9.6M complaints):\")\n",
    "print(f\"   ‚Ä¢ Total Complaints in Database: {len(df):,}\")\n",
    "print(f\"   ‚Ä¢ Complaints Relevant to CrediTrust: {total_business_complaints_full:,} \"\n",
    "      f\"({(total_business_complaints_full/len(df)*100):.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ NLP-ANALYZABLE DATA (3.0M with narratives):\")\n",
    "print(f\"   ‚Ä¢ NLP-viable Complaints: {len(viable_df):,}\")\n",
    "print(f\"   ‚Ä¢ Business-relevant & NLP-viable: {total_business_complaints_viable:,} \"\n",
    "      f\"({(total_business_complaints_viable/len(viable_df)*100):.1f}% of viable data)\")\n",
    "\n",
    "# Detailed product breakdown - SHOW BOTH PERSPECTIVES\n",
    "print(\"\\nüìä PRODUCT-WISE BREAKDOWN:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Product':<20} {'Total':>12} {'NLP-Viable':>12} {'Viable %':>10}\")\n",
    "\n",
    "for product in our_products:\n",
    "    # Full dataset counts\n",
    "    total_count = len(df[df['Product_Category'] == product])\n",
    "    \n",
    "    # NLP-viable counts\n",
    "    viable_count = len(viable_df[viable_df['Product_Category'] == product])\n",
    "    \n",
    "    # Calculate percentage viable\n",
    "    viable_pct = (viable_count / total_count * 100) if total_count > 0 else 0\n",
    "    \n",
    "    # Determine severity\n",
    "    if viable_pct > 50:\n",
    "        severity = \"‚úÖ HIGH\"\n",
    "    elif viable_pct > 30:\n",
    "        severity = \"‚ö†Ô∏è MEDIUM\"\n",
    "    else:\n",
    "        severity = \"üö® LOW\"\n",
    "    \n",
    "    print(f\"   ‚Ä¢ {product:<20} {total_count:>12,} {viable_count:>12,} {viable_pct:>9.1f}% {severity}\")\n",
    "\n",
    "# Calculate overall viability percentage for business products\n",
    "total_viable_pct = (total_business_complaints_viable / total_business_complaints_full * 100) if total_business_complaints_full > 0 else 0\n",
    "\n",
    "print(f\"\\nüìà KEY BUSINESS INSIGHT:\")\n",
    "print(f\"   ‚Ä¢ Only {total_viable_pct:.1f}% of business-relevant complaints have analyzable narratives\")\n",
    "print(f\"   ‚Ä¢ For NLP/AI analysis, focus on {total_business_complaints_viable:,} complaints\")\n",
    "print(f\"   ‚Ä¢ {total_business_complaints_full - total_business_complaints_viable:,} business complaints cannot be text-analyzed\")\n",
    "\n",
    "# Create a visualization-ready summary\n",
    "product_summary = pd.DataFrame({\n",
    "    'Product': our_products,\n",
    "    'Total_Complaints': [len(df[df['Product_Category'] == p]) for p in our_products],\n",
    "    'NLP_Viable': [len(viable_df[viable_df['Product_Category'] == p]) for p in our_products]\n",
    "})\n",
    "\n",
    "product_summary['Viable_Pct'] = (product_summary['NLP_Viable'] / product_summary['Total_Complaints'] * 100)\n",
    "product_summary['Missing_Narratives'] = product_summary['Total_Complaints'] - product_summary['NLP_Viable']\n",
    "\n",
    "print(\"\\nüìã SUMMARY DATAFRAME:\")\n",
    "print(product_summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ee1e8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üìà VISUALIZATION 2: PRODUCT ANALYSIS DASHBOARD\n",
      "====================================================================================================\n",
      "üé® Creating product analysis visualizations...\n",
      "‚úÖ Saved: Product Analysis Dashboard\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìà VISUALIZATION 2: PRODUCT ANALYSIS DASHBOARD\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìà VISUALIZATION 2: PRODUCT ANALYSIS DASHBOARD\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"üé® Creating product analysis visualizations...\")\n",
    "\n",
    "# 1. Product Distribution Comparison\n",
    "fig_products = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('All Products (Top 10)', 'Business Products (All)',\n",
    "                   'NLP-Viable vs Missing Narratives', 'Narrative Completeness by Product'),\n",
    "    specs=[[{'type': 'bar'}, {'type': 'pie'}],\n",
    "           [{'type': 'bar'}, {'type': 'bar'}]],\n",
    "    vertical_spacing=0.15,\n",
    "    horizontal_spacing=0.15\n",
    ")\n",
    "\n",
    "# Subplot 1: Top 10 Products (All Data)\n",
    "top_products_all = df['Product'].value_counts().head(10)\n",
    "fig_products.add_trace(\n",
    "    go.Bar(\n",
    "        x=top_products_all.values,\n",
    "        y=top_products_all.index,\n",
    "        orientation='h',\n",
    "        marker_color='#2E86AB',\n",
    "        name='All Products',\n",
    "        hovertemplate='%{y}<br>Count: %{x:,}<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Subplot 2: Our Products Distribution\n",
    "our_counts = business_df_viable['Product_Category'].value_counts()\n",
    "fig_products.add_trace(\n",
    "    go.Pie(\n",
    "        labels=our_counts.index,\n",
    "        values=our_counts.values,\n",
    "        hole=0.3,\n",
    "        marker_colors=['#A23B72', '#F18F01', '#73AB84', '#2E86AB'],\n",
    "        textinfo='label+percent',\n",
    "        name='Our Products'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Subplot 3: NLP-Viable vs Missing\n",
    "viable_counts = [len(business_df_viable), len(business_df_full) - len(business_df_viable)]\n",
    "fig_products.add_trace(\n",
    "    go.Bar(\n",
    "        x=['With Narratives', 'Without Narratives'],\n",
    "        y=viable_counts,\n",
    "        marker_color=['#73AB84', '#A23B72'],\n",
    "        text=[f'{count:,}' for count in viable_counts],\n",
    "        textposition='auto',\n",
    "        hovertemplate='%{x}<br>Count: %{y:,}<extra></extra>'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Subplot 4: Narrative Completeness by Product\n",
    "product_viability = []\n",
    "for product in our_products:\n",
    "    total = len(df[df['Product_Category'] == product])\n",
    "    viable = len(business_df_viable[business_df_viable['Product_Category'] == product])\n",
    "    product_viability.append({\n",
    "        'Product': product,\n",
    "        'Total': total,\n",
    "        'Viable': viable,\n",
    "        'Percentage': (viable / total * 100) if total > 0 else 0\n",
    "    })\n",
    "\n",
    "viability_df = pd.DataFrame(product_viability)\n",
    "fig_products.add_trace(\n",
    "    go.Bar(\n",
    "        x=viability_df['Product'],\n",
    "        y=viability_df['Percentage'],\n",
    "        marker_color=['#A23B72', '#F18F01', '#73AB84', '#2E86AB'],\n",
    "        text=[f'{p:.1f}%' for p in viability_df['Percentage']],\n",
    "        textposition='auto',\n",
    "        hovertemplate='%{x}<br>Viable: %{y:.1f}%<extra></extra>'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig_products.update_layout(\n",
    "    title=\"<b>Product Analysis Dashboard</b><br><i>Complaint Distribution & NLP Viability</i>\",\n",
    "    title_font_size=16,\n",
    "    height=800,\n",
    "    showlegend=False,\n",
    "    margin=dict(l=50, r=50, t=100, b=50)\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig_products.update_xaxes(title_text=\"Number of Complaints\", row=1, col=1)\n",
    "fig_products.update_yaxes(title_text=\"Product\", row=1, col=1)\n",
    "fig_products.update_xaxes(title_text=\"Category\", row=2, col=1)\n",
    "fig_products.update_yaxes(title_text=\"Number of Complaints\", row=2, col=1)\n",
    "fig_products.update_xaxes(title_text=\"Product\", row=2, col=2)\n",
    "fig_products.update_yaxes(title_text=\"Narrative Viability (%)\", row=2, col=2)\n",
    "\n",
    "fig_products.write_html(\"reports/product_analysis_dashboard.html\")\n",
    "print(\"‚úÖ Saved: Product Analysis Dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f67a1",
   "metadata": {},
   "source": [
    "CLASS BALANCE & STATISTICAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74cce42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "‚öñÔ∏è PHASE 4: CLASS BALANCE & STATISTICAL ANALYSIS\n",
      "====================================================================================================\n",
      "üìä USING NLP-VIABLE BUSINESS DATA FROM SECTION 4\n",
      "   ‚Ä¢ Business-relevant complaints: 515,810\n",
      "   ‚Ä¢ Business complaints with narratives: 515,810\n",
      "\n",
      "üìä PRODUCT DISTRIBUTION (NLP-Viable Business Data):\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ Credit Card           197,126 complaints ( 38.2%) üö® HIGH\n",
      "   ‚Ä¢ Savings Account       155,204 complaints ( 30.1%) üö® HIGH\n",
      "   ‚Ä¢ Money Transfer         97,204 complaints ( 18.8%) ‚ö†Ô∏è MEDIUM\n",
      "   ‚Ä¢ Personal Loan          66,276 complaints ( 12.8%) ‚úÖ LOW\n",
      "\n",
      "‚úÖ Saved class balance visualization: reports/class_balance_analysis.html\n",
      "\n",
      "üìä STATISTICAL IMBALANCE ANALYSIS (NLP-Viable Business Data):\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ Max/Min Ratio: 2.97x (Higher = More Imbalanced)\n",
      "   ‚Ä¢ Gini Coefficient: 0.711 (0=Perfect Balance, 1=Maximum Imbalance)\n",
      "   ‚Ä¢ Entropy Score: 1.886\n",
      "   ‚úÖ GOOD: Class balance is acceptable for AI modeling\n",
      "\n",
      "üìà NARRATIVE VIABILITY BY PRODUCT CATEGORY:\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ Credit Card           197,126/ 448,335 ( 44.0%) have narratives\n",
      "   ‚Ä¢ Personal Loan          66,276/ 135,172 ( 49.0%) have narratives\n",
      "   ‚Ä¢ Savings Account       155,204/ 377,383 ( 41.1%) have narratives\n",
      "   ‚Ä¢ Money Transfer         97,204/ 145,084 ( 67.0%) have narratives\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìä SECTION 5: CLASS BALANCE & STATISTICAL ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"‚öñÔ∏è PHASE 4: CLASS BALANCE & STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# CRITICAL: Use the filtered business data from Section 4\n",
    "print(\"üìä USING NLP-VIABLE BUSINESS DATA FROM SECTION 4\")\n",
    "print(f\"   ‚Ä¢ Business-relevant complaints: {len(business_df_viable):,}\")\n",
    "print(f\"   ‚Ä¢ Business complaints with narratives: {len(business_df_viable):,}\")\n",
    "\n",
    "# Calculate product distribution for NLP-VIABLE business data\n",
    "product_distribution = business_df_viable['Product_Category'].value_counts()\n",
    "product_percentage = (product_distribution / len(business_df_viable) * 100)\n",
    "\n",
    "print(\"\\nüìä PRODUCT DISTRIBUTION (NLP-Viable Business Data):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for product, count, percent in zip(product_distribution.index, \n",
    "                                   product_distribution.values, \n",
    "                                   product_percentage.values):\n",
    "    severity = \"üö® HIGH\" if percent > 25 else \"‚ö†Ô∏è MEDIUM\" if percent > 15 else \"‚úÖ LOW\"\n",
    "    print(f\"   ‚Ä¢ {product:<20} {count:>8,} complaints ({percent:>5.1f}%) {severity}\")\n",
    "\n",
    "# 1. Class Balance Visualization - DUAL PERSPECTIVE\n",
    "fig1 = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=('All Products (Full Dataset)', \n",
    "                    'Our Products (Full Dataset)',\n",
    "                    'Our Products (NLP-Viable)'),\n",
    "    specs=[[{'type': 'pie'}, {'type': 'pie'}, {'type': 'pie'}]],\n",
    "    column_widths=[0.33, 0.33, 0.34]\n",
    ")\n",
    "\n",
    "# Chart 1: All products in FULL dataset (top 10)\n",
    "all_counts_full = df['Product'].value_counts().head(10)\n",
    "fig1.add_trace(\n",
    "    go.Pie(\n",
    "        labels=all_counts_full.index,\n",
    "        values=all_counts_full.values,\n",
    "        hole=0.3,\n",
    "        name='All Products (Full)',\n",
    "        marker=dict(colors=px.colors.qualitative.Set3),\n",
    "        textinfo='label+percent',\n",
    "        textposition='inside'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Chart 2: Our products in FULL dataset\n",
    "our_products = ['Credit Card', 'Personal Loan', 'Savings Account', 'Money Transfer']\n",
    "business_df_full = df[df['Product_Category'].isin(our_products)]\n",
    "our_counts_full = business_df_full['Product_Category'].value_counts()\n",
    "\n",
    "fig1.add_trace(\n",
    "    go.Pie(\n",
    "        labels=our_counts_full.index,\n",
    "        values=our_counts_full.values,\n",
    "        hole=0.3,\n",
    "        name='Our Products (Full)',\n",
    "        marker=dict(colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']),\n",
    "        textinfo='label+percent',\n",
    "        textposition='inside'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Chart 3: Our products in NLP-VIABLE dataset (FOR AI ANALYSIS)\n",
    "fig1.add_trace(\n",
    "    go.Pie(\n",
    "        labels=product_distribution.index,\n",
    "        values=product_distribution.values,\n",
    "        hole=0.3,\n",
    "        name='Our Products (NLP-Viable)',\n",
    "        marker=dict(colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']),\n",
    "        textinfo='label+percent',\n",
    "        textposition='inside'\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig1.update_layout(\n",
    "    title_text=\"<b>Class Balance Analysis</b><br><i>Comparing Full Dataset vs NLP-Viable Data</i>\",\n",
    "    title_font_size=16,\n",
    "    showlegend=True,\n",
    "    height=500,\n",
    "    annotations=[\n",
    "        dict(text=\"9.6M Total\", x=0.12, y=1.05, xref=\"paper\", yref=\"paper\", showarrow=False, font=dict(size=12)),\n",
    "        dict(text=f\"{len(business_df_full):,} Business\", x=0.5, y=1.05, xref=\"paper\", yref=\"paper\", showarrow=False, font=dict(size=12)),\n",
    "        dict(text=f\"{len(business_df_viable):,} NLP-Viable\", x=0.88, y=1.05, xref=\"paper\", yref=\"paper\", showarrow=False, font=dict(size=12))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create reports directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "fig1.write_html(\"reports/class_balance_analysis.html\")\n",
    "print(\"\\n‚úÖ Saved class balance visualization: reports/class_balance_analysis.html\")\n",
    "\n",
    "# 2. Statistical Imbalance Metrics - FOR NLP-VIABLE DATA\n",
    "print(\"\\nüìä STATISTICAL IMBALANCE ANALYSIS (NLP-Viable Business Data):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if len(product_distribution) > 1:\n",
    "    imbalance_ratio = product_distribution.max() / product_distribution.min()\n",
    "    gini_coefficient = 1 - sum((product_distribution / product_distribution.sum())**2)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Max/Min Ratio: {imbalance_ratio:.2f}x (Higher = More Imbalanced)\")\n",
    "    print(f\"   ‚Ä¢ Gini Coefficient: {gini_coefficient:.3f} (0=Perfect Balance, 1=Maximum Imbalance)\")\n",
    "    print(f\"   ‚Ä¢ Entropy Score: {(-sum((product_distribution/product_distribution.sum()) * np.log2(product_distribution/product_distribution.sum()))):.3f}\")\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(f\"   ‚ö†Ô∏è  WARNING: Severe class imbalance detected (>10x ratio)\")\n",
    "        print(f\"   üí° RECOMMENDATION: Consider stratified sampling or weighted loss in AI model\")\n",
    "    elif imbalance_ratio > 5:\n",
    "        print(f\"   ‚ö†Ô∏è  NOTICE: Moderate class imbalance detected\")\n",
    "        print(f\"   üí° RECOMMENDATION: Monitor performance across all classes\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ GOOD: Class balance is acceptable for AI modeling\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Not enough product categories for imbalance analysis\")\n",
    "\n",
    "# 3. Narrative Viability by Product\n",
    "print(\"\\nüìà NARRATIVE VIABILITY BY PRODUCT CATEGORY:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for product in our_products:\n",
    "    total = len(df[df['Product_Category'] == product])\n",
    "    viable = len(viable_df[viable_df['Product_Category'] == product])\n",
    "    pct = (viable / total * 100) if total > 0 else 0\n",
    "    \n",
    "    print(f\"   ‚Ä¢ {product:<20} {viable:>8,}/{total:>8,} ({pct:>5.1f}%) have narratives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c147027",
   "metadata": {},
   "source": [
    "ADVANCED TEXT ANALYSIS - NLP DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d09027a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üéØ CREATING NLP-VIABLE DATASET FOR TEXT ANALYSIS\n",
      "====================================================================================================\n",
      "‚úÖ Created viable_df: 2,980,756 complaints with narratives\n",
      "   ‚Ä¢ From total dataset of: 9,609,797 complaints\n",
      "   ‚Ä¢ Percentage with narratives: 31.0%\n",
      "\n",
      "üìä Applying product mapping to NLP-viable data...\n",
      "\n",
      "‚úÖ Created business_df_viable: 515,810 complaints\n",
      "   ‚Ä¢ NLP-viable AND business-relevant\n",
      "   ‚Ä¢ Products: Credit Card, Personal Loan, Savings Account, Money Transfer\n",
      "\n",
      "====================================================================================================\n",
      "üéØ READY FOR TEXT ANALYSIS SECTIONS 6-10\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üéØ CRITICAL: CREATE NLP-VIABLE DATASET BEFORE SECTION 6\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üéØ CREATING NLP-VIABLE DATASET FOR TEXT ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# 1. Filter for complaints WITH narratives (31% of data)\n",
    "viable_df = df[df['Consumer complaint narrative'].notna()].copy()\n",
    "print(f\"‚úÖ Created viable_df: {len(viable_df):,} complaints with narratives\")\n",
    "print(f\"   ‚Ä¢ From total dataset of: {len(df):,} complaints\")\n",
    "print(f\"   ‚Ä¢ Percentage with narratives: {len(viable_df)/len(df)*100:.1f}%\")\n",
    "\n",
    "# 2. Apply product mapping to viable_df\n",
    "print(\"\\nüìä Applying product mapping to NLP-viable data...\")\n",
    "product_mapping = {\n",
    "    'Credit card': 'Credit Card',\n",
    "    'Credit card or prepaid card': 'Credit Card',\n",
    "    'Prepaid card': 'Credit Card',\n",
    "    'Payday loan, title loan, or personal loan': 'Personal Loan',\n",
    "    'Consumer Loan': 'Personal Loan',\n",
    "    'Vehicle loan or lease': 'Personal Loan',\n",
    "    'Bank account or service': 'Savings Account',\n",
    "    'Checking or savings account': 'Savings Account',\n",
    "    'Savings account': 'Savings Account',\n",
    "    'Money transfer, virtual currency, or money service': 'Money Transfer',\n",
    "    'Virtual currency': 'Money Transfer',\n",
    "    'Mortgage': 'Mortgage',\n",
    "    'Student loan': 'Student Loan',\n",
    "    'Debt collection': 'Debt Collection',\n",
    "    'Credit reporting, credit repair services, or other personal consumer reports': 'Credit Reporting'\n",
    "}\n",
    "\n",
    "viable_df['Product_Category'] = viable_df['Product'].map(product_mapping).fillna('Other')\n",
    "\n",
    "# 3. Create business_df_viable (NLP-viable AND business-relevant)\n",
    "our_products = ['Credit Card', 'Personal Loan', 'Savings Account', 'Money Transfer']\n",
    "business_df_viable = viable_df[viable_df['Product_Category'].isin(our_products)]\n",
    "\n",
    "print(f\"\\n‚úÖ Created business_df_viable: {len(business_df_viable):,} complaints\")\n",
    "print(f\"   ‚Ä¢ NLP-viable AND business-relevant\")\n",
    "print(f\"   ‚Ä¢ Products: {', '.join(our_products)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üéØ READY FOR TEXT ANALYSIS SECTIONS 6-10\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b89db3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üö® DOWNLOADING PUNKT_TAB FOR ENGLISH TOKENIZATION\n",
      "====================================================================================================\n",
      "üì¶ Downloading punkt_tab (English tokenizer tables)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\G5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ punkt_tab downloaded successfully\n",
      "\n",
      "üîß FINAL TOKENIZER TEST...\n",
      "‚úÖ word_tokenize working: ['I', 'have', 'a', 'credit', 'card', 'complaint', '.', 'The', 'bank', 'charged', 'me', '$', '500', '!']\n",
      "‚úÖ sent_tokenize working: ['I have a credit card complaint.', 'The bank charged me $500!']\n",
      "‚úÖ TOKENIZER STATUS: FALLBACK\n",
      "\n",
      "====================================================================================================\n",
      "üéØ TOKENIZER READY - PROCEED WITH SECTION 7\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üö® FINAL NLTK FIX - PUNKT_TAB SPECIFIC\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"üö® DOWNLOADING PUNKT_TAB FOR ENGLISH TOKENIZATION\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Download punkt_tab specifically\n",
    "print(\"üì¶ Downloading punkt_tab (English tokenizer tables)...\")\n",
    "try:\n",
    "    nltk.download('punkt_tab', quiet=False)\n",
    "    print(\"‚úÖ punkt_tab downloaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not download punkt_tab: {e}\")\n",
    "    print(\"üîÑ Attempting alternative download method...\")\n",
    "    \n",
    "    # Alternative: Download full punkt and extract\n",
    "    try:\n",
    "        nltk.download('punkt', quiet=False)\n",
    "        print(\"‚úÖ Full punkt package downloaded\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Alternative download failed: {e2}\")\n",
    "        print(\"\\nüîß USING FALLBACK TOKENIZER (no NLTK required)...\")\n",
    "        \n",
    "        # Create robust fallback tokenizer\n",
    "        import re\n",
    "        \n",
    "        def robust_word_tokenize(text):\n",
    "            \"\"\"Robust word tokenizer without NLTK\"\"\"\n",
    "            if pd.isna(text) or not str(text).strip():\n",
    "                return []\n",
    "            \n",
    "            text = str(text).lower()\n",
    "            # Remove URLs, emails, special characters (keep letters and basic punctuation)\n",
    "            text = re.sub(r'http\\S+|www\\S+|https\\S+|\\S+@\\S+', '', text)\n",
    "            # Tokenize on word boundaries\n",
    "            words = re.findall(r'\\b[a-z][a-z\\']+\\b', text)\n",
    "            return words\n",
    "        \n",
    "        # Monkey patch nltk functions\n",
    "        nltk.word_tokenize = robust_word_tokenize\n",
    "        \n",
    "        def robust_sent_tokenize(text):\n",
    "            \"\"\"Robust sentence tokenizer without NLTK\"\"\"\n",
    "            if pd.isna(text) or not str(text).strip():\n",
    "                return []\n",
    "            \n",
    "            # Split on sentence boundaries\n",
    "            sentences = re.split(r'[.!?]+', text)\n",
    "            return [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        nltk.sent_tokenize = robust_sent_tokenize\n",
    "        print(\"‚úÖ Fallback tokenizers activated\")\n",
    "\n",
    "# Test tokenizers\n",
    "print(\"\\nüîß FINAL TOKENIZER TEST...\")\n",
    "test_text = \"I have a credit card complaint. The bank charged me $500!\"\n",
    "\n",
    "try:\n",
    "    words = nltk.word_tokenize(test_text)\n",
    "    sentences = nltk.sent_tokenize(test_text)\n",
    "    print(f\"‚úÖ word_tokenize working: {words}\")\n",
    "    print(f\"‚úÖ sent_tokenize working: {sentences}\")\n",
    "    print(f\"‚úÖ TOKENIZER STATUS: {'NLTK' if 'punkt_tab' in str(nltk.word_tokenize) else 'FALLBACK'}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Tokenizer test failed: {e}\")\n",
    "    print(\"üîÑ Activating emergency fallback...\")\n",
    "    \n",
    "    # Emergency fallback\n",
    "    import re\n",
    "    \n",
    "    def emergency_tokenize(text):\n",
    "        return re.findall(r'\\b\\w+\\b', str(text).lower()) if text else []\n",
    "    \n",
    "    nltk.word_tokenize = emergency_tokenize\n",
    "    nltk.sent_tokenize = lambda x: [x]  # Simple sentence tokenizer\n",
    "    \n",
    "    print(\"‚úÖ Emergency fallback activated\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üéØ TOKENIZER READY - PROCEED WITH SECTION 7\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ed153c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üìù PHASE 5: ADVANCED TEXT ANALYSIS - NLP INSIGHTS (WORKING VERSION)\n",
      "====================================================================================================\n",
      "üöÄ WORKAROUND: Bypassing NLTK punkt issue with custom sentence counter\n",
      "   ‚Ä¢ Full viable dataset: 2,980,756 complaints\n",
      "   ‚Ä¢ Business-relevant subset: 515,810 complaints\n",
      "‚úÖ Samples created:\n",
      "   ‚Ä¢ viable_sample: 29,807 complaints\n",
      "   ‚Ä¢ business_sample: 20,000 complaints\n",
      "   ‚Ä¢ Expected runtime: 30-60 seconds\n",
      "\n",
      "üìè DOCUMENT LENGTH ANALYSIS (1% Sample):\n",
      "--------------------------------------------------------------------------------\n",
      "üìà Summary Statistics (1% Sample of 2.98M narratives):\n",
      "       Narrative_Length_Chars  Narrative_Length_Words  \\\n",
      "count                 29807.0                 29807.0   \n",
      "mean                    990.5                   173.9   \n",
      "std                    1262.2                   218.5   \n",
      "min                      11.0                     1.0   \n",
      "25%                     332.0                    59.0   \n",
      "50%                     654.0                   113.0   \n",
      "75%                    1180.0                   209.0   \n",
      "max                   32697.0                  5712.0   \n",
      "\n",
      "       Narrative_Length_Sentences  \n",
      "count                     29807.0  \n",
      "mean                          7.6  \n",
      "std                          10.0  \n",
      "min                           1.0  \n",
      "25%                           2.0  \n",
      "50%                           5.0  \n",
      "75%                           9.0  \n",
      "max                         302.0  \n",
      "\n",
      "üìä KEY INSIGHTS:\n",
      "   ‚Ä¢ Avg characters per complaint: ~990\n",
      "   ‚Ä¢ Avg words per complaint: ~174\n",
      "   ‚Ä¢ Avg sentences per complaint: ~7.6\n",
      "\n",
      "üìä Outlier Detection:\n",
      "   ‚Ä¢ Short Outliers (< -166 words): 0\n",
      "   ‚Ä¢ Long Outliers (> 434 words): 2133\n",
      "   ‚Ä¢ Total Outliers: 2,133 (7.2%)\n",
      "\n",
      "üìà CREATING SIMPLIFIED VISUALIZATIONS...\n",
      "\n",
      "‚úÖ Saved text length analysis: reports/text_length_analysis_final.html\n",
      "\n",
      "üìä PRODUCT-SPECIFIC TEXT LENGTH ANALYSIS:\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ Credit Card: 213.3 words, 1177 characters (n=7,650)\n",
      "   ‚Ä¢ Personal Loan: 227.5 words, 1241 characters (n=2,582)\n",
      "   ‚Ä¢ Savings Account: 220.1 words, 1212 characters (n=6,017)\n",
      "   ‚Ä¢ Money Transfer: 171.3 words, 988 characters (n=3,751)\n",
      "\n",
      "üéØ CRITICAL BUSINESS INSIGHTS:\n",
      "   1. Complaints average ~174 words\n",
      "   2. Money Transfer complaints are LONGEST at ~171 words\n",
      "   3. Personal Loan complaints are SHORTEST at ~228 words\n",
      "   4. Ready for vocabulary analysis in Section 7\n",
      "\n",
      "====================================================================================================\n",
      "‚úÖ SECTION 6 COMPLETE\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìù SECTION 6: ADVANCED TEXT ANALYSIS - NLP DEPTH (WORKING VERSION)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìù PHASE 5: ADVANCED TEXT ANALYSIS - NLP INSIGHTS (WORKING VERSION)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"üöÄ WORKAROUND: Bypassing NLTK punkt issue with custom sentence counter\")\n",
    "print(f\"   ‚Ä¢ Full viable dataset: {len(viable_df):,} complaints\")\n",
    "print(f\"   ‚Ä¢ Business-relevant subset: {len(business_df_viable):,} complaints\")\n",
    "\n",
    "# Create optimized samples\n",
    "sample_fraction = 0.01  # 1% for speed\n",
    "viable_sample_size = int(len(viable_df) * sample_fraction)\n",
    "viable_sample = viable_df.sample(viable_sample_size, random_state=42)\n",
    "\n",
    "business_sample_size = min(20000, len(business_df_viable))\n",
    "business_sample = business_df_viable.sample(business_sample_size, random_state=42)\n",
    "\n",
    "print(f\"‚úÖ Samples created:\")\n",
    "print(f\"   ‚Ä¢ viable_sample: {len(viable_sample):,} complaints\")\n",
    "print(f\"   ‚Ä¢ business_sample: {len(business_sample):,} complaints\")\n",
    "print(f\"   ‚Ä¢ Expected runtime: 30-60 seconds\")\n",
    "\n",
    "# 1. Document Length Analysis (WITHOUT NLTK DEPENDENCY)\n",
    "print(\"\\nüìè DOCUMENT LENGTH ANALYSIS (1% Sample):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Custom sentence counter that doesn't need NLTK\n",
    "def custom_sentence_counter(text):\n",
    "    \"\"\"Count sentences without NLTK dependency\"\"\"\n",
    "    if pd.isna(text) or not str(text).strip():\n",
    "        return 0\n",
    "    \n",
    "    text = str(text)\n",
    "    # Count sentence endings: . ! ? followed by space or end of string\n",
    "    sentence_ends = sum(1 for i in range(len(text)-1) \n",
    "                       if text[i] in '.!?' and text[i+1] in ' \\t\\n')\n",
    "    \n",
    "    # Add last sentence if text doesn't end with punctuation\n",
    "    if text[-1] not in '.!?' and len(text.strip()) > 0:\n",
    "        sentence_ends += 1\n",
    "    \n",
    "    return max(1, sentence_ends)  # At least 1 sentence\n",
    "\n",
    "# Calculate text statistics\n",
    "viable_sample['Narrative_Length_Chars'] = viable_sample['Consumer complaint narrative'].str.len()\n",
    "viable_sample['Narrative_Length_Words'] = viable_sample['Consumer complaint narrative'].str.split().str.len()\n",
    "viable_sample['Narrative_Length_Sentences'] = viable_sample['Consumer complaint narrative'].apply(custom_sentence_counter)\n",
    "\n",
    "text_stats = viable_sample[['Narrative_Length_Chars', 'Narrative_Length_Words', 'Narrative_Length_Sentences']].describe()\n",
    "\n",
    "print(\"üìà Summary Statistics (1% Sample of 2.98M narratives):\")\n",
    "print(text_stats.round(1))\n",
    "\n",
    "print(f\"\\nüìä KEY INSIGHTS:\")\n",
    "print(f\"   ‚Ä¢ Avg characters per complaint: ~{text_stats.loc['mean', 'Narrative_Length_Chars']:.0f}\")\n",
    "print(f\"   ‚Ä¢ Avg words per complaint: ~{text_stats.loc['mean', 'Narrative_Length_Words']:.0f}\")\n",
    "print(f\"   ‚Ä¢ Avg sentences per complaint: ~{text_stats.loc['mean', 'Narrative_Length_Sentences']:.1f}\")\n",
    "\n",
    "# Identify outliers\n",
    "Q1 = viable_sample['Narrative_Length_Words'].quantile(0.25)\n",
    "Q3 = viable_sample['Narrative_Length_Words'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers = viable_sample[(viable_sample['Narrative_Length_Words'] < (Q1 - 1.5 * IQR)) | \n",
    "                         (viable_sample['Narrative_Length_Words'] > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "print(f\"\\nüìä Outlier Detection:\")\n",
    "print(f\"   ‚Ä¢ Short Outliers (< {Q1 - 1.5 * IQR:.0f} words): {len(outliers[outliers['Narrative_Length_Words'] < (Q1 - 1.5 * IQR)])}\")\n",
    "print(f\"   ‚Ä¢ Long Outliers (> {Q3 + 1.5 * IQR:.0f} words): {len(outliers[outliers['Narrative_Length_Words'] > (Q3 + 1.5 * IQR)])}\")\n",
    "print(f\"   ‚Ä¢ Total Outliers: {len(outliers):,} ({len(outliers)/len(viable_sample)*100:.1f}%)\")\n",
    "\n",
    "# 2. SIMPLIFIED VISUALIZATION (Characters & Words only)\n",
    "print(\"\\nüìà CREATING SIMPLIFIED VISUALIZATIONS...\")\n",
    "\n",
    "fig2 = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Character Length Distribution (1% Sample)', \n",
    "                    'Word Length Distribution (1% Sample)'),\n",
    "    specs=[[{'type': 'histogram'}, {'type': 'histogram'}]]\n",
    ")\n",
    "\n",
    "fig2.add_trace(\n",
    "    go.Histogram(\n",
    "        x=viable_sample['Narrative_Length_Chars'].dropna(),\n",
    "        nbinsx=50,\n",
    "        name='Characters',\n",
    "        marker_color='#FF6B6B',\n",
    "        opacity=0.7,\n",
    "        hovertemplate='Characters: %{x}<br>Count: %{y}<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig2.add_trace(\n",
    "    go.Histogram(\n",
    "        x=viable_sample['Narrative_Length_Words'].dropna(),\n",
    "        nbinsx=50,\n",
    "        name='Words',\n",
    "        marker_color='#4ECDC4',\n",
    "        opacity=0.7,\n",
    "        hovertemplate='Words: %{x}<br>Count: %{y}<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig2.update_layout(\n",
    "    title_text=\"<b>Text Length Analysis - 1% Sample ({:,} complaints)</b>\".format(len(viable_sample)),\n",
    "    title_font_size=14,\n",
    "    height=400,\n",
    "    showlegend=False,\n",
    "    margin=dict(l=50, r=50, t=80, b=50)\n",
    ")\n",
    "\n",
    "fig2.update_xaxes(title_text=\"Character Count\", row=1, col=1)\n",
    "fig2.update_xaxes(title_text=\"Word Count\", row=1, col=2)\n",
    "fig2.update_yaxes(title_text=\"Number of Complaints\", row=1, col=1)\n",
    "fig2.update_yaxes(title_text=\"Number of Complaints\", row=1, col=2)\n",
    "\n",
    "# Create reports directory\n",
    "import os\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "fig2.write_html(\"reports/text_length_analysis_final.html\")\n",
    "print(\"\\n‚úÖ Saved text length analysis: reports/text_length_analysis_final.html\")\n",
    "\n",
    "# 3. Product-specific analysis\n",
    "print(\"\\nüìä PRODUCT-SPECIFIC TEXT LENGTH ANALYSIS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "product_stats = []\n",
    "for product in our_products:\n",
    "    subset = business_sample[business_sample['Product_Category'] == product]\n",
    "    if len(subset) > 0:\n",
    "        avg_words = subset['Consumer complaint narrative'].str.split().str.len().mean()\n",
    "        avg_chars = subset['Consumer complaint narrative'].str.len().mean()\n",
    "        product_stats.append({\n",
    "            'Product': product,\n",
    "            'Avg_Words': avg_words,\n",
    "            'Avg_Chars': avg_chars,\n",
    "            'Sample_Size': len(subset)\n",
    "        })\n",
    "        print(f\"   ‚Ä¢ {product}: {avg_words:.1f} words, {avg_chars:.0f} characters (n={len(subset):,})\")\n",
    "\n",
    "print(f\"\\nüéØ CRITICAL BUSINESS INSIGHTS:\")\n",
    "print(f\"   1. Complaints average ~{text_stats.loc['mean', 'Narrative_Length_Words']:.0f} words\")\n",
    "print(f\"   2. Money Transfer complaints are LONGEST at ~{next(p['Avg_Words'] for p in product_stats if p['Product'] == 'Money Transfer'):.0f} words\")\n",
    "print(f\"   3. Personal Loan complaints are SHORTEST at ~{next(p['Avg_Words'] for p in product_stats if p['Product'] == 'Personal Loan'):.0f} words\")\n",
    "print(f\"   4. Ready for vocabulary analysis in Section 7\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"‚úÖ SECTION 6 COMPLETE\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9394602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üìä TEXT ANALYSIS INSIGHTS & VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìä TEXT ANALYSIS: EXECUTIVE INSIGHTS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\nüéØ KEY FINDINGS FROM TEXT ANALYSIS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# 1. TEXT LENGTH ANALYSIS\n",
    "print(\"\\nüìè **TEXT LENGTH INSIGHTS:**\")\n",
    "print(\"   ‚Ä¢ Average complaint: 174 words, 991 characters\")\n",
    "print(\"   ‚Ä¢ Typical range: 59-209 words (25th-75th percentile)\")\n",
    "print(\"   ‚Ä¢ Maximum found: 5,712 words (extremely detailed complaint)\")\n",
    "print(\"   ‚Ä¢ Minimum found: 1 word (likely data entry error)\")\n",
    "\n",
    "# 2. BUSINESS IMPACT ANALYSIS\n",
    "print(\"\\nüíº **BUSINESS IMPLICATIONS:**\")\n",
    "print(\"   ‚Ä¢ **Complaint Complexity**: Average 174 words suggests detailed issues\")\n",
    "print(\"   ‚Ä¢ **Analyst Workload**: Each complaint takes ~1-2 minutes to read\")\n",
    "print(\"   ‚Ä¢ **AI Processing**: Text length suitable for NLP models\")\n",
    "print(\"   ‚Ä¢ **Resource Planning**: Need systems for 100-200 word documents\")\n",
    "\n",
    "# 3. OUTLIER ANALYSIS\n",
    "Q1 = viable_sample['Narrative_Length_Words'].quantile(0.25)\n",
    "Q3 = viable_sample['Narrative_Length_Words'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "short_outliers = viable_sample[viable_sample['Narrative_Length_Words'] < (Q1 - 1.5 * IQR)]\n",
    "long_outliers = viable_sample[viable_sample['Narrative_Length_Words'] > (Q3 + 1.5 * IQR)]\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  **OUTLIER ANALYSIS:**\")\n",
    "print(f\"   ‚Ä¢ Short outliers (< {Q1 - 1.5 * IQR:.0f} words): {len(short_outliers):,}\")\n",
    "print(f\"   ‚Ä¢ Long outliers (> {Q3 + 1.5 * IQR:.0f} words): {len(long_outliers):,}\")\n",
    "print(f\"   ‚Ä¢ Total outliers: {len(short_outliers) + len(long_outliers):,} ({((len(short_outliers) + len(long_outliers))/len(viable_sample)*100):.1f}%)\")\n",
    "\n",
    "# 4. VISUALIZATION DASHBOARD\n",
    "print(\"\\nüé® **CREATING TEXT ANALYSIS VISUALIZATIONS...**\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "# Create comprehensive text analysis dashboard\n",
    "fig_text_dashboard = make_subplots(\n",
    "    rows=3, cols=3,\n",
    "    subplot_titles=('Character Length Distribution', 'Word Length Distribution',\n",
    "                   'Sentence Length Distribution', 'Word Length by Product',\n",
    "                   'Cumulative Word Distribution', 'Outlier Analysis',\n",
    "                   'Length vs Product (Box Plot)', 'Length Statistics',\n",
    "                   'Text Length Categories'),\n",
    "    specs=[[{'type': 'histogram'}, {'type': 'histogram'}, {'type': 'histogram'}],\n",
    "           [{'type': 'violin'}, {'type': 'line'}, {'type': 'scatter'}],\n",
    "           [{'type': 'box'}, {'type': 'indicator'}, {'type': 'bar'}]],\n",
    "    vertical_spacing=0.08,\n",
    "    horizontal_spacing=0.08\n",
    ")\n",
    "\n",
    "# ===== ROW 1: HISTOGRAMS =====\n",
    "\n",
    "# 1. Character Length Distribution\n",
    "fig_text_dashboard.add_trace(\n",
    "    go.Histogram(\n",
    "        x=viable_sample['Narrative_Length_Chars'],\n",
    "        nbinsx=50,\n",
    "        name='Characters',\n",
    "        marker_color='#2E86AB',\n",
    "        opacity=0.7,\n",
    "        hovertemplate='<b>Characters: %{x}</b><br>Count: %{y}<br>Percentage: %{customdata:.1f}%<extra></extra>',\n",
    "        customdata=(np.ones(len(viable_sample)) / len(viable_sample) * 100)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Word Length Distribution\n",
    "fig_text_dashboard.add_trace(\n",
    "    go.Histogram(\n",
    "        x=viable_sample['Narrative_Length_Words'],\n",
    "        nbinsx=50,\n",
    "        name='Words',\n",
    "        marker_color='#A23B72',\n",
    "        opacity=0.7,\n",
    "        hovertemplate='<b>Words: %{x}</b><br>Count: %{y}<br>Percentage: %{customdata:.1f}%<extra></extra>',\n",
    "        customdata=(np.ones(len(viable_sample)) / len(viable_sample) * 100)\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Sentence Length Distribution\n",
    "if 'Narrative_Length_Sentences' in viable_sample.columns:\n",
    "    fig_text_dashboard.add_trace(\n",
    "        go.Histogram(\n",
    "            x=viable_sample['Narrative_Length_Sentences'],\n",
    "            nbinsx=30,\n",
    "            name='Sentences',\n",
    "            marker_color='#F18F01',\n",
    "            opacity=0.7,\n",
    "            hovertemplate='<b>Sentences: %{x}</b><br>Count: %{y}<br>Percentage: %{customdata:.1f}%<extra></extra>',\n",
    "            customdata=(np.ones(len(viable_sample)) / len(viable_sample) * 100)\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "\n",
    "# ===== ROW 2: DISTRIBUTION ANALYSIS =====\n",
    "\n",
    "# 4. Violin Plot by Product\n",
    "if 'business_sample' in locals():\n",
    "    for product in our_products:\n",
    "        subset = business_sample[business_sample['Product_Category'] == product]\n",
    "        if len(subset) > 0:\n",
    "            subset['Word_Length'] = subset['Consumer complaint narrative'].str.split().str.len()\n",
    "            fig_text_dashboard.add_trace(\n",
    "                go.Violin(\n",
    "                    y=subset['Word_Length'],\n",
    "                    name=product,\n",
    "                    side='positive',\n",
    "                    line_color={'Credit Card': '#2E86AB',\n",
    "                               'Personal Loan': '#A23B72',\n",
    "                               'Savings Account': '#F18F01',\n",
    "                               'Money Transfer': '#73AB84'}[product],\n",
    "                    fillcolor='rgba(255,255,255,0)',\n",
    "                    points=False,\n",
    "                    meanline_visible=True,\n",
    "                    hoverinfo='y+name'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "\n",
    "# 5. Cumulative Distribution\n",
    "sorted_words = np.sort(viable_sample['Narrative_Length_Words'])\n",
    "cumulative = np.arange(1, len(sorted_words) + 1) / len(sorted_words) * 100\n",
    "\n",
    "fig_text_dashboard.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sorted_words,\n",
    "        y=cumulative,\n",
    "        mode='lines',\n",
    "        name='Cumulative %',\n",
    "        line=dict(color='#2E86AB', width=3),\n",
    "        fill='tozeroy',\n",
    "        fillcolor='rgba(46, 134, 171, 0.2)',\n",
    "        hovertemplate='<b>Word Count: %{x}</b><br>Cumulative: %{y:.1f}%<extra></extra>'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Add percentiles\n",
    "percentiles = [25, 50, 75, 90, 95]\n",
    "percentile_values = np.percentile(viable_sample['Narrative_Length_Words'], percentiles)\n",
    "\n",
    "for pct, value in zip(percentiles, percentile_values):\n",
    "    fig_text_dashboard.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[value, value],\n",
    "            y=[0, 100],\n",
    "            mode='lines',\n",
    "            line=dict(color='#A23B72', width=1, dash='dash'),\n",
    "            showlegend=False,\n",
    "            hoverinfo='skip'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "# 6. Outlier Scatter Plot\n",
    "fig_text_dashboard.add_trace(\n",
    "    go.Scatter(\n",
    "        x=viable_sample['Narrative_Length_Chars'],\n",
    "        y=viable_sample['Narrative_Length_Words'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=4,\n",
    "            color=viable_sample['Narrative_Length_Words'],\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Word Count\", x=1.02)\n",
    "        ),\n",
    "        text=[f\"Complaint {i}\" for i in viable_sample.index[:len(viable_sample)]],\n",
    "        hovertemplate='<b>%{text}</b><br>Chars: %{x}<br>Words: %{y}<extra></extra>'\n",
    "    ),\n",
    "    row=2, col=3\n",
    ")\n",
    "\n",
    "# ===== ROW 3: SUMMARY VISUALIZATIONS =====\n",
    "\n",
    "# 7. Box Plot by Product\n",
    "for product in our_products:\n",
    "    if 'business_sample' in locals():\n",
    "        subset = business_sample[business_sample['Product_Category'] == product]\n",
    "        if len(subset) > 0:\n",
    "            subset['Word_Length'] = subset['Consumer complaint narrative'].str.split().str.len()\n",
    "            fig_text_dashboard.add_trace(\n",
    "                go.Box(\n",
    "                    y=subset['Word_Length'],\n",
    "                    name=product,\n",
    "                    marker_color={'Credit Card': '#2E86AB',\n",
    "                                 'Personal Loan': '#A23B72',\n",
    "                                 'Savings Account': '#F18F01',\n",
    "                                 'Money Transfer': '#73AB84'}[product],\n",
    "                    boxpoints='outliers',\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=3, col=1\n",
    "            )\n",
    "\n",
    "# 8. Statistics Indicator\n",
    "avg_words = viable_sample['Narrative_Length_Words'].mean()\n",
    "fig_text_dashboard.add_trace(\n",
    "    go.Indicator(\n",
    "        mode=\"number+gauge\",\n",
    "        value=avg_words,\n",
    "        title={'text': \"Avg Words/Complaint\"},\n",
    "        domain={'x': [0, 1], 'y': [0, 1]},\n",
    "        gauge={\n",
    "            'shape': \"bullet\",\n",
    "            'axis': {'range': [0, viable_sample['Narrative_Length_Words'].max()]},\n",
    "            'bar': {'color': \"#2E86AB\"},\n",
    "            'steps': [\n",
    "                {'range': [0, Q1], 'color': \"#73AB84\"},\n",
    "                {'range': [Q1, Q3], 'color': \"#F18F01\"},\n",
    "                {'range': [Q3, viable_sample['Narrative_Length_Words'].max()], 'color': \"#A23B72\"}\n",
    "            ],\n",
    "            'threshold': {\n",
    "                'line': {'color': \"black\", 'width': 2},\n",
    "                'thickness': 0.75,\n",
    "                'value': avg_words\n",
    "            }\n",
    "        }\n",
    "    ),\n",
    "    row=3, col=2\n",
    ")\n",
    "\n",
    "# 9. Text Length Categories\n",
    "bins = [0, 50, 100, 200, 500, 1000, float('inf')]\n",
    "labels = ['Very Short (<50)', 'Short (50-100)', 'Medium (100-200)', \n",
    "          'Long (200-500)', 'Very Long (500-1000)', 'Extreme (>1000)']\n",
    "viable_sample['Length_Category'] = pd.cut(viable_sample['Narrative_Length_Words'], \n",
    "                                          bins=bins, labels=labels, right=False)\n",
    "category_counts = viable_sample['Length_Category'].value_counts().sort_index()\n",
    "\n",
    "fig_text_dashboard.add_trace(\n",
    "    go.Bar(\n",
    "        x=category_counts.values,\n",
    "        y=category_counts.index,\n",
    "        orientation='h',\n",
    "        marker_color=['#73AB84', '#2E86AB', '#F18F01', '#A23B72', '#5D4E6D', '#3A3042'],\n",
    "        text=[f'{count:,}' for count in category_counts.values],\n",
    "        textposition='auto',\n",
    "        hovertemplate='<b>%{y}</b><br>Count: %{x:,}<br>Percentage: %{customdata:.1f}%<extra></extra>',\n",
    "        customdata=(category_counts.values / len(viable_sample) * 100)\n",
    "    ),\n",
    "    row=3, col=3\n",
    ")\n",
    "\n",
    "# ===== UPDATE LAYOUT =====\n",
    "fig_text_dashboard.update_layout(\n",
    "    title=\"<b>TEXT ANALYSIS DASHBOARD</b><br><i>Comprehensive Complaint Length Analysis</i>\",\n",
    "    title_font_size=18,\n",
    "    height=1000,\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=1.02,\n",
    "        bgcolor='rgba(255, 255, 255, 0.8)',\n",
    "        bordercolor='#333',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    margin=dict(l=50, r=200, t=120, b=50),\n",
    "    paper_bgcolor='white',\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "fig_text_dashboard.update_xaxes(title_text=\"Character Count\", row=1, col=1)\n",
    "fig_text_dashboard.update_xaxes(title_text=\"Word Count\", row=1, col=2)\n",
    "if 'Narrative_Length_Sentences' in viable_sample.columns:\n",
    "    fig_text_dashboard.update_xaxes(title_text=\"Sentence Count\", row=1, col=3)\n",
    "\n",
    "fig_text_dashboard.update_yaxes(title_text=\"Number of Complaints\", row=1, col=1)\n",
    "fig_text_dashboard.update_yaxes(title_text=\"Number of Complaints\", row=1, col=2)\n",
    "if 'Narrative_Length_Sentences' in viable_sample.columns:\n",
    "    fig_text_dashboard.update_yaxes(title_text=\"Number of Complaints\", row=1, col=3)\n",
    "\n",
    "fig_text_dashboard.update_xaxes(title_text=\"Word Count\", row=2, col=2)\n",
    "fig_text_dashboard.update_yaxes(title_text=\"Cumulative Percentage\", row=2, col=2)\n",
    "fig_text_dashboard.update_xaxes(title_text=\"Character Count\", row=2, col=3)\n",
    "fig_text_dashboard.update_yaxes(title_text=\"Word Count\", row=2, col=3)\n",
    "\n",
    "fig_text_dashboard.update_yaxes(title_text=\"Product\", row=3, col=1)\n",
    "fig_text_dashboard.update_xaxes(title_text=\"Word Count\", row=3, col=1)\n",
    "fig_text_dashboard.update_xaxes(title_text=\"Number of Complaints\", row=3, col=3)\n",
    "\n",
    "# Save the dashboard\n",
    "import os\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "fig_text_dashboard.write_html(\"reports/text_analysis_dashboard.html\")\n",
    "\n",
    "print(\"‚úÖ Created: Text Analysis Dashboard\")\n",
    "print(\"   ‚Ä¢ File: reports/text_analysis_dashboard.html\")\n",
    "print(\"   ‚Ä¢ Interactive HTML with 9 visualization panels\")\n",
    "\n",
    "# ============================================================================\n",
    "# üìã EXECUTIVE SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìã TEXT ANALYSIS: EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\nüéØ **KEY BUSINESS INSIGHTS:**\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"1. **COMPLAINT COMPLEXITY**\")\n",
    "print(f\"   ‚Ä¢ Average: 174 words, 991 characters per complaint\")\n",
    "print(f\"   ‚Ä¢ 50% of complaints: 113-209 words (detailed explanations)\")\n",
    "print(f\"   ‚Ä¢ 25% are very detailed (>209 words)\")\n",
    "\n",
    "print(\"\\n2. **RESOURCE IMPLICATIONS**\")\n",
    "print(f\"   ‚Ä¢ Reading time: ~{avg_words/200:.1f} minutes per complaint (at 200 wpm)\")\n",
    "print(f\"   ‚Ä¢ Analyst workload: {avg_words/200*1940.2/60:.1f} hours/day for all complaints\")\n",
    "print(f\"   ‚Ä¢ AI processing: Suitable for transformer models (BERT, GPT)\")\n",
    "\n",
    "print(\"\\n3. **DATA QUALITY**\")\n",
    "print(f\"   ‚Ä¢ Outliers: {len(short_outliers) + len(long_outliers):,} complaints ({((len(short_outliers) + len(long_outliers))/len(viable_sample)*100):.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Short outliers (<{Q1 - 1.5 * IQR:.0f} words): Possibly incomplete submissions\")\n",
    "print(f\"   ‚Ä¢ Long outliers (>{Q3 + 1.5 * IQR:.0f} words): Highly detailed cases needing attention\")\n",
    "\n",
    "print(\"\\n4. **PROCESS OPTIMIZATION**\")\n",
    "print(f\"   ‚Ä¢ Target processing: 100-200 word range (covers 50% of cases)\")\n",
    "print(f\"   ‚Ä¢ Automated triage: Flag outliers for manual review\")\n",
    "print(f\"   ‚Ä¢ Training data: Sufficient length for accurate NLP modeling\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"‚úÖ TEXT ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üî§ PHASE 6: VOCABULARY & LINGUISTIC ANALYSIS\n",
      "====================================================================================================\n",
      "üìä Analyzing vocabulary for 515,810 business-relevant, NLP-viable complaints\n",
      "   ‚Ä¢ Using sample of 10,000 complaints for vocabulary analysis\n",
      "\n",
      "üìä VOCABULARY ANALYSIS ACROSS PRODUCTS (NLP-Viable Data):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Credit Card:\n",
      "   ‚Ä¢ Total Words: 45,748,273\n",
      "   ‚Ä¢ Unique Words: 123,637\n",
      "   ‚Ä¢ Vocabulary Richness: 0.0027\n",
      "   ‚Ä¢ Top 5 Words: ['.', 'the', 'i', 'xxxx', 'to']\n",
      "\n",
      "Personal Loan:\n",
      "   ‚Ä¢ Total Words: 16,610,059\n",
      "   ‚Ä¢ Unique Words: 72,805\n",
      "   ‚Ä¢ Vocabulary Richness: 0.0044\n",
      "   ‚Ä¢ Top 5 Words: ['the', '.', 'i', 'xxxx', 'to']\n",
      "\n",
      "Savings Account:\n",
      "   ‚Ä¢ Total Words: 37,551,942\n",
      "   ‚Ä¢ Unique Words: 106,436\n",
      "   ‚Ä¢ Vocabulary Richness: 0.0028\n",
      "   ‚Ä¢ Top 5 Words: ['.', 'the', 'i', 'xxxx', 'to']\n",
      "\n",
      "Money Transfer:\n",
      "   ‚Ä¢ Total Words: 17,819,178\n",
      "   ‚Ä¢ Unique Words: 61,517\n",
      "   ‚Ä¢ Vocabulary Richness: 0.0035\n",
      "   ‚Ä¢ Top 5 Words: ['.', 'and', 'the', 'to', ',']\n",
      "\n",
      "üìä VOCABULARY OVERLAP ANALYSIS (NLP-Viable Products):\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ Credit Card: 123,637 unique words\n",
      "   ‚Ä¢ Personal Loan: 72,805 unique words\n",
      "   ‚Ä¢ Savings Account: 106,436 unique words\n",
      "   ‚Ä¢ Money Transfer: 61,517 unique words\n",
      "\n",
      "Jaccard Similarity Matrix (Vocabulary Overlap in NLP-Viable Data):\n",
      "                Credit Card Personal Loan Savings Account Money Transfer\n",
      "Credit Card             1.0      0.247647        0.270862       0.236644\n",
      "Personal Loan      0.247647           1.0        0.249432        0.26309\n",
      "Savings Account    0.270862      0.249432             1.0       0.264249\n",
      "Money Transfer     0.236644       0.26309        0.264249            1.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üî§ SECTION 7: VOCABULARY & LINGUISTIC ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üî§ PHASE 6: VOCABULARY & LINGUISTIC ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Note: We are analyzing ONLY the viable complaints (with narratives)\n",
    "print(f\"üìä Analyzing vocabulary for {len(business_df_viable):,} business-relevant, NLP-viable complaints\")\n",
    "\n",
    "# Sample for vocabulary analysis (for performance)\n",
    "sample_size = min(10000, len(business_df_viable))\n",
    "sample_df = business_df_viable.sample(sample_size, random_state=42)\n",
    "print(f\"   ‚Ä¢ Using sample of {sample_size:,} complaints for vocabulary analysis\")\n",
    "\n",
    "def analyze_vocabulary(text_series):\n",
    "    \"\"\"Advanced vocabulary analysis\"\"\"\n",
    "    all_words = []\n",
    "    for text in text_series.dropna():\n",
    "        tokens = word_tokenize(str(text).lower())\n",
    "        all_words.extend(tokens)\n",
    "    \n",
    "    word_counts = Counter(all_words)\n",
    "    total_words = len(all_words)\n",
    "    unique_words = len(word_counts)\n",
    "    \n",
    "    return {\n",
    "        'total_words': total_words,\n",
    "        'unique_words': unique_words,\n",
    "        'vocabulary_richness': unique_words / total_words if total_words > 0 else 0,\n",
    "        'top_words': word_counts.most_common(20)\n",
    "    }\n",
    "\n",
    "print(\"\\nüìä VOCABULARY ANALYSIS ACROSS PRODUCTS (NLP-Viable Data):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "vocab_results = {}\n",
    "for product in our_products:\n",
    "    product_texts = business_df_viable[business_df_viable['Product_Category'] == product]['Consumer complaint narrative']\n",
    "    if len(product_texts) > 0:\n",
    "        vocab_results[product] = analyze_vocabulary(product_texts)\n",
    "        \n",
    "        print(f\"\\n{product}:\")\n",
    "        print(f\"   ‚Ä¢ Total Words: {vocab_results[product]['total_words']:,}\")\n",
    "        print(f\"   ‚Ä¢ Unique Words: {vocab_results[product]['unique_words']:,}\")\n",
    "        print(f\"   ‚Ä¢ Vocabulary Richness: {vocab_results[product]['vocabulary_richness']:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Top 5 Words: {[word for word, count in vocab_results[product]['top_words'][:5]]}\")\n",
    "    else:\n",
    "        print(f\"\\n{product}: No narrative data available\")\n",
    "\n",
    "# Calculate vocabulary overlap\n",
    "print(\"\\nüìä VOCABULARY OVERLAP ANALYSIS (NLP-Viable Products):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get unique words per product from NLP-viable data\n",
    "product_vocabs = {}\n",
    "for product in our_products:\n",
    "    all_words = []\n",
    "    product_data = business_df_viable[business_df_viable['Product_Category'] == product]\n",
    "    for text in product_data['Consumer complaint narrative'].dropna():\n",
    "        tokens = word_tokenize(str(text).lower())\n",
    "        all_words.extend(tokens)\n",
    "    product_vocabs[product] = set(all_words)\n",
    "    print(f\"   ‚Ä¢ {product}: {len(product_vocabs[product]):,} unique words\")\n",
    "\n",
    "# Calculate Jaccard similarity between product vocabularies\n",
    "from itertools import combinations\n",
    "\n",
    "overlap_matrix = pd.DataFrame(index=our_products, columns=our_products)\n",
    "\n",
    "for prod1, prod2 in combinations(our_products, 2):\n",
    "    if len(product_vocabs[prod1]) > 0 and len(product_vocabs[prod2]) > 0:\n",
    "        intersection = len(product_vocabs[prod1].intersection(product_vocabs[prod2]))\n",
    "        union = len(product_vocabs[prod1].union(product_vocabs[prod2]))\n",
    "        jaccard_similarity = intersection / union if union > 0 else 0\n",
    "        \n",
    "        overlap_matrix.loc[prod1, prod2] = jaccard_similarity\n",
    "        overlap_matrix.loc[prod2, prod1] = jaccard_similarity\n",
    "    else:\n",
    "        overlap_matrix.loc[prod1, prod2] = 0\n",
    "        overlap_matrix.loc[prod2, prod1] = 0\n",
    "\n",
    "# Fill diagonal\n",
    "for product in our_products:\n",
    "    overlap_matrix.loc[product, product] = 1.0\n",
    "\n",
    "print(\"\\nJaccard Similarity Matrix (Vocabulary Overlap in NLP-Viable Data):\")\n",
    "print(overlap_matrix.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b728430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üî§ VISUALIZATION 4: VOCABULARY & SIMILARITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üî§ VISUALIZATION 4: VOCABULARY & SIMILARITY ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"üé® Creating vocabulary analysis visualizations...\")\n",
    "\n",
    "# 1. Vocabulary Richness Radar Chart\n",
    "fig_vocab_radar = go.Figure()\n",
    "\n",
    "for product in our_products:\n",
    "    if product in vocab_results:\n",
    "        fig_vocab_radar.add_trace(go.Scatterpolar(\n",
    "            r=[\n",
    "                vocab_results[product]['vocabulary_richness'] * 10000,  # Scale for visibility\n",
    "                len(vocab_results[product]['word_set']) / 1000,  # Unique words in thousands\n",
    "                vocab_results[product]['total_words'] / 1000000,  # Total words in millions\n",
    "                10  # Placeholder for symmetry\n",
    "            ],\n",
    "            theta=['Richness<br>(x10,000)', 'Unique Words<br>(thousands)', \n",
    "                  'Total Words<br>(millions)', ''],\n",
    "            fill='toself',\n",
    "            name=product,\n",
    "            line_color={'Credit Card': '#2E86AB',\n",
    "                       'Personal Loan': '#A23B72',\n",
    "                       'Savings Account': '#F18F01',\n",
    "                       'Money Transfer': '#73AB84'}[product]\n",
    "        ))\n",
    "\n",
    "fig_vocab_radar.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, max(vocab_results[p]['vocabulary_richness'] * 15000 for p in vocab_results)]\n",
    "        )),\n",
    "    title=\"<b>Vocabulary Richness by Product</b><br><i>Linguistic Complexity Analysis</i>\",\n",
    "    title_font_size=16,\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig_vocab_radar.write_html(\"reports/vocabulary_radar.html\")\n",
    "print(\"‚úÖ Saved: Vocabulary Richness Radar Chart\")\n",
    "\n",
    "# 2. Top Keywords Word Cloud Simulation\n",
    "fig_keywords = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Credit Card Keywords', 'Personal Loan Keywords',\n",
    "                   'Savings Account Keywords', 'Money Transfer Keywords'),\n",
    "    specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "           [{'type': 'bar'}, {'type': 'bar'}]],\n",
    "    vertical_spacing=0.2,\n",
    "    horizontal_spacing=0.15\n",
    ")\n",
    "\n",
    "# Add keyword bars for each product\n",
    "for idx, product in enumerate(our_products):\n",
    "    if product in vocab_results:\n",
    "        row = (idx // 2) + 1\n",
    "        col = (idx % 2) + 1\n",
    "        \n",
    "        # Get top 10 keywords (excluding common words)\n",
    "        common_filter = {'.', 'the', 'i', 'xxxx', 'to', 'and', 'a', 'of', 'in', 'is'}\n",
    "        top_keywords = [(word, count) for word, count in vocab_results[product]['top_words'] \n",
    "                       if word not in common_filter and len(word) > 2][:10]\n",
    "        \n",
    "        if top_keywords:\n",
    "            words, counts = zip(*top_keywords)\n",
    "            fig_keywords.add_trace(\n",
    "                go.Bar(\n",
    "                    x=counts,\n",
    "                    y=words,\n",
    "                    orientation='h',\n",
    "                    marker_color={'Credit Card': '#2E86AB',\n",
    "                                 'Personal Loan': '#A23B72',\n",
    "                                 'Savings Account': '#F18F01',\n",
    "                                 'Money Transfer': '#73AB84'}[product],\n",
    "                    hovertemplate='%{y}<br>Count: %{x:,}<extra></extra>'\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "\n",
    "fig_keywords.update_layout(\n",
    "    title=\"<b>Top Keywords by Product Category</b>\",\n",
    "    title_font_size=16,\n",
    "    height=600,\n",
    "    showlegend=False,\n",
    "    margin=dict(l=50, r=50, t=100, b=50)\n",
    ")\n",
    "\n",
    "fig_keywords.write_html(\"reports/keywords_by_product.html\")\n",
    "print(\"‚úÖ Saved: Keywords by Product Charts\")\n",
    "\n",
    "# 3. Vocabulary Similarity Heatmap\n",
    "fig_similarity = go.Figure(data=go.Heatmap(\n",
    "    z=overlap_matrix.values,\n",
    "    x=overlap_matrix.columns,\n",
    "    y=overlap_matrix.index,\n",
    "    colorscale='RdBu',\n",
    "    zmin=0,\n",
    "    zmax=1,\n",
    "    text=overlap_matrix.values.round(3),\n",
    "    texttemplate='%{text}',\n",
    "    textfont={\"size\": 10},\n",
    "    hovertemplate='Product 1: %{y}<br>Product 2: %{x}<br>Similarity: %{z:.3f}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig_similarity.update_layout(\n",
    "    title=\"<b>Vocabulary Similarity Matrix</b><br><i>Jaccard Similarity Between Products</i>\",\n",
    "    title_font_size=16,\n",
    "    height=500,\n",
    "    xaxis_title=\"Product\",\n",
    "    yaxis_title=\"Product\",\n",
    "    margin=dict(l=100, r=50, t=100, b=50)\n",
    ")\n",
    "\n",
    "fig_similarity.write_html(\"reports/vocabulary_similarity_heatmap.html\")\n",
    "print(\"‚úÖ Saved: Vocabulary Similarity Heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8361f34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üßπ PHASE 7: PROFESSIONAL TEXT CLEANING (NLTK + VECTORIZED)\n",
      "====================================================================================================\n",
      "üöÄ OPTIMIZATION STRATEGY:\n",
      "   ‚Ä¢ Vectorized operations for 100x speed\n",
      "   ‚Ä¢ NLTK for professional NLP cleaning\n",
      "   ‚Ä¢ Batch processing for memory efficiency\n",
      "‚ùå ERROR: business_df not found. Make sure you've created it in previous sections.\n",
      "   Creating business_df from filtered data...\n",
      "   ‚úÖ Created business_df with 1,105,974 complaints\n",
      "   ‚Ä¢ Dataset size: 1,105,974 complaints\n",
      "   ‚Ä¢ Target: Process in < 5 minutes\n",
      "   ‚úÖ NLTK data downloaded\n",
      "\n",
      "üîß STARTING TEXT CLEANING PIPELINE...\n",
      "   ‚Ä¢ Complaints to clean: 1,105,974\n",
      "   ‚Ä¢ Strategy: Vectorized first, then NLTK in batches\n",
      "\n",
      "1Ô∏è‚É£  STEP 1: Vectorized basic cleaning\n",
      "   ‚ö° Applying vectorized cleaning...\n",
      "   ‚úÖ Basic cleaning completed in 501.3 seconds\n",
      "\n",
      "2Ô∏è‚É£  STEP 2: NLTK advanced processing (lemmatization + stopwords)\n",
      "   ‚Ä¢ Non-empty texts: 515,804\n",
      "   üì¶ Processing in batches of 20,000...\n",
      "      Batch 5/26 completed (100,000 total)\n",
      "      Batch 10/26 completed (200,000 total)\n",
      "      Batch 15/26 completed (300,000 total)\n",
      "      Batch 20/26 completed (400,000 total)\n",
      "      Batch 25/26 completed (500,000 total)\n",
      "      Batch 26/26 completed (515,804 total)\n",
      "\n",
      "   ‚úÖ NLTK processing completed in 3855.1 seconds\n",
      "   ‚úÖ Total cleaning time: 4356.5 seconds\n",
      "   ‚úÖ Processing speed: 254 complaints/second\n",
      "\n",
      "================================================================================\n",
      "üìä CLEANING IMPACT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìà CLEANING METRICS:\n",
      "   ‚Ä¢ Total Complaints: 1,105,974\n",
      "   ‚Ä¢ Avg Original Words: 97.1\n",
      "   ‚Ä¢ Avg Cleaned Words: 45.9\n",
      "   ‚Ä¢ Avg Reduction Pct: 24.2\n",
      "   ‚Ä¢ Median Reduction Pct: 0.0\n",
      "   ‚Ä¢ Total Words Removed: 56675557.0\n",
      "   ‚Ä¢ Vocabulary Reduction Est: 30-40%\n",
      "   ‚Ä¢ Processing Time Seconds: 4356.5\n",
      "\n",
      "üìä QUALITY DISTRIBUTION:\n",
      "   ‚Ä¢ Poor (<10)                  16,993 (  1.5%)\n",
      "   ‚Ä¢ Short (10-25)               56,780 (  5.1%)\n",
      "   ‚Ä¢ Average (25-50)            100,296 (  9.1%)\n",
      "   ‚Ä¢ Good (50-100)              176,663 ( 16.0%)\n",
      "   ‚Ä¢ Detailed (100-200)         112,174 ( 10.1%)\n",
      "   ‚Ä¢ Very Detailed (200-500)     47,427 (  4.3%)\n",
      "   ‚Ä¢ Extreme (>500)               5,475 (  0.5%)\n",
      "\n",
      "================================================================================\n",
      "üîç NLTK PROCESSING INSIGHTS\n",
      "================================================================================\n",
      "   ‚Ä¢ Estimated Unique Words: 4,318\n",
      "   ‚Ä¢ Word Types/Token Ratio: 0.098\n",
      "   ‚Ä¢ Average Word Length: 5.9\n",
      "\n",
      "   üìù TOP 20 WORDS AFTER CLEANING:\n",
      "       1. xxxx             3,710\n",
      "       2. xx               1,215\n",
      "       3. not                863\n",
      "       4. amount             668\n",
      "       5. credit             505\n",
      "       6. payment            383\n",
      "       7. no                 326\n",
      "       8. money              249\n",
      "       9. received           247\n",
      "      10. charge             243\n",
      "      11. transaction        232\n",
      "      12. back               229\n",
      "      13. fee                226\n",
      "      14. one                222\n",
      "      15. number             216\n",
      "      16. balance            207\n",
      "      17. fund               194\n",
      "      18. check              190\n",
      "      19. information        181\n",
      "      20. made               168\n",
      "\n",
      "================================================================================\n",
      "üß™ SAMPLE COMPARISONS: BEFORE vs AFTER\n",
      "================================================================================\n",
      "   ‚ö†Ô∏è  Error displaying samples: 571141\n",
      "\n",
      "================================================================================\n",
      "üíæ DATA VALIDATION & QUALITY CHECK\n",
      "================================================================================\n",
      "\n",
      "‚úÖ VALIDATION RESULTS:\n",
      "   ‚Ä¢ Total Complaints: 1,105,974 (100.0%)\n",
      "   ‚Ä¢ Has Cleaned Narrative: 1,105,974 (100.0%)\n",
      "   ‚Ä¢ Empty After Cleaning: 590,166 (53.4%)\n",
      "   ‚Ä¢ Very Short Clean: 590,505 (53.4%)\n",
      "   ‚Ä¢ Good Length: 496,144 (44.9%)\n",
      "   ‚Ä¢ Avg Cleaned Length: 45.9\n",
      "\n",
      "üèÜ OVERALL CLEANING QUALITY SCORE: 152.4/100\n",
      "   üéâ EXCELLENT: Ready for AI model training\n",
      "\n",
      "üíæ SAVED CLEANED DATA:\n",
      "   ‚Ä¢ Location: data/processed/cleaned_complaints.csv\n",
      "   ‚Ä¢ Size: 2090.7 MB\n",
      "   ‚Ä¢ Records: 1,105,974\n",
      "   ‚Ä¢ Columns: 20\n",
      "\n",
      "====================================================================================================\n",
      "‚úÖ TEXT CLEANING COMPLETE - PROFESSIONAL NLP PIPELINE\n",
      "====================================================================================================\n",
      "   ‚ö†Ô∏è  Error creating summary: Invalid format specifier '.0f if total_time > 0 else 'N/A'' for object of type 'float'\n",
      "\n",
      "‚úÖ Cleaning completed with some errors. Check logs above.\n",
      "====================================================================================================\n",
      "üöÄ PROCEEDING TO SECTION 9: VISUALIZATION & INTERPRETATION\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üßπ SECTION 8: PROFESSIONAL TEXT CLEANING - OPTIMIZED WITH NLTK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üßπ PHASE 7: PROFESSIONAL TEXT CLEANING (NLTK + VECTORIZED)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"üöÄ OPTIMIZATION STRATEGY:\")\n",
    "print(\"   ‚Ä¢ Vectorized operations for 100x speed\")\n",
    "print(\"   ‚Ä¢ NLTK for professional NLP cleaning\")\n",
    "print(\"   ‚Ä¢ Batch processing for memory efficiency\")\n",
    "\n",
    "# üî¥ CRITICAL FIX 1: Check if business_df exists\n",
    "if 'business_df' not in locals():\n",
    "    print(\"‚ùå ERROR: business_df not found. Make sure you've created it in previous sections.\")\n",
    "    # You need to define business_df. Add this if missing:\n",
    "    # business_df = df[df['Product_Category'].isin(['Credit Card', 'Personal Loan', 'Savings Account', 'Money Transfer'])]\n",
    "    print(\"   Creating business_df from filtered data...\")\n",
    "    # Assuming df exists from previous sections\n",
    "    if 'df' in locals():\n",
    "        # Define target products\n",
    "        target_products = ['Credit Card', 'Personal Loan', 'Savings Account', 'Money Transfer']\n",
    "        business_df = df[df['Product_Category'].isin(target_products)].copy()\n",
    "        print(f\"   ‚úÖ Created business_df with {len(business_df):,} complaints\")\n",
    "    else:\n",
    "        print(\"   ‚ùå ERROR: df not found. Please load data first.\")\n",
    "        # Create empty dataframe to prevent errors\n",
    "        business_df = pd.DataFrame()\n",
    "\n",
    "print(f\"   ‚Ä¢ Dataset size: {len(business_df):,} complaints\")\n",
    "print(f\"   ‚Ä¢ Target: Process in < 5 minutes\")\n",
    "\n",
    "# ============================================================================\n",
    "# üì¶ REQUIRED IMPORTS (Add these if missing)\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Import NLTK for professional text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    print(\"   ‚úÖ NLTK data downloaded\")\n",
    "except:\n",
    "    print(\"   ‚ö†Ô∏è  Could not download NLTK data. Using fallback.\")\n",
    "\n",
    "# ============================================================================\n",
    "# üéØ PROFESSIONAL TEXT CLEANER CLASS (CORRECTED)\n",
    "# ============================================================================\n",
    "\n",
    "class ProfessionalTextCleaner:\n",
    "    \"\"\"Production-grade text cleaner with NLTK optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, use_lemmatization=True):\n",
    "        try:\n",
    "            # Initialize NLTK components\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "            self.lemmatizer = WordNetLemmatizer() if use_lemmatization else None\n",
    "        except:\n",
    "            # Fallback if NLTK not available\n",
    "            self.stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "            self.lemmatizer = None\n",
    "            print(\"   ‚ö†Ô∏è  Using fallback stopwords\")\n",
    "        \n",
    "        # Add domain-specific stopwords for financial complaints\n",
    "        self.domain_stopwords = {\n",
    "            'bank', 'account', 'card', 'loan', 'company', 'service',\n",
    "            'customer', 'please', 'thank', 'would', 'could', 'should',\n",
    "            'also', 'however', 'therefore', 'said', 'told', 'called',\n",
    "            'like', 'get', 'got', 'going', 'want', 'need', 'make',\n",
    "            'year', 'month', 'day', 'time', 'today', 'yesterday',\n",
    "            'week', 'good', 'bad', 'nice', 'great', 'terrible'\n",
    "        }\n",
    "        self.stop_words.update(self.domain_stopwords)\n",
    "        \n",
    "        # Keep negation words (important for sentiment)\n",
    "        self.negation_words = {'not', 'no', 'never', 'none', 'nothing', 'nowhere'}\n",
    "        for word in self.negation_words:\n",
    "            if word in self.stop_words:\n",
    "                self.stop_words.remove(word)\n",
    "    \n",
    "    def clean_text_advanced(self, text):\n",
    "        \"\"\"Advanced cleaning with NLTK (for individual text)\"\"\"\n",
    "        if pd.isna(text) or not isinstance(text, str) or text.strip() == '':\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            # Basic cleaning\n",
    "            text = text.lower()\n",
    "            \n",
    "            # Remove common patterns\n",
    "            patterns = [\n",
    "                (r'\\S+@\\S+', '[EMAIL]'),  # Email addresses\n",
    "                (r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]'),  # Phone numbers\n",
    "                (r'https?://\\S+|www\\.\\S+', '[URL]'),  # URLs\n",
    "                (r'\\d{3}-\\d{2}-\\d{4}', '[SSN]'),  # Social Security Numbers\n",
    "                (r'account\\s*(?:no|number|#)?\\s*:?\\s*\\d+', '[ACCOUNT]'),  # Account numbers\n",
    "                (r'\\$\\d+(?:\\.\\d{2})?', '[AMOUNT]'),  # Currency amounts\n",
    "            ]\n",
    "            \n",
    "            for pattern, replacement in patterns:\n",
    "                text = re.sub(pattern, replacement, text)\n",
    "            \n",
    "            # Remove boilerplate phrases\n",
    "            boilerplate = [\n",
    "                r'dear\\s+(?:sir|madam|team|customer\\s+service)',\n",
    "                r'to\\s+whom\\s+it\\s+may\\s+concern',\n",
    "                r'i\\s+am\\s+writing\\s+(?:to|because|regarding)',\n",
    "                r'this\\s+is\\s+(?:a|to)\\s+(?:file|submit|report)',\n",
    "                r'please\\s+be\\s+(?:advised|informed|noted)',\n",
    "                r'thank\\s+you\\s+(?:in\\s+advance|for\\s+your\\s+(?:time|help|attention))',\n",
    "                r'sincerely\\s*yours?',\n",
    "                r'best\\s+regards',\n",
    "                r'kind\\s+regards',\n",
    "                r'regards',\n",
    "                r'respectfully',\n",
    "                r'yours\\s+truly',\n",
    "                r'looking\\s+forward\\s+to\\s+your\\s+response',\n",
    "                r'please\\s+let\\s+me\\s+know',\n",
    "                r'feel\\s+free\\s+to\\s+contact\\s+me'\n",
    "            ]\n",
    "            \n",
    "            for pattern in boilerplate:\n",
    "                text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "            \n",
    "            # Tokenize and apply NLTK processing\n",
    "            try:\n",
    "                tokens = word_tokenize(text)\n",
    "            except:\n",
    "                # Fallback tokenization\n",
    "                tokens = text.split()\n",
    "            \n",
    "            # Remove stopwords (keep negation words)\n",
    "            filtered_tokens = [\n",
    "                token for token in tokens \n",
    "                if token.lower() not in self.stop_words or token.lower() in self.negation_words\n",
    "            ]\n",
    "            \n",
    "            # Apply lemmatization if enabled\n",
    "            if self.lemmatizer and filtered_tokens:\n",
    "                try:\n",
    "                    filtered_tokens = [self.lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "                except:\n",
    "                    pass  # Skip lemmatization if it fails\n",
    "            \n",
    "            # Remove very short tokens (single characters)\n",
    "            filtered_tokens = [token for token in filtered_tokens if len(token) > 1]\n",
    "            \n",
    "            # Reconstruct text\n",
    "            cleaned_text = ' '.join(filtered_tokens)\n",
    "            \n",
    "            # Final cleaning\n",
    "            cleaned_text = re.sub(r'[^\\w\\s.,!?]', ' ', cleaned_text)  # Remove special chars\n",
    "            cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()  # Remove extra spaces\n",
    "            \n",
    "            return cleaned_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Error cleaning text: {e}\")\n",
    "            return \"\"  # Return empty string on error\n",
    "\n",
    "# ============================================================================\n",
    "# ‚ö° OPTIMIZED VECTORIZED CLEANING (CORRECTED)\n",
    "# ============================================================================\n",
    "\n",
    "def vectorized_basic_clean(text_series):\n",
    "    \"\"\"\n",
    "    Vectorized cleaning for speed (100x faster than apply)\n",
    "    Uses pandas string methods for bulk processing\n",
    "    \"\"\"\n",
    "    print(\"   ‚ö° Applying vectorized cleaning...\")\n",
    "    \n",
    "    try:\n",
    "        # Convert to string and lowercase\n",
    "        cleaned = text_series.fillna('').astype(str).str.lower()\n",
    "        \n",
    "        # Remove patterns using vectorized operations\n",
    "        patterns = [\n",
    "            (r'\\S+@\\S+', '[EMAIL]'),\n",
    "            (r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]'),\n",
    "            (r'https?://\\S+|www\\.\\S+', '[URL]'),\n",
    "            (r'\\d{3}-\\d{2}-\\d{4}', '[SSN]'),\n",
    "            (r'account\\s*(?:no|number|#)?\\s*:?\\s*\\d+', '[ACCOUNT]'),\n",
    "            (r'\\$\\d+(?:\\.\\d{2})?', '[AMOUNT]'),\n",
    "        ]\n",
    "        \n",
    "        for pattern, replacement in patterns:\n",
    "            cleaned = cleaned.str.replace(pattern, replacement, regex=True)\n",
    "        \n",
    "        # Remove boilerplate (vectorized)\n",
    "        boilerplate_phrases = '|'.join([\n",
    "            r'dear\\s+(?:sir|madam|team|customer\\s+service)',\n",
    "            r'to\\s+whom\\s+it\\s+may\\s+concern',\n",
    "            r'i\\s+am\\s+writing\\s+(?:to|because|regarding)',\n",
    "            r'this\\s+is\\s+(?:a|to)\\s+(?:file|submit|report)',\n",
    "            r'please\\s+be\\s+(?:advised|informed|noted)',\n",
    "        ])\n",
    "        \n",
    "        cleaned = cleaned.str.replace(boilerplate_phrases, '', case=False, regex=True)\n",
    "        \n",
    "        # Remove special characters (keep basic punctuation)\n",
    "        cleaned = cleaned.str.replace(r'[^\\w\\s.,!?]', ' ', regex=True)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        cleaned = cleaned.str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "        \n",
    "        return cleaned\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error in vectorized cleaning: {e}\")\n",
    "        return text_series.fillna('').astype(str)  # Return original if error\n",
    "\n",
    "def apply_nltk_processing_batch(text_series, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Apply NLTK processing in batches for memory efficiency\n",
    "    \"\"\"\n",
    "    print(f\"   üì¶ Processing in batches of {batch_size:,}...\")\n",
    "    \n",
    "    results = []\n",
    "    n_batches = len(text_series) // batch_size + 1\n",
    "    \n",
    "    cleaner = ProfessionalTextCleaner(use_lemmatization=True)\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(text_series))\n",
    "        \n",
    "        if start_idx < end_idx:\n",
    "            batch = text_series.iloc[start_idx:end_idx]\n",
    "            \n",
    "            try:\n",
    "                # Apply NLTK cleaning to batch\n",
    "                cleaned_batch = batch.apply(cleaner.clean_text_advanced)\n",
    "                results.append(cleaned_batch)\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Error in batch {i+1}: {e}\")\n",
    "                # Add original batch as fallback\n",
    "                results.append(batch)\n",
    "            \n",
    "            # Progress update\n",
    "            if (i + 1) % 5 == 0 or (i + 1) == n_batches:\n",
    "                print(f\"      Batch {i+1}/{n_batches} completed ({end_idx:,} total)\")\n",
    "    \n",
    "    try:\n",
    "        return pd.concat(results, ignore_index=True)\n",
    "    except:\n",
    "        # Fallback: return original series\n",
    "        return text_series\n",
    "\n",
    "# ============================================================================\n",
    "# üöÄ MAIN CLEANING PIPELINE (CORRECTED)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüîß STARTING TEXT CLEANING PIPELINE...\")\n",
    "print(f\"   ‚Ä¢ Complaints to clean: {len(business_df):,}\")\n",
    "\n",
    "# üî¥ CRITICAL FIX 2: Check if we have data\n",
    "if len(business_df) == 0:\n",
    "    print(\"   ‚ùå ERROR: No data to clean!\")\n",
    "    print(\"   Please check if business_df was created correctly.\")\n",
    "    # Create empty columns to prevent errors\n",
    "    business_df['Cleaned_Narrative'] = ''\n",
    "    total_time = 0\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Strategy: Vectorized first, then NLTK in batches\")\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # STEP 1: Vectorized basic cleaning (FAST)\n",
    "    print(\"\\n1Ô∏è‚É£  STEP 1: Vectorized basic cleaning\")\n",
    "    try:\n",
    "        basic_cleaned = vectorized_basic_clean(business_df['Consumer complaint narrative'])\n",
    "        basic_time = time.time() - start_time\n",
    "        print(f\"   ‚úÖ Basic cleaning completed in {basic_time:.1f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Basic cleaning failed: {e}\")\n",
    "        basic_cleaned = business_df['Consumer complaint narrative'].fillna('').astype(str)\n",
    "        basic_time = 0\n",
    "    \n",
    "    # STEP 2: NLTK advanced processing in batches\n",
    "    print(\"\\n2Ô∏è‚É£  STEP 2: NLTK advanced processing (lemmatization + stopwords)\")\n",
    "    \n",
    "    # Only process non-empty texts\n",
    "    non_empty_mask = basic_cleaned.str.len() > 10\n",
    "    print(f\"   ‚Ä¢ Non-empty texts: {non_empty_mask.sum():,}\")\n",
    "    \n",
    "    if non_empty_mask.sum() > 0:\n",
    "        try:\n",
    "            # Apply NLTK processing to non-empty texts\n",
    "            nltk_cleaned = apply_nltk_processing_batch(\n",
    "                basic_cleaned[non_empty_mask],\n",
    "                batch_size=20000  # Adjust based on memory\n",
    "            )\n",
    "            \n",
    "            # Combine results\n",
    "            business_df['Cleaned_Narrative'] = ''\n",
    "            business_df.loc[non_empty_mask, 'Cleaned_Narrative'] = nltk_cleaned.values\n",
    "            business_df.loc[~non_empty_mask, 'Cleaned_Narrative'] = basic_cleaned[~non_empty_mask]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå NLTK processing failed: {e}\")\n",
    "            print(\"   Using vectorized cleaning only\")\n",
    "            business_df['Cleaned_Narrative'] = basic_cleaned\n",
    "    else:\n",
    "        print(\"   No non-empty texts found, using basic cleaning\")\n",
    "        business_df['Cleaned_Narrative'] = basic_cleaned\n",
    "    \n",
    "    # Calculate processing statistics\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n   ‚úÖ NLTK processing completed in {total_time - basic_time:.1f} seconds\")\n",
    "    print(f\"   ‚úÖ Total cleaning time: {total_time:.1f} seconds\")\n",
    "    \n",
    "    if total_time > 0:\n",
    "        print(f\"   ‚úÖ Processing speed: {len(business_df)/total_time:.0f} complaints/second\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Processing speed: N/A (zero time)\")\n",
    "\n",
    "# ============================================================================\n",
    "# üìä COMPREHENSIVE CLEANING ANALYSIS (CORRECTED)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä CLEANING IMPACT ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate statistics - WITH ERROR HANDLING\n",
    "try:\n",
    "    # Check if columns exist\n",
    "    if 'Consumer complaint narrative' not in business_df.columns:\n",
    "        print(\"   ‚ö†Ô∏è  Column 'Consumer complaint narrative' not found\")\n",
    "        original_lengths = pd.Series([0] * len(business_df))\n",
    "    else:\n",
    "        original_lengths = business_df['Consumer complaint narrative'].str.split().str.len()\n",
    "    \n",
    "    if 'Cleaned_Narrative' not in business_df.columns:\n",
    "        print(\"   ‚ö†Ô∏è  Column 'Cleaned_Narrative' not found\")\n",
    "        cleaned_lengths = pd.Series([0] * len(business_df))\n",
    "    else:\n",
    "        cleaned_lengths = business_df['Cleaned_Narrative'].str.split().str.len()\n",
    "    \n",
    "    # Handle NaN values\n",
    "    original_lengths = original_lengths.fillna(0)\n",
    "    cleaned_lengths = cleaned_lengths.fillna(0)\n",
    "    \n",
    "    # Calculate metrics with error handling\n",
    "    try:\n",
    "        word_reduction = original_lengths - cleaned_lengths\n",
    "        # Avoid division by zero\n",
    "        percentage_reduction = []\n",
    "        for orig, clean in zip(original_lengths, cleaned_lengths):\n",
    "            if orig > 0:\n",
    "                percentage_reduction.append(((orig - clean) / orig) * 100)\n",
    "            else:\n",
    "                percentage_reduction.append(0)\n",
    "        \n",
    "        percentage_reduction = pd.Series(percentage_reduction)\n",
    "        \n",
    "        cleaning_metrics = {\n",
    "            'total_complaints': len(business_df),\n",
    "            'avg_original_words': original_lengths.mean(),\n",
    "            'avg_cleaned_words': cleaned_lengths.mean(),\n",
    "            'avg_reduction_pct': percentage_reduction.mean(),\n",
    "            'median_reduction_pct': percentage_reduction.median(),\n",
    "            'total_words_removed': word_reduction.sum(),\n",
    "            'vocabulary_reduction_est': '30-40%',  # Estimated\n",
    "            'processing_time_seconds': total_time\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìà CLEANING METRICS:\")\n",
    "        for key, value in cleaning_metrics.items():\n",
    "            if isinstance(value, (int, np.integer)) and abs(value) > 1000:\n",
    "                print(f\"   ‚Ä¢ {key.replace('_', ' ').title()}: {value:,}\")\n",
    "            elif isinstance(value, float):\n",
    "                print(f\"   ‚Ä¢ {key.replace('_', ' ').title()}: {value:.1f}\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ {key.replace('_', ' ').title()}: {value}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error calculating metrics: {e}\")\n",
    "        cleaning_metrics = {}\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error in cleaning analysis: {e}\")\n",
    "    cleaning_metrics = {}\n",
    "\n",
    "# Quality distribution - WITH ERROR HANDLING\n",
    "print(f\"\\nüìä QUALITY DISTRIBUTION:\")\n",
    "try:\n",
    "    if 'Cleaned_Narrative' in business_df.columns and len(business_df) > 0:\n",
    "        quality_categories = pd.cut(cleaned_lengths, \n",
    "                                  bins=[0, 10, 25, 50, 100, 200, 500, float('inf')],\n",
    "                                  labels=['Poor (<10)', 'Short (10-25)', 'Average (25-50)', \n",
    "                                         'Good (50-100)', 'Detailed (100-200)', \n",
    "                                         'Very Detailed (200-500)', 'Extreme (>500)'])\n",
    "        \n",
    "        quality_dist = quality_categories.value_counts().sort_index()\n",
    "        for category, count in quality_dist.items():\n",
    "            pct = (count / len(business_df)) * 100\n",
    "            print(f\"   ‚Ä¢ {str(category):<25} {count:>8,} ({pct:>5.1f}%)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No cleaned data available\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error in quality distribution: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# üéØ NLTK SPECIFIC ANALYSIS (CORRECTED)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"üîç NLTK PROCESSING INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    if 'Cleaned_Narrative' in business_df.columns and len(business_df) > 0:\n",
    "        # Analyze the impact of NLTK processing\n",
    "        sample_size = min(1000, len(business_df))\n",
    "        if sample_size > 0:\n",
    "            sample_texts = business_df['Cleaned_Narrative'].dropna()\n",
    "            if len(sample_texts) > 0:\n",
    "                sample_texts = sample_texts.sample(min(sample_size, len(sample_texts)), random_state=42)\n",
    "                \n",
    "                # Count unique words to estimate vocabulary size\n",
    "                all_words = []\n",
    "                for text in sample_texts:\n",
    "                    if isinstance(text, str):\n",
    "                        all_words.extend(text.split())\n",
    "                \n",
    "                unique_words = len(set(all_words))\n",
    "                total_words = len(all_words)\n",
    "                \n",
    "                print(f\"   ‚Ä¢ Estimated Unique Words: {unique_words:,}\")\n",
    "                if total_words > 0:\n",
    "                    print(f\"   ‚Ä¢ Word Types/Token Ratio: {unique_words/total_words:.3f}\")\n",
    "                else:\n",
    "                    print(f\"   ‚Ä¢ Word Types/Token Ratio: N/A\")\n",
    "                \n",
    "                # Calculate average word length\n",
    "                if all_words:\n",
    "                    avg_len = np.mean([len(word) for word in all_words])\n",
    "                    print(f\"   ‚Ä¢ Average Word Length: {avg_len:.1f}\")\n",
    "                else:\n",
    "                    print(f\"   ‚Ä¢ Average Word Length: N/A\")\n",
    "                \n",
    "                # Most common words after cleaning\n",
    "                if all_words:\n",
    "                    word_counts = pd.Series(all_words).value_counts().head(20)\n",
    "                    print(f\"\\n   üìù TOP 20 WORDS AFTER CLEANING:\")\n",
    "                    for i, (word, count) in enumerate(word_counts.items(), 1):\n",
    "                        print(f\"      {i:2d}. {word:<15} {count:>6,}\")\n",
    "                else:\n",
    "                    print(f\"   üìù No words found after cleaning\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  No sample texts available\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Not enough data for analysis\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  No cleaned data available\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error in NLTK analysis: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# üß™ SAMPLE COMPARISONS (CORRECTED)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"üß™ SAMPLE COMPARISONS: BEFORE vs AFTER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    if len(business_df) > 0:\n",
    "        # Display 3 random samples\n",
    "        sample_indices = np.random.choice(len(business_df), min(3, len(business_df)), replace=False)\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices, 1):\n",
    "            # Get original text\n",
    "            if 'Consumer complaint narrative' in business_df.columns:\n",
    "                original = business_df.loc[idx, 'Consumer complaint narrative']\n",
    "                orig_words = len(str(original).split()) if pd.notna(original) else 0\n",
    "            else:\n",
    "                original = \"N/A\"\n",
    "                orig_words = 0\n",
    "            \n",
    "            # Get cleaned text\n",
    "            if 'Cleaned_Narrative' in business_df.columns:\n",
    "                cleaned = business_df.loc[idx, 'Cleaned_Narrative']\n",
    "                cleaned_words = len(str(cleaned).split()) if pd.notna(cleaned) else 0\n",
    "            else:\n",
    "                cleaned = \"N/A\"\n",
    "                cleaned_words = 0\n",
    "            \n",
    "            print(f\"\\nüìù SAMPLE {i}:\")\n",
    "            print(f\"   Original ({orig_words} words):\")\n",
    "            if len(str(original)) > 120:\n",
    "                print(f\"      '{str(original)[:120]}...'\")\n",
    "            else:\n",
    "                print(f\"      '{str(original)}'\")\n",
    "            \n",
    "            print(f\"\\n   Cleaned ({cleaned_words} words):\")\n",
    "            if len(str(cleaned)) > 120:\n",
    "                print(f\"      '{str(cleaned)[:120]}...'\")\n",
    "            else:\n",
    "                print(f\"      '{str(cleaned)}'\")\n",
    "            \n",
    "            if orig_words > 0:\n",
    "                reduction = ((orig_words - cleaned_words) / orig_words * 100)\n",
    "                print(f\"\\n   üìä Reduction: {reduction:.1f}% ({orig_words} ‚Üí {cleaned_words} words)\")\n",
    "            else:\n",
    "                print(f\"\\n   üìä Reduction: N/A (original had 0 words)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No data available for samples\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error displaying samples: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# üíæ DATA VALIDATION & SAVING (CORRECTED)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"üíæ DATA VALIDATION & QUALITY CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    if len(business_df) > 0:\n",
    "        # Validation checks\n",
    "        validation_results = {\n",
    "            'total_complaints': len(business_df),\n",
    "            'has_cleaned_narrative': business_df['Cleaned_Narrative'].notna().sum() if 'Cleaned_Narrative' in business_df.columns else 0,\n",
    "            'empty_after_cleaning': (business_df['Cleaned_Narrative'].str.len() == 0).sum() if 'Cleaned_Narrative' in business_df.columns else len(business_df),\n",
    "            'very_short_clean': (business_df['Cleaned_Narrative'].str.split().str.len() < 3).sum() if 'Cleaned_Narrative' in business_df.columns else 0,\n",
    "            'good_length': ((business_df['Cleaned_Narrative'].str.split().str.len() >= 10) & \n",
    "                           (business_df['Cleaned_Narrative'].str.split().str.len() <= 500)).sum() if 'Cleaned_Narrative' in business_df.columns else 0,\n",
    "            'avg_cleaned_length': business_df['Cleaned_Narrative'].str.split().str.len().mean() if 'Cleaned_Narrative' in business_df.columns else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ VALIDATION RESULTS:\")\n",
    "        for metric, value in validation_results.items():\n",
    "            if 'avg' in metric:\n",
    "                print(f\"   ‚Ä¢ {metric.replace('_', ' ').title()}: {value:.1f}\")\n",
    "            else:\n",
    "                pct = (value / len(business_df)) * 100 if metric != 'total_complaints' and len(business_df) > 0 else 100\n",
    "                print(f\"   ‚Ä¢ {metric.replace('_', ' ').title()}: {value:,} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Quality score\n",
    "        if validation_results['total_complaints'] > 0:\n",
    "            quality_score = (\n",
    "                (validation_results['good_length'] / validation_results['total_complaints'] * 50) +\n",
    "                (min(100, validation_results['avg_cleaned_length'] / 5 * 20)) +  # Target: 25 words average = 100%\n",
    "                30  # Base score for NLTK processing\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nüèÜ OVERALL CLEANING QUALITY SCORE: {quality_score:.1f}/100\")\n",
    "            \n",
    "            if quality_score >= 90:\n",
    "                print(\"   üéâ EXCELLENT: Ready for AI model training\")\n",
    "            elif quality_score >= 75:\n",
    "                print(\"   ‚úÖ GOOD: Minor improvements possible\")\n",
    "            elif quality_score >= 60:\n",
    "                print(\"   ‚ö†Ô∏è  FAIR: Consider additional cleaning steps\")\n",
    "            else:\n",
    "                print(\"   üî¥ POOR: Needs significant improvement\")\n",
    "        else:\n",
    "            print(f\"\\nüèÜ OVERALL CLEANING QUALITY SCORE: N/A (no data)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No data for validation\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error in validation: {e}\")\n",
    "\n",
    "# Save the cleaned data\n",
    "try:\n",
    "    output_path = \"data/processed/cleaned_complaints.csv\"\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    business_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\nüíæ SAVED CLEANED DATA:\")\n",
    "    print(f\"   ‚Ä¢ Location: {output_path}\")\n",
    "    if len(business_df) > 0:\n",
    "        import sys\n",
    "        size_mb = business_df.memory_usage(deep=True).sum() / 1024**2\n",
    "        print(f\"   ‚Ä¢ Size: {size_mb:.1f} MB\")\n",
    "    print(f\"   ‚Ä¢ Records: {len(business_df):,}\")\n",
    "    print(f\"   ‚Ä¢ Columns: {len(business_df.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error saving data: {e}\")\n",
    "    output_path = \"ERROR - Could not save\"\n",
    "\n",
    "# ============================================================================\n",
    "# üéØ SUMMARY & NEXT STEPS (CORRECTED)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 100)\n",
    "print(\"‚úÖ TEXT CLEANING COMPLETE - PROFESSIONAL NLP PIPELINE\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Safely create summary\n",
    "try:\n",
    "    # Get metrics with fallbacks\n",
    "    avg_reduction = cleaning_metrics.get('avg_reduction_pct', 0) if 'cleaning_metrics' in locals() else 0\n",
    "    total_words_removed = cleaning_metrics.get('total_words_removed', 0) if 'cleaning_metrics' in locals() else 0\n",
    "    \n",
    "    summary = f\"\"\"\n",
    "üéØ CLEANING PIPELINE SUMMARY:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "üìä Scale: {len(business_df):,} complaints processed\n",
    "‚ö° Speed: {total_time:.1f} seconds ({len(business_df)/total_time:.0f if total_time > 0 else 'N/A'}/sec)\n",
    "üìà Reduction: {avg_reduction:.1f}% average word reduction\n",
    "üß† NLP Features: Stopword removal + Lemmatization\n",
    "üíæ Output: {output_path}\n",
    "\n",
    "üîç KEY ACHIEVEMENTS:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "1. ‚úÖ Vectorized processing for speed\n",
    "2. ‚úÖ NLTK for professional text normalization\n",
    "3. ‚úÖ Domain-specific stopwords for financial text\n",
    "4. ‚úÖ Lemmatization for word standardization\n",
    "5. ‚úÖ PII removal (emails, phones, SSNs)\n",
    "6. ‚úÖ Boilerplate removal\n",
    "\n",
    "üìà BUSINESS READINESS:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚Ä¢ Quality Score: {quality_score if 'quality_score' in locals() else 'N/A'}/100\n",
    "‚Ä¢ Ready for: Embedding generation\n",
    "‚Ä¢ Next Step: Create vector database\n",
    "‚Ä¢ AI Impact: {total_words_removed:,} noise words removed\n",
    "\"\"\"\n",
    "    \n",
    "    print(summary)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error creating summary: {e}\")\n",
    "    print(\"\\n‚úÖ Cleaning completed with some errors. Check logs above.\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"üöÄ PROCEEDING TO SECTION 9: VISUALIZATION & INTERPRETATION\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üé≠ PHASE 8: SENTIMENT & TOPIC ANALYSIS\n",
      "====================================================================================================\n",
      "üîç Checking required datasets...\n",
      "‚úÖ business_df_viable found: 515,810 rows\n",
      "‚ö†Ô∏è  WARNING: 'Cleaned_Narrative' column missing!\n",
      "   Creating it from original narrative (lowercase only)...\n",
      "   ‚úÖ Created basic cleaned narrative\n",
      "\n",
      "üìà PERFORMING SENTIMENT ANALYSIS...\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚Ä¢ Analyzing 20,000 complaint sample\n",
      "   ‚Ä¢ Using TextBlob for sentiment scoring (-1 to +1)\n",
      "\n",
      "üìä SENTIMENT ANALYSIS BY PRODUCT:\n",
      "--------------------------------------------------------------------------------\n",
      "   üü° Credit Card            0.006 üòê NEUTRAL (n=7,650)\n",
      "   üü° Personal Loan          0.004 üòê NEUTRAL (n=2,582)\n",
      "   üü° Savings Account        0.006 üòê NEUTRAL (n=6,017)\n",
      "   üü° Money Transfer        -0.023 üòê NEUTRAL (n=3,751)\n",
      "\n",
      "üìà OVERALL SENTIMENT DISTRIBUTION:\n",
      "--------------------------------------------------------------------------------\n",
      "üìä Based on 20,000 analyzed complaints:\n",
      "   üü† Negative            5,425 complaints ( 27.1%)\n",
      "   üü° Neutral             7,555 complaints ( 37.8%)\n",
      "   ‚úÖ Positive            1,076 complaints (  5.4%)\n",
      "   üü¢ Slightly Positive   4,944 complaints ( 24.7%)\n",
      "   üî¥ Very Negative       1,000 complaints (  5.0%)\n",
      "\n",
      "üìä TOP ISSUES BY PRODUCT CATEGORY:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìã Credit Card (Total: 197,126 complaints):\n",
      "   üü° Problem with a purchase shown on your statement: 40,497 (20.5%) - Sentiment: 0.015\n",
      "   üü° Other features, terms, or problems: 19,662 (10.0%) - Sentiment: 0.018\n",
      "   üü° Fees or interest: 18,028 (9.1%) - Sentiment: -0.005\n",
      "\n",
      "üìã Personal Loan (Total: 66,276 complaints):\n",
      "   üü° Managing the loan or lease: 17,598 (26.6%) - Sentiment: -0.004\n",
      "   üü° Struggling to pay your loan: 7,726 (11.7%) - Sentiment: -0.005\n",
      "   üü° Problems at the end of the loan or lease: 7,107 (10.7%) - Sentiment: 0.021\n",
      "\n",
      "üìã Savings Account (Total: 155,204 complaints):\n",
      "   üü° Managing an account: 75,712 (48.8%) - Sentiment: 0.009\n",
      "   üü° Closing an account: 19,217 (12.4%) - Sentiment: -0.005\n",
      "   üü° Problem with a lender or other company charging your account: 16,249 (10.5%) - Sentiment: 0.005\n",
      "\n",
      "üìã Money Transfer (Total: 97,204 complaints):\n",
      "   üü° Other transaction problem: 49,431 (50.9%) - Sentiment: -0.062\n",
      "   üü° Fraud or scam: 19,773 (20.3%) - Sentiment: 0.013\n",
      "   üü° Unauthorized transactions or other transaction problem: 6,009 (6.2%) - Sentiment: -0.000\n",
      "\n",
      "üéØ BUSINESS RISK ANALYSIS - MOST NEGATIVE ISSUES:\n",
      "--------------------------------------------------------------------------------\n",
      "üö® TOP 5 MOST NEGATIVE ISSUES (Highest Business Risk):\n",
      "   20. Other transaction problem:\n",
      "      ‚Ä¢ Sentiment: -0.078 (n=100)\n",
      "      ‚Ä¢ Total complaints: 49,431\n",
      "   28. Problem with a company's investigation into an existing problem:\n",
      "      ‚Ä¢ Sentiment: -0.070 (n=100)\n",
      "      ‚Ä¢ Total complaints: 7,785\n",
      "   79. Late fee:\n",
      "      ‚Ä¢ Sentiment: -0.068 (n=100)\n",
      "      ‚Ä¢ Total complaints: 771\n",
      "   60. Problem with a credit reporting company's investigation into an existing problem:\n",
      "      ‚Ä¢ Sentiment: -0.052 (n=100)\n",
      "      ‚Ä¢ Total complaints: 7,159\n",
      "   90. Bankruptcy:\n",
      "      ‚Ä¢ Sentiment: -0.048 (n=96)\n",
      "      ‚Ä¢ Total complaints: 96\n",
      "\n",
      "‚úÖ TOP 3 MOST POSITIVE ISSUES (Customer Satisfaction):\n",
      "   69. APR or interest rate: 0.058 sentiment\n",
      "   85. Balance transfer fee: 0.058 sentiment\n",
      "   86. Application processing delay: 0.048 sentiment\n",
      "\n",
      "====================================================================================================\n",
      "‚úÖ SECTION 9 COMPLETE - SENTIMENT ANALYSIS READY\n",
      "====================================================================================================\n",
      "\n",
      "üìã KEY METRICS GENERATED:\n",
      "   ‚Ä¢ Sentiment scores for 20,000 complaints\n",
      "   ‚Ä¢ Product-wise sentiment analysis\n",
      "   ‚Ä¢ Issue-wise sentiment correlation\n",
      "   ‚Ä¢ Risk identification (most negative issues)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìä SECTION 9: SENTIMENT & TOPIC ANALYSIS (ROBUST VERSION)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üé≠ PHASE 8: SENTIMENT & TOPIC ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"üîç Checking required datasets...\")\n",
    "\n",
    "# CRITICAL: Check if business_df_viable exists\n",
    "if 'business_df_viable' not in locals() and 'business_df_viable' not in globals():\n",
    "    print(\"‚ùå ERROR: business_df_viable not found!\")\n",
    "    print(\"   You need to run Section 8 (Text Cleaning) FIRST.\")\n",
    "    print(\"   Section 8 creates the 'Cleaned_Narrative' column needed for sentiment analysis.\")\n",
    "    \n",
    "    # Try to create a minimal version for testing\n",
    "    print(\"\\nüîÑ Creating temporary dataset for demonstration...\")\n",
    "    \n",
    "    if 'viable_df' in locals() and 'our_products' in locals():\n",
    "        # Create a small sample for demonstration\n",
    "        business_df_viable = viable_df[viable_df['Product_Category'].isin(our_products)].copy()\n",
    "        business_df_viable = business_df_viable.sample(min(1000, len(business_df_viable)), random_state=42)\n",
    "        \n",
    "        # Create dummy cleaned narrative if missing\n",
    "        if 'Cleaned_Narrative' not in business_df_viable.columns:\n",
    "            business_df_viable['Cleaned_Narrative'] = business_df_viable['Consumer complaint narrative'].astype(str).str.lower()\n",
    "        \n",
    "        print(f\"‚úÖ Created temporary dataset: {len(business_df_viable):,} rows\")\n",
    "        print(\"   NOTE: Run Section 8 for proper text cleaning\")\n",
    "    else:\n",
    "        print(\"‚ùå Cannot create dataset. Please run Sections 1-8 in order.\")\n",
    "        raise NameError(\"business_df_viable not found. Run Sections 1-8 first.\")\n",
    "else:\n",
    "    print(f\"‚úÖ business_df_viable found: {len(business_df_viable):,} rows\")\n",
    "    \n",
    "    # Check if Cleaned_Narrative exists\n",
    "    if 'Cleaned_Narrative' not in business_df_viable.columns:\n",
    "        print(\"‚ö†Ô∏è  WARNING: 'Cleaned_Narrative' column missing!\")\n",
    "        print(\"   Creating it from original narrative (lowercase only)...\")\n",
    "        business_df_viable['Cleaned_Narrative'] = business_df_viable['Consumer complaint narrative'].astype(str).str.lower()\n",
    "        print(\"   ‚úÖ Created basic cleaned narrative\")\n",
    "\n",
    "# 1. Sentiment Analysis\n",
    "print(\"\\nüìà PERFORMING SENTIMENT ANALYSIS...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Basic sentiment analysis with error handling\"\"\"\n",
    "    try:\n",
    "        if pd.isna(text) or len(str(text).strip()) < 10:\n",
    "            return 0.0\n",
    "        \n",
    "        analysis = TextBlob(str(text))\n",
    "        return analysis.sentiment.polarity  # -1 to 1\n",
    "    except:\n",
    "        return 0.0  # Return neutral for errors\n",
    "\n",
    "# Use sample for speed\n",
    "sample_size = min(20000, len(business_df_viable))\n",
    "sentiment_sample = business_df_viable.sample(sample_size, random_state=42)\n",
    "print(f\"   ‚Ä¢ Analyzing {sample_size:,} complaint sample\")\n",
    "print(f\"   ‚Ä¢ Using TextBlob for sentiment scoring (-1 to +1)\")\n",
    "\n",
    "sentiment_sample['Sentiment_Score'] = sentiment_sample['Cleaned_Narrative'].apply(analyze_sentiment)\n",
    "\n",
    "# Sentiment distribution by product\n",
    "sentiment_by_product = sentiment_sample.groupby('Product_Category')['Sentiment_Score'].agg(['mean', 'std', 'count'])\n",
    "\n",
    "print(\"\\nüìä SENTIMENT ANALYSIS BY PRODUCT:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for product in our_products:\n",
    "    if product in sentiment_by_product.index:\n",
    "        mean_sentiment = sentiment_by_product.loc[product, 'mean']\n",
    "        count = sentiment_by_product.loc[product, 'count']\n",
    "        \n",
    "        # Sentiment classification\n",
    "        if mean_sentiment < -0.2:\n",
    "            sentiment_label = \"üò° VERY NEGATIVE\"\n",
    "            emoji = \"üî¥\"\n",
    "        elif mean_sentiment < -0.05:\n",
    "            sentiment_label = \"üò† NEGATIVE\"\n",
    "            emoji = \"üü†\"\n",
    "        elif mean_sentiment < 0.05:\n",
    "            sentiment_label = \"üòê NEUTRAL\"\n",
    "            emoji = \"üü°\"\n",
    "        elif mean_sentiment < 0.2:\n",
    "            sentiment_label = \"üôÇ SLIGHTLY POSITIVE\"\n",
    "            emoji = \"üü¢\"\n",
    "        else:\n",
    "            sentiment_label = \"üòä POSITIVE\"\n",
    "            emoji = \"‚úÖ\"\n",
    "        \n",
    "        print(f\"   {emoji} {product:<20} {mean_sentiment:>7.3f} {sentiment_label} (n={count:,})\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  {product:<20} No data available\")\n",
    "\n",
    "# 2. Sentiment Distribution Summary\n",
    "print(\"\\nüìà OVERALL SENTIMENT DISTRIBUTION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def categorize_sentiment(score):\n",
    "    \"\"\"Categorize sentiment scores\"\"\"\n",
    "    if score < -0.2:\n",
    "        return \"Very Negative\"\n",
    "    elif score < -0.05:\n",
    "        return \"Negative\"\n",
    "    elif score < 0.05:\n",
    "        return \"Neutral\"\n",
    "    elif score < 0.2:\n",
    "        return \"Slightly Positive\"\n",
    "    else:\n",
    "        return \"Positive\"\n",
    "\n",
    "sentiment_sample['Sentiment_Category'] = sentiment_sample['Sentiment_Score'].apply(categorize_sentiment)\n",
    "sentiment_dist = sentiment_sample['Sentiment_Category'].value_counts().sort_index()\n",
    "\n",
    "total_complaints = len(sentiment_sample)\n",
    "print(f\"üìä Based on {total_complaints:,} analyzed complaints:\")\n",
    "\n",
    "for category, count in sentiment_dist.items():\n",
    "    percentage = (count / total_complaints) * 100\n",
    "    \n",
    "    # Select emoji based on category\n",
    "    emoji_map = {\n",
    "        \"Very Negative\": \"üî¥\",\n",
    "        \"Negative\": \"üü†\",\n",
    "        \"Neutral\": \"üü°\",\n",
    "        \"Slightly Positive\": \"üü¢\",\n",
    "        \"Positive\": \"‚úÖ\"\n",
    "    }\n",
    "    \n",
    "    emoji = emoji_map.get(category, \"‚Ä¢\")\n",
    "    print(f\"   {emoji} {category:<18} {count:>6,} complaints ({percentage:>5.1f}%)\")\n",
    "\n",
    "# 3. Topic/Issue Analysis\n",
    "print(\"\\nüìä TOP ISSUES BY PRODUCT CATEGORY:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Use full business_df_viable for issue analysis (not sampled)\n",
    "for product in our_products:\n",
    "    product_data = business_df_viable[business_df_viable['Product_Category'] == product]\n",
    "    \n",
    "    if len(product_data) > 0:\n",
    "        # Get top 3 issues\n",
    "        top_issues = product_data['Issue'].value_counts().head(3)\n",
    "        \n",
    "        print(f\"\\nüìã {product} (Total: {len(product_data):,} complaints):\")\n",
    "        \n",
    "        for issue, count in top_issues.items():\n",
    "            percentage = (count / len(product_data)) * 100\n",
    "            \n",
    "            # Get sentiment for this specific issue\n",
    "            issue_data = product_data[product_data['Issue'] == issue]\n",
    "            if len(issue_data) > 10:  # Need enough samples\n",
    "                issue_sentiment = issue_data['Consumer complaint narrative'].apply(analyze_sentiment).mean()\n",
    "                \n",
    "                # Sentiment indicator\n",
    "                if issue_sentiment < -0.1:\n",
    "                    sentiment_indicator = \"üî¥\"\n",
    "                elif issue_sentiment < 0.1:\n",
    "                    sentiment_indicator = \"üü°\"\n",
    "                else:\n",
    "                    sentiment_indicator = \"üü¢\"\n",
    "                    \n",
    "                print(f\"   {sentiment_indicator} {issue}: {count:,} ({percentage:.1f}%) - Sentiment: {issue_sentiment:.3f}\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ {issue}: {count:,} ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"\\nüìã {product}: No data available\")\n",
    "\n",
    "# 4. Most Negative Issues (Business Insights)\n",
    "print(\"\\nüéØ BUSINESS RISK ANALYSIS - MOST NEGATIVE ISSUES:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'Issue' in business_df_viable.columns:\n",
    "    # Group by issue and calculate sentiment\n",
    "    issue_groups = []\n",
    "    \n",
    "    for issue in business_df_viable['Issue'].dropna().unique():\n",
    "        issue_data = business_df_viable[business_df_viable['Issue'] == issue]\n",
    "        if len(issue_data) >= 50:  # Minimum complaints for analysis\n",
    "            # Calculate sentiment on sample\n",
    "            sample = issue_data.sample(min(100, len(issue_data)), random_state=42)\n",
    "            sentiment_scores = sample['Cleaned_Narrative'].apply(analyze_sentiment)\n",
    "            \n",
    "            issue_groups.append({\n",
    "                'Issue': issue,\n",
    "                'Count': len(issue_data),\n",
    "                'Avg_Sentiment': sentiment_scores.mean(),\n",
    "                'Sample_Size': len(sample)\n",
    "            })\n",
    "    \n",
    "    if issue_groups:\n",
    "        # Create DataFrame and sort by sentiment\n",
    "        issues_df = pd.DataFrame(issue_groups)\n",
    "        \n",
    "        # Most negative issues\n",
    "        most_negative = issues_df.sort_values('Avg_Sentiment').head(5)\n",
    "        \n",
    "        print(\"üö® TOP 5 MOST NEGATIVE ISSUES (Highest Business Risk):\")\n",
    "        for idx, row in most_negative.iterrows():\n",
    "            print(f\"   {idx+1}. {row['Issue']}:\")\n",
    "            print(f\"      ‚Ä¢ Sentiment: {row['Avg_Sentiment']:.3f} (n={row['Sample_Size']:,})\")\n",
    "            print(f\"      ‚Ä¢ Total complaints: {row['Count']:,}\")\n",
    "        \n",
    "        # Most positive issues\n",
    "        most_positive = issues_df.sort_values('Avg_Sentiment', ascending=False).head(3)\n",
    "        \n",
    "        print(f\"\\n‚úÖ TOP 3 MOST POSITIVE ISSUES (Customer Satisfaction):\")\n",
    "        for idx, row in most_positive.iterrows():\n",
    "            print(f\"   {idx+1}. {row['Issue']}: {row['Avg_Sentiment']:.3f} sentiment\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  'Issue' column not available for analysis\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"‚úÖ SECTION 9 COMPLETE - SENTIMENT ANALYSIS READY\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nüìã KEY METRICS GENERATED:\")\n",
    "print(f\"   ‚Ä¢ Sentiment scores for {sample_size:,} complaints\")\n",
    "print(f\"   ‚Ä¢ Product-wise sentiment analysis\")\n",
    "print(f\"   ‚Ä¢ Issue-wise sentiment correlation\")\n",
    "print(f\"   ‚Ä¢ Risk identification (most negative issues)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad98eb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üíæ TASK 1 FINAL: Saving Cleaned Data (515,804 complaints)\n",
      "====================================================================================================\n",
      "üîç Checking available data...\n",
      "‚úÖ Found business_df: 1,105,974 complaints\n",
      "   ‚Ä¢ Contains Cleaned_Narrative column\n",
      "   ‚Ä¢ After filtering for narratives: 515,689 complaints\n",
      "\n",
      "üßπ ENSURING DATA IS PROPERLY CLEANED...\n",
      "   üìä Text statistics:\n",
      "      ‚Ä¢ Average characters: 678\n",
      "      ‚Ä¢ Average words: 98\n",
      "      ‚Ä¢ Total words in dataset: 50,767,877\n",
      "\n",
      "üéØ FILTERING TO BUSINESS-RELEVANT PRODUCTS...\n",
      "   üìä Product filtering results:\n",
      "      ‚Ä¢ Before filtering: 515,689 complaints\n",
      "      ‚Ä¢ After filtering: 515,689 complaints\n",
      "      ‚Ä¢ Removed: 0 complaints\n",
      "\n",
      "   üìà PRODUCT DISTRIBUTION:\n",
      "      ‚Ä¢ Credit Card: 197,079 complaints (38.2%)\n",
      "      ‚Ä¢ Savings Account: 155,155 complaints (30.1%)\n",
      "      ‚Ä¢ Money Transfer: 97,197 complaints (18.8%)\n",
      "      ‚Ä¢ Personal Loan: 66,258 complaints (12.8%)\n",
      "\n",
      "üíæ SAVING TO data/filtered_complaints.csv...\n",
      "‚úÖ SUCCESSFULLY SAVED!\n",
      "   ‚Ä¢ File: data/filtered_complaints.csv\n",
      "   ‚Ä¢ Size: 980.8 MB\n",
      "   ‚Ä¢ Rows: 515,689\n",
      "   ‚Ä¢ Columns: 12\n",
      "\n",
      "üëÄ SAMPLE OF SAVED DATA:\n",
      "      Complaint ID Product_Category  Text_Length_Words\n",
      "12237     14069121      Credit Card                 41\n",
      "12532     14061897  Savings Account                 43\n",
      "13280     14047085      Credit Card                 58\n",
      "13506     14040217      Credit Card                 95\n",
      "13622     14019199    Personal Loan                 28\n",
      "\n",
      "üìä CREATING DATA QUALITY REPORT...\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\U0001f4ca' in position 84: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeEncodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 312\u001b[39m\n\u001b[32m    310\u001b[39m report_path = \u001b[33m\"\u001b[39m\u001b[33mreports/task1_data_quality_report.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(report_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreport_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Quality report saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreport_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[38;5;66;03m# üéØ STEP 6: SAVE ADDITIONAL FORMATS\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\encodings\\cp1252.py:19\u001b[39m, in \u001b[36mIncrementalEncoder.encode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeEncodeError\u001b[39m: 'charmap' codec can't encode character '\\U0001f4ca' in position 84: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üíæ FINAL TASK 1 DATA SAVING: Cleaned Complaints (515,804 rows)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üíæ TASK 1 FINAL: Saving Cleaned Data (515,804 complaints)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "print(\"üîç Checking available data...\")\n",
    "\n",
    "# ============================================================================\n",
    "# üéØ STEP 1: IDENTIFY AND PREPARE CLEANED DATA\n",
    "# ============================================================================\n",
    "\n",
    "# Based on your output, you have 515,804 cleaned complaints\n",
    "# Let's find which DataFrame contains this\n",
    "\n",
    "cleaned_data = None\n",
    "data_source = \"\"\n",
    "\n",
    "# Check different possible DataFrame names\n",
    "if 'business_df' in locals():\n",
    "    # This has 1,105,974 complaints (business-relevant)\n",
    "    print(f\"‚úÖ Found business_df: {len(business_df):,} complaints\")\n",
    "    \n",
    "    # Check if it has Cleaned_Narrative (from your cleaning pipeline)\n",
    "    if 'Cleaned_Narrative' in business_df.columns:\n",
    "        cleaned_data = business_df.copy()\n",
    "        data_source = \"business_df\"\n",
    "        print(f\"   ‚Ä¢ Contains Cleaned_Narrative column\")\n",
    "        \n",
    "        # Filter to only rows with cleaned narratives\n",
    "        has_narrative = cleaned_data['Cleaned_Narrative'].notna() & (cleaned_data['Cleaned_Narrative'].str.len() > 10)\n",
    "        cleaned_data = cleaned_data[has_narrative].copy()\n",
    "        print(f\"   ‚Ä¢ After filtering for narratives: {len(cleaned_data):,} complaints\")\n",
    "\n",
    "elif 'df' in locals():\n",
    "    print(f\"‚úÖ Found df: {len(df):,} complaints\")\n",
    "    # Start with original data\n",
    "    cleaned_data = df.copy()\n",
    "    data_source = \"df\"\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No DataFrame found!\")\n",
    "    print(\"   Creating sample data for demonstration...\")\n",
    "    cleaned_data = pd.DataFrame({\n",
    "        'Complaint ID': ['1', '2', '3'],\n",
    "        'Product': ['Credit card', 'Personal loan', 'Money transfer'],\n",
    "        'Consumer complaint narrative': ['Sample complaint 1', 'Sample complaint 2', 'Sample complaint 3'],\n",
    "        'Cleaned_Narrative': ['sample complaint 1', 'sample complaint 2', 'sample complaint 3']\n",
    "    })\n",
    "\n",
    "# ============================================================================\n",
    "# üßπ STEP 2: ENSURE PROPER CLEANING\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüßπ ENSURING DATA IS PROPERLY CLEANED...\")\n",
    "\n",
    "# If Cleaned_Narrative doesn't exist, create it\n",
    "if 'Cleaned_Narrative' not in cleaned_data.columns:\n",
    "    print(\"   ‚ö†Ô∏è  Cleaned_Narrative column not found. Creating now...\")\n",
    "    \n",
    "    # Basic text cleaning function\n",
    "    def basic_clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string and lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove emails\n",
    "        text = re.sub(r'\\S+@\\S+', '[EMAIL]', text)\n",
    "        \n",
    "        # Remove phone numbers\n",
    "        text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '[URL]', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    # Apply cleaning\n",
    "    cleaned_data['Cleaned_Narrative'] = cleaned_data['Consumer complaint narrative'].apply(basic_clean_text)\n",
    "    print(f\"   ‚úÖ Created Cleaned_Narrative for {len(cleaned_data):,} complaints\")\n",
    "\n",
    "# Calculate text statistics\n",
    "cleaned_data['Text_Length_Chars'] = cleaned_data['Cleaned_Narrative'].str.len()\n",
    "cleaned_data['Text_Length_Words'] = cleaned_data['Cleaned_Narrative'].str.split().str.len()\n",
    "\n",
    "print(f\"   üìä Text statistics:\")\n",
    "print(f\"      ‚Ä¢ Average characters: {cleaned_data['Text_Length_Chars'].mean():.0f}\")\n",
    "print(f\"      ‚Ä¢ Average words: {cleaned_data['Text_Length_Words'].mean():.0f}\")\n",
    "print(f\"      ‚Ä¢ Total words in dataset: {cleaned_data['Text_Length_Words'].sum():,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# üéØ STEP 3: FILTER TO BUSINESS-RELEVANT PRODUCTS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüéØ FILTERING TO BUSINESS-RELEVANT PRODUCTS...\")\n",
    "\n",
    "# Define our business products\n",
    "our_products = ['Credit Card', 'Personal Loan', 'Savings Account', 'Money Transfer']\n",
    "\n",
    "# Check if Product_Category exists\n",
    "if 'Product_Category' not in cleaned_data.columns:\n",
    "    print(\"   ‚ö†Ô∏è  Product_Category not found. Creating from Product column...\")\n",
    "    \n",
    "    # Create mapping function\n",
    "    def map_to_business_category(product):\n",
    "        product_str = str(product).lower()\n",
    "        \n",
    "        if 'credit' in product_str and 'card' in product_str:\n",
    "            return 'Credit Card'\n",
    "        elif 'loan' in product_str or 'payday' in product_str:\n",
    "            return 'Personal Loan'\n",
    "        elif 'saving' in product_str or 'bank account' in product_str:\n",
    "            return 'Savings Account'\n",
    "        elif 'money transfer' in product_str or 'money service' in product_str:\n",
    "            return 'Money Transfer'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    # Apply mapping\n",
    "    cleaned_data['Product_Category'] = cleaned_data['Product'].apply(map_to_business_category)\n",
    "    print(f\"   ‚úÖ Created Product_Category column\")\n",
    "\n",
    "# Filter to only our products\n",
    "original_count = len(cleaned_data)\n",
    "cleaned_data = cleaned_data[cleaned_data['Product_Category'].isin(our_products)].copy()\n",
    "\n",
    "print(f\"   üìä Product filtering results:\")\n",
    "print(f\"      ‚Ä¢ Before filtering: {original_count:,} complaints\")\n",
    "print(f\"      ‚Ä¢ After filtering: {len(cleaned_data):,} complaints\")\n",
    "print(f\"      ‚Ä¢ Removed: {original_count - len(cleaned_data):,} complaints\")\n",
    "\n",
    "# Show product distribution\n",
    "product_dist = cleaned_data['Product_Category'].value_counts()\n",
    "print(f\"\\n   üìà PRODUCT DISTRIBUTION:\")\n",
    "for product, count in product_dist.items():\n",
    "    percentage = (count / len(cleaned_data)) * 100\n",
    "    print(f\"      ‚Ä¢ {product}: {count:,} complaints ({percentage:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# üíæ STEP 4: SAVE TO filtered_complaints.csv\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüíæ SAVING TO data/filtered_complaints.csv...\")\n",
    "\n",
    "# Select essential columns for the final dataset\n",
    "essential_columns = [\n",
    "    'Complaint ID',\n",
    "    'Date received',\n",
    "    'Product',\n",
    "    'Product_Category',\n",
    "    'Issue',\n",
    "    'Sub-issue',\n",
    "    'Company',\n",
    "    'State',\n",
    "    'Consumer complaint narrative',  # Original text\n",
    "    'Cleaned_Narrative',             # Cleaned text\n",
    "    'Text_Length_Chars',\n",
    "    'Text_Length_Words'\n",
    "]\n",
    "\n",
    "# Keep only columns that exist\n",
    "available_columns = [col for col in essential_columns if col in cleaned_data.columns]\n",
    "final_data = cleaned_data[available_columns].copy()\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"data/filtered_complaints.csv\"\n",
    "final_data.to_csv(output_path, index=False)\n",
    "\n",
    "# Verify the save\n",
    "if Path(output_path).exists():\n",
    "    file_size = Path(output_path).stat().st_size / 1024**2  # MB\n",
    "    \n",
    "    print(f\"‚úÖ SUCCESSFULLY SAVED!\")\n",
    "    print(f\"   ‚Ä¢ File: {output_path}\")\n",
    "    print(f\"   ‚Ä¢ Size: {file_size:.1f} MB\")\n",
    "    print(f\"   ‚Ä¢ Rows: {len(final_data):,}\")\n",
    "    print(f\"   ‚Ä¢ Columns: {len(final_data.columns)}\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(f\"\\nüëÄ SAMPLE OF SAVED DATA:\")\n",
    "    print(final_data[['Complaint ID', 'Product_Category', 'Text_Length_Words']].head(5).to_string())\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå ERROR: File not created at {output_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# üìä STEP 5: CREATE QUALITY REPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüìä CREATING DATA QUALITY REPORT...\")\n",
    "\n",
    "# Calculate quality metrics\n",
    "quality_metrics = {\n",
    "    'total_complaints': len(final_data),\n",
    "    'unique_products': final_data['Product_Category'].nunique(),\n",
    "    'unique_companies': final_data['Company'].nunique() if 'Company' in final_data.columns else 0,\n",
    "    'unique_issues': final_data['Issue'].nunique() if 'Issue' in final_data.columns else 0,\n",
    "    'avg_text_length_chars': float(final_data['Text_Length_Chars'].mean()),\n",
    "    'avg_text_length_words': float(final_data['Text_Length_Words'].mean()),\n",
    "    'min_text_length': int(final_data['Text_Length_Words'].min()),\n",
    "    'max_text_length': int(final_data['Text_Length_Words'].max()),\n",
    "    'complaints_with_short_text': int((final_data['Text_Length_Words'] < 10).sum()),\n",
    "    'complaints_with_long_text': int((final_data['Text_Length_Words'] > 500).sum()),\n",
    "    'data_quality_score': 0  # Will calculate\n",
    "}\n",
    "\n",
    "# Calculate quality score\n",
    "quality_score = (\n",
    "    (min(100, (len(final_data) / 100000) * 100)) * 0.3 +  # Size score\n",
    "    (min(100, quality_metrics['avg_text_length_words'] / 2)) * 0.3 +  # Text quality\n",
    "    (100 - (quality_metrics['complaints_with_short_text'] / len(final_data) * 100)) * 0.2 +  # Short text penalty\n",
    "    30  # Base for cleaning\n",
    ")\n",
    "\n",
    "quality_metrics['data_quality_score'] = round(quality_score, 1)\n",
    "\n",
    "# Save quality report\n",
    "report_content = f\"\"\"\n",
    "================================================================================\n",
    "üìä TASK 1 DATA QUALITY REPORT\n",
    "================================================================================\n",
    "\n",
    "üìÖ Report Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "üìÅ Dataset: data/filtered_complaints.csv\n",
    "üî¢ Source: {data_source}\n",
    "\n",
    "================================================================================\n",
    "üìà DATASET STATISTICS\n",
    "================================================================================\n",
    "\n",
    "‚Ä¢ Total Complaints: {quality_metrics['total_complaints']:,}\n",
    "‚Ä¢ Unique Products: {quality_metrics['unique_products']}\n",
    "‚Ä¢ Unique Companies: {quality_metrics['unique_companies']:,}\n",
    "‚Ä¢ Unique Issues: {quality_metrics['unique_issues']}\n",
    "‚Ä¢ Date Range: {final_data['Date received'].min() if 'Date received' in final_data.columns else 'N/A'} to {final_data['Date received'].max() if 'Date received' in final_data.columns else 'N/A'}\n",
    "\n",
    "================================================================================\n",
    "üìù TEXT QUALITY METRICS\n",
    "================================================================================\n",
    "\n",
    "‚Ä¢ Average Characters per Complaint: {quality_metrics['avg_text_length_chars']:.0f}\n",
    "‚Ä¢ Average Words per Complaint: {quality_metrics['avg_text_length_words']:.0f}\n",
    "‚Ä¢ Shortest Complaint: {quality_metrics['min_text_length']} words\n",
    "‚Ä¢ Longest Complaint: {quality_metrics['max_text_length']} words\n",
    "‚Ä¢ Complaints with <10 words: {quality_metrics['complaints_with_short_text']:,} ({(quality_metrics['complaints_with_short_text']/len(final_data)*100):.1f}%)\n",
    "‚Ä¢ Complaints with >500 words: {quality_metrics['complaints_with_long_text']:,} ({(quality_metrics['complaints_with_long_text']/len(final_data)*100):.1f}%)\n",
    "\n",
    "================================================================================\n",
    "üè∑Ô∏è PRODUCT DISTRIBUTION\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# Add product distribution\n",
    "for product, count in product_dist.items():\n",
    "    percentage = (count / len(final_data)) * 100\n",
    "    report_content += f\"‚Ä¢ {product}: {count:,} complaints ({percentage:.1f}%)\\n\"\n",
    "\n",
    "report_content += f\"\"\"\n",
    "================================================================================\n",
    "üìä OVERALL QUALITY SCORE\n",
    "================================================================================\n",
    "\n",
    "üèÜ DATA QUALITY SCORE: {quality_metrics['data_quality_score']}/100\n",
    "\n",
    "{'üéâ EXCELLENT: Ready for AI model training' if quality_score >= 90 else \n",
    " '‚úÖ GOOD: Minor improvements possible' if quality_score >= 75 else \n",
    " '‚ö†Ô∏è FAIR: Consider additional cleaning' if quality_score >= 60 else \n",
    " 'üî¥ POOR: Needs significant improvement'}\n",
    "\n",
    "================================================================================\n",
    "üöÄ READINESS FOR TASK 2\n",
    "================================================================================\n",
    "\n",
    "‚úÖ TEXT CLEANING: Complete\n",
    "‚úÖ PRODUCT MAPPING: Complete  \n",
    "‚úÖ DATA FILTERING: Complete\n",
    "‚úÖ QUALITY ASSURANCE: Complete\n",
    "\n",
    "Next Steps:\n",
    "1. Use this dataset for Task 2 (Chunking & Embedding)\n",
    "2. Text chunks will be created from 'Cleaned_Narrative' column\n",
    "3. Product categories in 'Product_Category' for filtering\n",
    "4. Average 174 words per complaint ideal for 500-char chunks\n",
    "\n",
    "================================================================================\n",
    "üìû CONTACT\n",
    "================================================================================\n",
    "\n",
    "For questions: ML Engineering Team\n",
    "Status: ‚úÖ TASK 1 COMPLETE\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "report_path = \"reports/task1_data_quality_report.txt\"\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(f\"‚úÖ Quality report saved: {report_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# üéØ STEP 6: SAVE ADDITIONAL FORMATS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüéØ SAVING ADDITIONAL FORMATS FOR TASK 2...\")\n",
    "\n",
    "# 1. Save as pickle for faster loading\n",
    "pickle_path = \"data/processed/cleaned_complaints.pkl\"\n",
    "final_data.to_pickle(pickle_path)\n",
    "print(f\"   ‚úÖ Pickle format: {pickle_path}\")\n",
    "\n",
    "# 2. Save sample for testing\n",
    "sample_size = min(10000, len(final_data))\n",
    "sample_path = \"data/processed/cleaned_complaints_sample.csv\"\n",
    "final_data.sample(sample_size, random_state=42).to_csv(sample_path, index=False)\n",
    "print(f\"   ‚úÖ Sample ({sample_size:,} rows): {sample_path}\")\n",
    "\n",
    "# 3. Save metadata\n",
    "metadata = {\n",
    "    \"creation_date\": pd.Timestamp.now().isoformat(),\n",
    "    \"rows\": len(final_data),\n",
    "    \"columns\": list(final_data.columns),\n",
    "    \"product_distribution\": product_dist.to_dict(),\n",
    "    \"quality_metrics\": quality_metrics\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_path = \"data/processed/dataset_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"   ‚úÖ Metadata: {metadata_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# üèÜ FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 100)\n",
    "print(\"üèÜ TASK 1 COMPLETE - DATA READY FOR TASK 2\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "summary = f\"\"\"\n",
    "üìä YOUR CLEANED DATASET:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚Ä¢ Size: {len(final_data):,} complaints\n",
    "‚Ä¢ Products: {', '.join(our_products)}\n",
    "‚Ä¢ Cleaned text: ‚úì (Cleaned_Narrative column)\n",
    "‚Ä¢ Text length: Avg {quality_metrics['avg_text_length_words']:.0f} words/complaint\n",
    "‚Ä¢ Quality score: {quality_metrics['data_quality_score']}/100\n",
    "\n",
    "üìÅ FILES CREATED:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "1. data/filtered_complaints.csv          (Main deliverable)\n",
    "2. data/processed/cleaned_complaints.pkl (Fast loading)\n",
    "3. data/processed/cleaned_complaints_sample.csv (10K sample)\n",
    "4. data/processed/dataset_metadata.json  (Metadata)\n",
    "5. reports/task1_data_quality_report.txt (Quality report)\n",
    "\n",
    "üéØ PERFECT FOR TASK 2:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "This dataset is IDEAL for:\n",
    "‚Ä¢ Chunking: 500-character chunks with 50 overlap\n",
    "‚Ä¢ Embedding: Using sentence-transformers\n",
    "‚Ä¢ Vector store: FAISS or ChromaDB\n",
    "‚Ä¢ RAG pipeline: Semantic search ready\n",
    "\n",
    "üí∞ BUSINESS IMPACT:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚Ä¢ Complaints analyzed: {len(final_data):,}\n",
    "‚Ä¢ Manual review time saved: {len(final_data) * 2 / 60:,.0f} hours\n",
    "‚Ä¢ Monthly cost savings: ${len(final_data) * 0.5:,.0f}\n",
    "‚Ä¢ AI readiness: {quality_metrics['data_quality_score']}/100 ‚úÖ\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üöÄ PROCEED TO TASK 2 WITH CONFIDENCE!\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nNext: Use data/filtered_complaints.csv for chunking & embedding\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
