{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d91415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Config loaded - Data path: d:\\10 acadamy\\Intelligent Complaint Analysis for Financial Services\\notebooks\\..\\data\\processed\\filtered_complaints.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\10 acadamy\\Intelligent Complaint Analysis for Financial Services\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete - Processing ALL your real data\n"
     ]
    }
   ],
   "source": [
    "import sys, pandas as pd, numpy as np\n",
    "sys.path.append('..')\n",
    "from src.task2_config import DATA_PATH\n",
    "from src.loader import load_real_data\n",
    "from src.chunker import chunk_all_complaints\n",
    "from src.embedder import generate_all_embeddings\n",
    "from src.vectorstore import create_vectorstore, search\n",
    "from src.task2_reporter import create_final_summary\n",
    "print(\"âœ… Setup complete - Processing ALL your real data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc42124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading from: ..\\data\\processed\\filtered_complaints.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 265,694 processed complaints\n",
      "ğŸ“Š Products:\n",
      "Product_Category\n",
      "Mortgage        130134\n",
      "Credit Card      80620\n",
      "Student Loan     53194\n",
      "Payday Loan       1746\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load YOUR processed data from Task 1\n",
    "from pathlib import Path\n",
    "\n",
    "processed_file = Path('../data/processed/filtered_complaints.csv')\n",
    "print(f\"ğŸ“‚ Loading from: {processed_file}\")\n",
    "df = pd.read_csv(processed_file)\n",
    "print(f\"âœ… Loaded {len(df):,} processed complaints\")\n",
    "print(f\"ğŸ“Š Products:\\n{df['Product_Category'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab9d2c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading YOUR real data: d:\\10 acadamy\\Intelligent Complaint Analysis for Financial Services\\notebooks\\..\\data\\processed\\filtered_complaints.csv\n",
      "âœ… Loaded 265,694 complaints\n",
      "ğŸ“‹ Columns: ['Complaint ID', 'Date received', 'Product', 'Product_Category', 'Issue', 'Company', 'State', 'Consumer complaint narrative', 'Cleaned_Narrative']\n",
      "\n",
      "ğŸ“Š Processing ALL 265,694 complaints\n",
      "ğŸ“‹ Columns: ['Complaint ID', 'Date received', 'Product', 'Product_Category', 'Issue', 'Company', 'State', 'Consumer complaint narrative', 'Cleaned_Narrative']\n"
     ]
    }
   ],
   "source": [
    "df = load_real_data()\n",
    "print(f\"\\nğŸ“Š Processing ALL {len(df):,} complaints\")\n",
    "print(f\"ğŸ“‹ Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef77763a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 265,694 complaints\n",
      "\n",
      "ğŸ“‹ COLUMNS:\n",
      "  â€¢ 'Complaint ID'\n",
      "  â€¢ 'Date received'\n",
      "  â€¢ 'Product'\n",
      "  â€¢ 'Product_Category'\n",
      "  â€¢ 'Issue'\n",
      "  â€¢ 'Company'\n",
      "  â€¢ 'State'\n",
      "  â€¢ 'Consumer complaint narrative'\n",
      "  â€¢ 'Cleaned_Narrative'\n",
      "\n",
      "ğŸ“ Sample narrative (first 100 chars):\n",
      "I signed a purchase agreement with Lennar Corporation on XX/XX/year>, for a new construction home in\n",
      "\n",
      "ğŸ“Š Complaints with narratives: 265,694/265,694 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "import sys, pandas as pd\n",
    "sys.path.append('..')\n",
    "from src.task2_config import DATA_PATH\n",
    "\n",
    "# Load and inspect\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"âœ… Loaded {len(df):,} complaints\")\n",
    "print(f\"\\nğŸ“‹ COLUMNS:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  â€¢ '{col}'\")\n",
    "    \n",
    "# Check the narrative column\n",
    "narrative_col = 'Consumer complaint narrative'\n",
    "if narrative_col in df.columns:\n",
    "    print(f\"\\nğŸ“ Sample narrative (first 100 chars):\")\n",
    "    print(df[narrative_col].iloc[0][:100] if pd.notna(df[narrative_col].iloc[0]) else \"EMPTY\")\n",
    "    \n",
    "    # Count non-empty narratives\n",
    "    non_empty = df[narrative_col].notna().sum()\n",
    "    print(f\"\\nğŸ“Š Complaints with narratives: {non_empty:,}/{len(df):,} ({(non_empty/len(df)*100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a229320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 219386/265694 [38:54<18:08, 42.54it/s]  "
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Chunking\"):\n",
    "    text = str(row['Consumer complaint narrative'])\n",
    "    if len(text) < 10: continue\n",
    "    for i, chunk in enumerate(splitter.split_text(text)):\n",
    "        chunks.append({\n",
    "            'chunk_id': len(chunks),\n",
    "            'complaint_id': idx,\n",
    "            'product': row['Product_Category'],\n",
    "            'chunk_index': i,\n",
    "            'total_chunks': len(splitter.split_text(text)),\n",
    "            'chunk_text': chunk,\n",
    "            'chunk_length': len(chunk)\n",
    "        })\n",
    "\n",
    "chunks_df = pd.DataFrame(chunks)\n",
    "chunks_df.to_parquet('../data/chunks/all_chunks.parquet')\n",
    "print(f\"âœ… Created {len(chunks_df):,} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d1349",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m             embeddings.append(vec/norm \u001b[38;5;28;01mif\u001b[39;00m norm>\u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m vec)\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.array(embeddings)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m texts = \u001b[43mchunks_df\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mchunk_text\u001b[39m\u001b[33m'\u001b[39m].tolist()\n\u001b[32m     22\u001b[39m embedder = SimpleEmbedder(\u001b[32m500\u001b[39m).fit(texts)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Save in batches\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'chunks_df' is not defined"
     ]
    }
   ],
   "source": [
    "class SimpleEmbedder:\n",
    "    def __init__(self, vocab_size=500): \n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_idx = {}\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        word_counts = Counter()\n",
    "        for text in texts[:10000]:\n",
    "            word_counts.update(str(text).lower().split())\n",
    "        for i, (w, _) in enumerate(word_counts.most_common(self.vocab_size)):\n",
    "            self.word_to_idx[w] = i\n",
    "        return self\n",
    "    \n",
    "    def encode(self, texts):\n",
    "        embeddings = []\n",
    "        for text in tqdm(texts, desc=\"Embedding\"):\n",
    "            vec = np.zeros(self.vocab_size)\n",
    "            for w in str(text).lower().split():\n",
    "                if w in self.word_to_idx:\n",
    "                    vec[self.word_to_idx[w]] += 1\n",
    "            norm = np.linalg.norm(vec)\n",
    "            embeddings.append(vec/norm if norm > 0 else vec)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "# Now chunks_df exists from Cell 3\n",
    "texts = chunks_df['chunk_text'].tolist()\n",
    "embedder = SimpleEmbedder(500).fit(texts)\n",
    "\n",
    "# Save in batches\n",
    "os.makedirs('simple_embeddings', exist_ok=True)\n",
    "start = time.time()\n",
    "for i in range(0, len(texts), 10000):\n",
    "    batch = texts[i:i+10000]\n",
    "    np.save(f'simple_embeddings/batch_{i//10000:04d}.npy', embedder.encode(batch))\n",
    "    print(f\"Batch {i//10000+1}: {min(i+10000, len(texts)):,}/{len(texts):,} - {(time.time()-start)/60:.1f} min\")\n",
    "\n",
    "print(f\"\\nâœ… Done: {len(texts):,} embeddings in {(time.time()-start)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all embeddings\n",
    "all_embeddings = []\n",
    "batch_files = sorted(Path('simple_embeddings').glob('batch_*.npy'))\n",
    "for f in tqdm(batch_files, desc=\"Loading embeddings\"):\n",
    "    all_embeddings.append(np.load(f))\n",
    "embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "faiss.normalize_L2(embeddings)\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "# Save metadata\n",
    "metadata = [{\n",
    "    'chunk_id': int(r['chunk_id']),\n",
    "    'complaint_id': int(r['complaint_id']),\n",
    "    'product': r['product'],\n",
    "    'chunk_text': r['chunk_text'][:200]\n",
    "} for _, r in chunks_df.iterrows()]\n",
    "\n",
    "# Save everything\n",
    "os.makedirs('vector_store', exist_ok=True)\n",
    "faiss.write_index(index, 'vector_store/faiss_index.idx')\n",
    "with open('vector_store/metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(f\"\\nâœ… FAISS index: {index.ntotal:,} vectors\")\n",
    "print(f\"âœ… Metadata: {len(metadata):,} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3408a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, k=3):\n",
    "    # Simple query embedding\n",
    "    q_vec = np.zeros(500)\n",
    "    for w in query.lower().split():\n",
    "        if w in embedder.word_to_idx:\n",
    "            q_vec[embedder.word_to_idx[w]] += 1\n",
    "    q_vec = q_vec / np.linalg.norm(q_vec) if np.linalg.norm(q_vec) > 0 else q_vec\n",
    "    \n",
    "    # Search\n",
    "    scores, indices = index.search(q_vec.reshape(1, -1).astype('float32'), k)\n",
    "    \n",
    "    print(f\"\\nğŸ” Query: '{query}'\")\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        if idx < len(metadata):\n",
    "            print(f\"\\n  {i+1}. Score: {score:.3f} | Product: {metadata[idx]['product']}\")\n",
    "            print(f\"     {metadata[idx]['chunk_text'][:150]}...\")\n",
    "\n",
    "# Test\n",
    "search(\"credit card fraud\")\n",
    "search(\"mortgage payment late\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
